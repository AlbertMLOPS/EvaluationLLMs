{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tasa de respuestas correctas LLMs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install streamlit\n",
    "# #https://dashboard.ngrok.com/signup\n",
    "#!pip install --upgrade typing_extensions\n",
    "#!pip install openai\n",
    "#!pip install pypdf\n",
    "#!pip install langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install -U langchain-community"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importe la clase PyPDFLoader del módulo langchain.document_loaders.\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "\n",
    "# Cree una instancia de la clase Py\n",
    "# PDFLoader, pasando el nombre del archivo del documento PDF que desea cargar.\n",
    "loader = PyPDFLoader(\"llm_doc.pdf\")\n",
    "\n",
    "# Este método lee el contenido del archivo PDF especificado al crear la instancia del cargador.\n",
    "# El contenido del documento cargado se almacena en la variable 'documentos' para su posterior procesamiento.\n",
    "documents = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Esta clase se utiliza para dividir textos en fragmentos más pequeños, basándose en el número de caracteres.\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "# Se especifica el tamaño de cada fragmento de texto (chunk_size) en 1000 caracteres y\n",
    "# la superposición entre fragmentos consecutivos (chunk_overlap) en 100 caracteres.\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size    = 1000,\n",
    "                                               chunk_overlap = 100)\n",
    "\n",
    "# Este método divide el contenido de los documentos cargados en fragmentos más pequeños,\n",
    "# basándose en los parámetros de tamaño y superposición especificados.\n",
    "doc_splits = text_splitter.split_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install --upgrade pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'producer': 'Adobe PDF Library 17.0',\n",
       " 'creator': 'Adobe InDesign 19.0 (Windows)',\n",
       " 'creationdate': '2023-11-27T11:41:44+05:30',\n",
       " 'moddate': '2023-11-27T11:41:45+05:30',\n",
       " 'trapped': '/False',\n",
       " 'source': 'llm_doc.pdf',\n",
       " 'total_pages': 14,\n",
       " 'page': 0,\n",
       " 'page_label': '1'}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_splits[0].metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Chunks</th>\n",
       "      <th>Metadata</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Building Pipelines and Environments for  \\nLar...</td>\n",
       "      <td>{'producer': 'Adobe PDF Library 17.0', 'creato...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Contents\\nIntroduction to LLMOps 1\\nWhy LLMOps...</td>\n",
       "      <td>{'producer': 'Adobe PDF Library 17.0', 'creato...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.\\nIntroduction to LLMOps\\nGenerative AI mode...</td>\n",
       "      <td>{'producer': 'Adobe PDF Library 17.0', 'creato...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>translation and even creative writing.\\nUsing ...</td>\n",
       "      <td>{'producer': 'Adobe PDF Library 17.0', 'creato...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>with LLMs. Together, these allow data scientis...</td>\n",
       "      <td>{'producer': 'Adobe PDF Library 17.0', 'creato...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              Chunks  \\\n",
       "0  Building Pipelines and Environments for  \\nLar...   \n",
       "1  Contents\\nIntroduction to LLMOps 1\\nWhy LLMOps...   \n",
       "2  1.\\nIntroduction to LLMOps\\nGenerative AI mode...   \n",
       "3  translation and even creative writing.\\nUsing ...   \n",
       "4  with LLMs. Together, these allow data scientis...   \n",
       "\n",
       "                                            Metadata  \n",
       "0  {'producer': 'Adobe PDF Library 17.0', 'creato...  \n",
       "1  {'producer': 'Adobe PDF Library 17.0', 'creato...  \n",
       "2  {'producer': 'Adobe PDF Library 17.0', 'creato...  \n",
       "3  {'producer': 'Adobe PDF Library 17.0', 'creato...  \n",
       "4  {'producer': 'Adobe PDF Library 17.0', 'creato...  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "# Crear una lista de diccionarios a partir de los fragmentos de documento divididos previamente.\n",
    "# Cada diccionario contiene dos claves: 'Chunks', que almacena el contenido del fragmento de texto,\n",
    "# y 'Metadata', que almacena los metadatos asociados a dicho fragmento.\n",
    "data = [{'Chunks': doc.page_content, 'Metadata': doc.metadata} for doc in doc_splits]\n",
    "# Crear un DataFrame de pandas a partir de la lista de diccionarios 'data'.\n",
    "df_vector_store = pd.DataFrame(data)\n",
    "df_vector_store.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install openai==0.28"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import AzureOpenAI\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "\n",
    "# === 1. Cargar credenciales desde JSON ===\n",
    "def cargar_credenciales(ruta_credenciales):\n",
    "    with open(ruta_credenciales, \"r\") as file:\n",
    "        return json.load(file)\n",
    "\n",
    "# === 2. Crear cliente AzureOpenAI ===\n",
    "def crear_cliente_azure(creds):\n",
    "    client = AzureOpenAI(\n",
    "        api_key=creds[\"AZURE_API_KEY\"],\n",
    "        api_version=creds[\"AZURE_API_VERSION\"],\n",
    "        azure_endpoint=creds[\"AZURE_ENDPOINT\"]\n",
    "    )\n",
    "    return client\n",
    "\n",
    "# === 3. Generar embeddings usando cliente (AzureOpenAI) ===\n",
    "def text_embedding(text=[]):\n",
    "    embeddings = client.embeddings.create(model=\"text-embedding-ada-002\",\n",
    "                                          input=text,\n",
    "                                          encoding_format=\"float\")\n",
    "    return embeddings.data[0].embedding\n",
    "\n",
    "# === 4. Uso real del código ===\n",
    "ruta_credenciales = \"credentials.json\"\n",
    "creds = cargar_credenciales(ruta_credenciales)\n",
    "client = crear_cliente_azure(creds)\n",
    "\n",
    "# Aplicar embeddings\n",
    "df_vector_store[\"Embedding\"] = df_vector_store[\"Chunks\"].apply(lambda x: text_embedding([x]))\n",
    "df_vector_store[\"Embedding\"] = df_vector_store[\"Embedding\"].apply(np.array)\n",
    "\n",
    "# Guardar a archivo\n",
    "#df_vector_store.to_pickle('df_vector_store.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Chunks</th>\n",
       "      <th>Metadata</th>\n",
       "      <th>Embedding</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Building Pipelines and Environments for  \\nLar...</td>\n",
       "      <td>{'producer': 'Adobe PDF Library 17.0', 'creato...</td>\n",
       "      <td>[-0.001881948, -0.012164683, 0.016830197, -0.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Contents\\nIntroduction to LLMOps 1\\nWhy LLMOps...</td>\n",
       "      <td>{'producer': 'Adobe PDF Library 17.0', 'creato...</td>\n",
       "      <td>[0.0030545578, -0.0043646125, -0.0022074673, -...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.\\nIntroduction to LLMOps\\nGenerative AI mode...</td>\n",
       "      <td>{'producer': 'Adobe PDF Library 17.0', 'creato...</td>\n",
       "      <td>[-0.021225467, -0.008492879, -0.013970853, -0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>translation and even creative writing.\\nUsing ...</td>\n",
       "      <td>{'producer': 'Adobe PDF Library 17.0', 'creato...</td>\n",
       "      <td>[-0.0065719597, -0.0055598076, -0.0033509966, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>with LLMs. Together, these allow data scientis...</td>\n",
       "      <td>{'producer': 'Adobe PDF Library 17.0', 'creato...</td>\n",
       "      <td>[0.0069066356, -0.009416879, 0.0025986563, -0....</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              Chunks  \\\n",
       "0  Building Pipelines and Environments for  \\nLar...   \n",
       "1  Contents\\nIntroduction to LLMOps 1\\nWhy LLMOps...   \n",
       "2  1.\\nIntroduction to LLMOps\\nGenerative AI mode...   \n",
       "3  translation and even creative writing.\\nUsing ...   \n",
       "4  with LLMs. Together, these allow data scientis...   \n",
       "\n",
       "                                            Metadata  \\\n",
       "0  {'producer': 'Adobe PDF Library 17.0', 'creato...   \n",
       "1  {'producer': 'Adobe PDF Library 17.0', 'creato...   \n",
       "2  {'producer': 'Adobe PDF Library 17.0', 'creato...   \n",
       "3  {'producer': 'Adobe PDF Library 17.0', 'creato...   \n",
       "4  {'producer': 'Adobe PDF Library 17.0', 'creato...   \n",
       "\n",
       "                                           Embedding  \n",
       "0  [-0.001881948, -0.012164683, 0.016830197, -0.0...  \n",
       "1  [0.0030545578, -0.0043646125, -0.0022074673, -...  \n",
       "2  [-0.021225467, -0.008492879, -0.013970853, -0....  \n",
       "3  [-0.0065719597, -0.0055598076, -0.0033509966, ...  \n",
       "4  [0.0069066356, -0.009416879, 0.0025986563, -0....  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_vector_store.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definir una cadena de texto que contiene la consulta de interés.\n",
    "#query = '¿Cómo se selecciona un modelo llm?'\n",
    "\n",
    "query = '¿Qué son los LLMOps?'\n",
    "\n",
    "# Generar la representación vectorial (embedding) de la consulta de texto utilizando la función 'text_embedding'.\n",
    "query_embedding = text_embedding(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definir una función para calcular el producto punto entre dos vectores.\n",
    "def get_dot_product(row):\n",
    "    return np.dot(row, query_vector)\n",
    "\n",
    "# Definir una función para calcular la similitud de coseno entre dos vectores.\n",
    "def cosine_similarity(row):\n",
    "    denominator1 = np.linalg.norm(row)\n",
    "    denominator2 = np.linalg.norm(query_vector.ravel())\n",
    "    dot_prod = np.dot(row, query_vector)\n",
    "    return dot_prod/(denominator1*denominator2)\n",
    "\n",
    "# Definir una función para obtener los fragmentos de texto más relevantes desde un almacenamiento\n",
    "# vectorial dada una consulta.\n",
    "def get_context_from_query(query, vector_store, n_chunks = 5):\n",
    "    global query_vector\n",
    "    # Convertir el embedding de la consulta en un array de numpy.\n",
    "    query_vector = np.array(query_embedding)\n",
    "    # Calcular la similitud de coseno para cada vector en el almacenamiento y ordenar los resultados\n",
    "    # de mayor a menor similitud, seleccionando los índices de los 'n_chunks' más altos.\n",
    "    top_matched = (\n",
    "        vector_store[\"Embedding\"]\n",
    "        .apply(cosine_similarity)\n",
    "        .sort_values(ascending=False)[:n_chunks]\n",
    "        .index)\n",
    "    # Seleccionar los fragmentos de texto correspondientes a los índices de mayor similitud.\n",
    "    top_matched_df = vector_store[vector_store.index.isin(top_matched)][[\"Chunks\"]]\n",
    "    # Devolver una lista con los fragmentos de texto seleccionados.\n",
    "    return list(top_matched_df['Chunks'])\n",
    "\n",
    "# Utilizar la función 'get_context_from_query' para obtener los fragmentos de texto más relevantes\n",
    "# dada la consulta especificada y el almacenamiento vectorial 'df_vector_store', limitando el número\n",
    "# de fragmentos a 5.\n",
    "Context_List = get_context_from_query(\n",
    "    query        = query,\n",
    "    vector_store = df_vector_store,\n",
    "    n_chunks     = 5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_prompt = \"\"\"\n",
    "Eres una Inteligencia Artificial super avanzada que trabaja asistente personal.\n",
    "Utilice los RESULTADOS DE BÚSQUEDA SEMANTICA para responder las preguntas del usuario.\n",
    "Solo debes utilizar la informacion de la BUSQUEDA SEMANTICA si es que hace sentido y tiene relacion con la pregunta del usuario.\n",
    "Si la respuesta no se encuentra dentro del contexto de la búsqueda semántica, no inventes una respuesta, y responde amablemente que no tienes información para responder.\n",
    "\n",
    "RESULTADOS DE BÚSQUEDA SEMANTICA:\n",
    "{source}\n",
    "\n",
    "Lee cuidadosamente las instrucciones, respira profundo y escribe una respuesta para el usuario!\n",
    "\"\"\".format(source = str(Context_List))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Los LLMOps, o Large Language Model Operations, son un marco de herramientas y mejores prácticas diseñadas para gestionar el ciclo de vida de las aplicaciones impulsadas por modelos de lenguaje de gran tamaño, desde el desarrollo hasta la implementación y el mantenimiento. Estas herramientas se centran en la experimentación, implementación, gestión y monitoreo de grandes modelos de lenguaje, con el objetivo de permitir capacidades de inteligencia artificial con estos modelos, desarrollando mejores indicaciones, contextos más extensos, inferencias más rápidas y técnicas personalizadas que permitan una experimentación e innovación rápidas con los modelos de lenguaje de gran tamaño.\n"
     ]
    }
   ],
   "source": [
    "# Models\n",
    "# modelo_banco = \"gpt-4o-mini_clasificacion-PQRS\"\n",
    "# modelo_banco = \"gpt-35-turbo-16k-PQR\"\n",
    "\n",
    "modelo_banco = \"gpt-35-turbo-16k-PQR\"\n",
    "\n",
    "completion = client.chat.completions.create(\n",
    "  model=modelo_banco,\n",
    "  temperature = 0.0,\n",
    "  messages=[\n",
    "    {\"role\": \"system\", \"content\": custom_prompt},\n",
    "    {\"role\": \"user\", \"content\": query}\n",
    "  ]\n",
    ")\n",
    "\n",
    "# Imprimir el contenido del mensaje de la primera opción de completación generada por el modelo.\n",
    "print(completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tasa de respuestas correctas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Item</th>\n",
       "      <th>Integrante</th>\n",
       "      <th>Pregunta</th>\n",
       "      <th>Respuesta</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>P1</td>\n",
       "      <td>Katherine</td>\n",
       "      <td>¿Qué son los LLMOps?</td>\n",
       "      <td>Los LLMOps son un marco de herramientas y mejo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>P2</td>\n",
       "      <td>Katherine</td>\n",
       "      <td>¿Por qué son importantes los LLMOps?</td>\n",
       "      <td>Los LLMOps son esenciales para superar desafío...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Item Integrante                              Pregunta  \\\n",
       "0   P1  Katherine                  ¿Qué son los LLMOps?   \n",
       "1   P2  Katherine  ¿Por qué son importantes los LLMOps?   \n",
       "\n",
       "                                           Respuesta  \n",
       "0  Los LLMOps son un marco de herramientas y mejo...  \n",
       "1  Los LLMOps son esenciales para superar desafío...  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "base = pd.read_excel(\"preguntas_BOT.xlsx\")\n",
    "base.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pregunta1  Similaridad:  0.9116775409886054  Feedback:  1\n",
      "Pregunta2  Similaridad:  0.9463164461463259  Feedback:  1\n",
      "Pregunta3  Similaridad:  0.9194636226766232  Feedback:  1\n",
      "Pregunta4  Similaridad:  0.8998629977361134  Feedback:  0\n",
      "Pregunta5  Similaridad:  0.9476553843912816  Feedback:  1\n",
      "Pregunta6  Similaridad:  0.9759774868664376  Feedback:  1\n",
      "Pregunta7  Similaridad:  0.9029540936275927  Feedback:  1\n",
      "Pregunta8  Similaridad:  0.8601067311907453  Feedback:  0\n",
      "Pregunta9  Similaridad:  0.8450317926436656  Feedback:  0\n",
      "Pregunta10  Similaridad:  0.9198931560805396  Feedback:  1\n"
     ]
    }
   ],
   "source": [
    "es_correcta=[]\n",
    "respuestas = []\n",
    "\n",
    "for i in range(10):\n",
    "    query = base[\"Pregunta\"][i]\n",
    "    respuesta_humana = base[\"Respuesta\"][i]\n",
    "\n",
    "    query_embedding = text_embedding(query)\n",
    "    query_vector = np.array(query_embedding)\n",
    "\n",
    "    def cosine_similarity(row_embedding):\n",
    "        denominator1 = np.linalg.norm(row_embedding)\n",
    "        denominator2 = np.linalg.norm(query_vector)\n",
    "        dot_prod = np.dot(row_embedding, query_vector)\n",
    "        return dot_prod/(denominator1*denominator2)\n",
    "\n",
    "    top_matched=(\n",
    "        df_vector_store[\"Embedding\"].apply(cosine_similarity).sort_values(ascending=False)[:5].index\n",
    "    )\n",
    "\n",
    "    context_chunks=list(df_vector_store.loc[top_matched,'Chunks'])\n",
    "\n",
    "    custom_prompt = \"\"\"\n",
    "    Eres una Inteligencia Artificial super avanzada que trabaja asistente personal.\n",
    "    Utilice los RESULTADOS DE BÚSQUEDA SEMANTICA para responder las preguntas del usuario.\n",
    "    Solo debes utilizar la informacion de la BUSQUEDA SEMANTICA si es que hace sentido y tiene relacion con la pregunta del usuario.\n",
    "    Si la respuesta no se encuentra dentro del contexto de la búsqueda semántica, no inventes una respuesta, y responde amablemente que no tienes información para responder.\n",
    "\n",
    "    RESULTADOS DE BÚSQUEDA SEMANTICA:\n",
    "    {source}\n",
    "\n",
    "    Lee cuidadosamente las instrucciones, respira profundo y escribe una respuesta para el usuario!\n",
    "    \"\"\".format(source = str(context_chunks))\n",
    "\n",
    "    completion = client.chat.completions.create(\n",
    "    model=modelo_banco,\n",
    "    temperature = 0.0,\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": custom_prompt},\n",
    "        {\"role\": \"user\", \"content\": query}\n",
    "    ]\n",
    "    )\n",
    "\n",
    "    # Imprimir el contenido del mensaje de la primera opción de completación generada por el modelo.\n",
    "    respuesta_modelo=completion.choices[0].message.content\n",
    "    respuestas.append(respuesta_modelo)\n",
    "\n",
    "    emb_modelo=np.array(text_embedding(respuesta_modelo))\n",
    "    emb_humana=np.array(text_embedding(respuesta_humana))\n",
    "    def cosine_(v1,v2):\n",
    "        return np.dot(v1,v2)/(np.linalg.norm(v1)*np.linalg.norm(v2))\n",
    "    similitud=cosine_(emb_modelo,emb_humana)\n",
    "\n",
    "    correcta = 1 if similitud>=0.90 else 0\n",
    "    print(\"Pregunta\"+str(i+1),\" Similaridad: \",similitud,\" Feedback: \",correcta)  \n",
    "    es_correcta.append(correcta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La tasa de respuestas correctas es: 70.0\n"
     ]
    }
   ],
   "source": [
    "print(\"La tasa de respuestas correctas es:\",100*np.mean(es_correcta))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "my_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
