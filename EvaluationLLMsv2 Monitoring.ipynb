{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ST (Pruebas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuración cargada exitosamente desde config.yaml\n",
      "Cliente Azure OpenAI inicializado.\n",
      "Cliente Gemini/Gemma configurado.\n",
      "Cargando y procesando PDF...\n",
      "Generando embeddings para GPT (Azure)...\n",
      "Generando embeddings para Gemma...\n",
      "Enviando solicitud al LLM para generar 10 pares QA...\n",
      "Generación del LLM completada. Procesando pares QA...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following layers were not sharded: embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.LayerNorm.weight, pooler.dense.weight, encoder.layer.*.attention.self.value.bias, encoder.layer.*.attention.self.key.bias, encoder.layer.*.attention.output.LayerNorm.bias, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.output.dense.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.self.query.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.intermediate.dense.weight, embeddings.word_embeddings.weight, encoder.layer.*.output.dense.weight, encoder.layer.*.attention.self.value.weight, encoder.layer.*.attention.output.dense.bias, embeddings.position_embeddings.weight, embeddings.LayerNorm.weight, encoder.layer.*.output.LayerNorm.bias, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, pooler.dense.bias, encoder.layer.*.attention.self.query.bias\n",
      "The following layers were not sharded: embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.LayerNorm.weight, pooler.dense.weight, encoder.layer.*.attention.self.value.bias, encoder.layer.*.attention.self.key.bias, encoder.layer.*.attention.output.LayerNorm.bias, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.output.dense.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.self.query.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.intermediate.dense.weight, embeddings.word_embeddings.weight, encoder.layer.*.output.dense.weight, encoder.layer.*.attention.self.value.weight, encoder.layer.*.attention.output.dense.bias, embeddings.position_embeddings.weight, embeddings.LayerNorm.weight, encoder.layer.*.output.LayerNorm.bias, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, pooler.dense.bias, encoder.layer.*.attention.self.query.bias\n",
      "The following layers were not sharded: embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.LayerNorm.weight, pooler.dense.weight, encoder.layer.*.attention.self.value.bias, encoder.layer.*.attention.self.key.bias, encoder.layer.*.attention.output.LayerNorm.bias, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.output.dense.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.self.query.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.intermediate.dense.weight, embeddings.word_embeddings.weight, encoder.layer.*.output.dense.weight, encoder.layer.*.attention.self.value.weight, encoder.layer.*.attention.output.dense.bias, embeddings.position_embeddings.weight, embeddings.LayerNorm.weight, encoder.layer.*.output.LayerNorm.bias, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, pooler.dense.bias, encoder.layer.*.attention.self.query.bias\n",
      "The following layers were not sharded: embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.LayerNorm.weight, pooler.dense.weight, encoder.layer.*.attention.self.value.bias, encoder.layer.*.attention.self.key.bias, encoder.layer.*.attention.output.LayerNorm.bias, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.output.dense.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.self.query.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.intermediate.dense.weight, embeddings.word_embeddings.weight, encoder.layer.*.output.dense.weight, encoder.layer.*.attention.self.value.weight, encoder.layer.*.attention.output.dense.bias, embeddings.position_embeddings.weight, embeddings.LayerNorm.weight, encoder.layer.*.output.LayerNorm.bias, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, pooler.dense.bias, encoder.layer.*.attention.self.query.bias\n",
      "The following layers were not sharded: embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.LayerNorm.weight, pooler.dense.weight, encoder.layer.*.attention.self.value.bias, encoder.layer.*.attention.self.key.bias, encoder.layer.*.attention.output.LayerNorm.bias, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.output.dense.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.self.query.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.intermediate.dense.weight, embeddings.word_embeddings.weight, encoder.layer.*.output.dense.weight, encoder.layer.*.attention.self.value.weight, encoder.layer.*.attention.output.dense.bias, embeddings.position_embeddings.weight, embeddings.LayerNorm.weight, encoder.layer.*.output.LayerNorm.bias, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, pooler.dense.bias, encoder.layer.*.attention.self.query.bias\n",
      "The following layers were not sharded: embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.LayerNorm.weight, pooler.dense.weight, encoder.layer.*.attention.self.value.bias, encoder.layer.*.attention.self.key.bias, encoder.layer.*.attention.output.LayerNorm.bias, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.output.dense.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.self.query.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.intermediate.dense.weight, embeddings.word_embeddings.weight, encoder.layer.*.output.dense.weight, encoder.layer.*.attention.self.value.weight, encoder.layer.*.attention.output.dense.bias, embeddings.position_embeddings.weight, embeddings.LayerNorm.weight, encoder.layer.*.output.LayerNorm.bias, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, pooler.dense.bias, encoder.layer.*.attention.self.query.bias\n",
      "The following layers were not sharded: embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.LayerNorm.weight, pooler.dense.weight, encoder.layer.*.attention.self.value.bias, encoder.layer.*.attention.self.key.bias, encoder.layer.*.attention.output.LayerNorm.bias, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.output.dense.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.self.query.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.intermediate.dense.weight, embeddings.word_embeddings.weight, encoder.layer.*.output.dense.weight, encoder.layer.*.attention.self.value.weight, encoder.layer.*.attention.output.dense.bias, embeddings.position_embeddings.weight, embeddings.LayerNorm.weight, encoder.layer.*.output.LayerNorm.bias, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, pooler.dense.bias, encoder.layer.*.attention.self.query.bias\n",
      "The following layers were not sharded: embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.LayerNorm.weight, pooler.dense.weight, encoder.layer.*.attention.self.value.bias, encoder.layer.*.attention.self.key.bias, encoder.layer.*.attention.output.LayerNorm.bias, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.output.dense.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.self.query.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.intermediate.dense.weight, embeddings.word_embeddings.weight, encoder.layer.*.output.dense.weight, encoder.layer.*.attention.self.value.weight, encoder.layer.*.attention.output.dense.bias, embeddings.position_embeddings.weight, embeddings.LayerNorm.weight, encoder.layer.*.output.LayerNorm.bias, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, pooler.dense.bias, encoder.layer.*.attention.self.query.bias\n",
      "The following layers were not sharded: embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.LayerNorm.weight, pooler.dense.weight, encoder.layer.*.attention.self.value.bias, encoder.layer.*.attention.self.key.bias, encoder.layer.*.attention.output.LayerNorm.bias, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.output.dense.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.self.query.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.intermediate.dense.weight, embeddings.word_embeddings.weight, encoder.layer.*.output.dense.weight, encoder.layer.*.attention.self.value.weight, encoder.layer.*.attention.output.dense.bias, embeddings.position_embeddings.weight, embeddings.LayerNorm.weight, encoder.layer.*.output.LayerNorm.bias, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, pooler.dense.bias, encoder.layer.*.attention.self.query.bias\n",
      "The following layers were not sharded: embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.LayerNorm.weight, pooler.dense.weight, encoder.layer.*.attention.self.value.bias, encoder.layer.*.attention.self.key.bias, encoder.layer.*.attention.output.LayerNorm.bias, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.output.dense.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.self.query.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.intermediate.dense.weight, embeddings.word_embeddings.weight, encoder.layer.*.output.dense.weight, encoder.layer.*.attention.self.value.weight, encoder.layer.*.attention.output.dense.bias, embeddings.position_embeddings.weight, embeddings.LayerNorm.weight, encoder.layer.*.output.LayerNorm.bias, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, pooler.dense.bias, encoder.layer.*.attention.self.query.bias\n",
      "The following layers were not sharded: embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.LayerNorm.weight, pooler.dense.weight, encoder.layer.*.attention.self.value.bias, encoder.layer.*.attention.self.key.bias, encoder.layer.*.attention.output.LayerNorm.bias, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.output.dense.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.self.query.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.intermediate.dense.weight, embeddings.word_embeddings.weight, encoder.layer.*.output.dense.weight, encoder.layer.*.attention.self.value.weight, encoder.layer.*.attention.output.dense.bias, embeddings.position_embeddings.weight, embeddings.LayerNorm.weight, encoder.layer.*.output.LayerNorm.bias, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, pooler.dense.bias, encoder.layer.*.attention.self.query.bias\n",
      "The following layers were not sharded: embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.LayerNorm.weight, pooler.dense.weight, encoder.layer.*.attention.self.value.bias, encoder.layer.*.attention.self.key.bias, encoder.layer.*.attention.output.LayerNorm.bias, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.output.dense.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.self.query.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.intermediate.dense.weight, embeddings.word_embeddings.weight, encoder.layer.*.output.dense.weight, encoder.layer.*.attention.self.value.weight, encoder.layer.*.attention.output.dense.bias, embeddings.position_embeddings.weight, embeddings.LayerNorm.weight, encoder.layer.*.output.LayerNorm.bias, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, pooler.dense.bias, encoder.layer.*.attention.self.query.bias\n",
      "The following layers were not sharded: embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.LayerNorm.weight, pooler.dense.weight, encoder.layer.*.attention.self.value.bias, encoder.layer.*.attention.self.key.bias, encoder.layer.*.attention.output.LayerNorm.bias, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.output.dense.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.self.query.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.intermediate.dense.weight, embeddings.word_embeddings.weight, encoder.layer.*.output.dense.weight, encoder.layer.*.attention.self.value.weight, encoder.layer.*.attention.output.dense.bias, embeddings.position_embeddings.weight, embeddings.LayerNorm.weight, encoder.layer.*.output.LayerNorm.bias, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, pooler.dense.bias, encoder.layer.*.attention.self.query.bias\n",
      "The following layers were not sharded: embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.LayerNorm.weight, pooler.dense.weight, encoder.layer.*.attention.self.value.bias, encoder.layer.*.attention.self.key.bias, encoder.layer.*.attention.output.LayerNorm.bias, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.output.dense.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.self.query.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.intermediate.dense.weight, embeddings.word_embeddings.weight, encoder.layer.*.output.dense.weight, encoder.layer.*.attention.self.value.weight, encoder.layer.*.attention.output.dense.bias, embeddings.position_embeddings.weight, embeddings.LayerNorm.weight, encoder.layer.*.output.LayerNorm.bias, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, pooler.dense.bias, encoder.layer.*.attention.self.query.bias\n",
      "The following layers were not sharded: embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.LayerNorm.weight, pooler.dense.weight, encoder.layer.*.attention.self.value.bias, encoder.layer.*.attention.self.key.bias, encoder.layer.*.attention.output.LayerNorm.bias, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.output.dense.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.self.query.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.intermediate.dense.weight, embeddings.word_embeddings.weight, encoder.layer.*.output.dense.weight, encoder.layer.*.attention.self.value.weight, encoder.layer.*.attention.output.dense.bias, embeddings.position_embeddings.weight, embeddings.LayerNorm.weight, encoder.layer.*.output.LayerNorm.bias, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, pooler.dense.bias, encoder.layer.*.attention.self.query.bias\n",
      "The following layers were not sharded: embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.LayerNorm.weight, pooler.dense.weight, encoder.layer.*.attention.self.value.bias, encoder.layer.*.attention.self.key.bias, encoder.layer.*.attention.output.LayerNorm.bias, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.output.dense.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.self.query.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.intermediate.dense.weight, embeddings.word_embeddings.weight, encoder.layer.*.output.dense.weight, encoder.layer.*.attention.self.value.weight, encoder.layer.*.attention.output.dense.bias, embeddings.position_embeddings.weight, embeddings.LayerNorm.weight, encoder.layer.*.output.LayerNorm.bias, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, pooler.dense.bias, encoder.layer.*.attention.self.query.bias\n",
      "The following layers were not sharded: embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.LayerNorm.weight, pooler.dense.weight, encoder.layer.*.attention.self.value.bias, encoder.layer.*.attention.self.key.bias, encoder.layer.*.attention.output.LayerNorm.bias, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.output.dense.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.self.query.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.intermediate.dense.weight, embeddings.word_embeddings.weight, encoder.layer.*.output.dense.weight, encoder.layer.*.attention.self.value.weight, encoder.layer.*.attention.output.dense.bias, embeddings.position_embeddings.weight, embeddings.LayerNorm.weight, encoder.layer.*.output.LayerNorm.bias, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, pooler.dense.bias, encoder.layer.*.attention.self.query.bias\n",
      "The following layers were not sharded: embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.LayerNorm.weight, pooler.dense.weight, encoder.layer.*.attention.self.value.bias, encoder.layer.*.attention.self.key.bias, encoder.layer.*.attention.output.LayerNorm.bias, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.output.dense.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.self.query.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.intermediate.dense.weight, embeddings.word_embeddings.weight, encoder.layer.*.output.dense.weight, encoder.layer.*.attention.self.value.weight, encoder.layer.*.attention.output.dense.bias, embeddings.position_embeddings.weight, embeddings.LayerNorm.weight, encoder.layer.*.output.LayerNorm.bias, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, pooler.dense.bias, encoder.layer.*.attention.self.query.bias\n",
      "The following layers were not sharded: embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.LayerNorm.weight, pooler.dense.weight, encoder.layer.*.attention.self.value.bias, encoder.layer.*.attention.self.key.bias, encoder.layer.*.attention.output.LayerNorm.bias, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.output.dense.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.self.query.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.intermediate.dense.weight, embeddings.word_embeddings.weight, encoder.layer.*.output.dense.weight, encoder.layer.*.attention.self.value.weight, encoder.layer.*.attention.output.dense.bias, embeddings.position_embeddings.weight, embeddings.LayerNorm.weight, encoder.layer.*.output.LayerNorm.bias, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, pooler.dense.bias, encoder.layer.*.attention.self.query.bias\n",
      "The following layers were not sharded: embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.LayerNorm.weight, pooler.dense.weight, encoder.layer.*.attention.self.value.bias, encoder.layer.*.attention.self.key.bias, encoder.layer.*.attention.output.LayerNorm.bias, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.output.dense.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.self.query.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.intermediate.dense.weight, embeddings.word_embeddings.weight, encoder.layer.*.output.dense.weight, encoder.layer.*.attention.self.value.weight, encoder.layer.*.attention.output.dense.bias, embeddings.position_embeddings.weight, embeddings.LayerNorm.weight, encoder.layer.*.output.LayerNorm.bias, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, pooler.dense.bias, encoder.layer.*.attention.self.query.bias\n",
      "The following layers were not sharded: embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.LayerNorm.weight, pooler.dense.weight, encoder.layer.*.attention.self.value.bias, encoder.layer.*.attention.self.key.bias, encoder.layer.*.attention.output.LayerNorm.bias, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.output.dense.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.self.query.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.intermediate.dense.weight, embeddings.word_embeddings.weight, encoder.layer.*.output.dense.weight, encoder.layer.*.attention.self.value.weight, encoder.layer.*.attention.output.dense.bias, embeddings.position_embeddings.weight, embeddings.LayerNorm.weight, encoder.layer.*.output.LayerNorm.bias, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, pooler.dense.bias, encoder.layer.*.attention.self.query.bias\n",
      "The following layers were not sharded: embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.LayerNorm.weight, pooler.dense.weight, encoder.layer.*.attention.self.value.bias, encoder.layer.*.attention.self.key.bias, encoder.layer.*.attention.output.LayerNorm.bias, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.output.dense.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.self.query.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.intermediate.dense.weight, embeddings.word_embeddings.weight, encoder.layer.*.output.dense.weight, encoder.layer.*.attention.self.value.weight, encoder.layer.*.attention.output.dense.bias, embeddings.position_embeddings.weight, embeddings.LayerNorm.weight, encoder.layer.*.output.LayerNorm.bias, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, pooler.dense.bias, encoder.layer.*.attention.self.query.bias\n",
      "The following layers were not sharded: embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.LayerNorm.weight, pooler.dense.weight, encoder.layer.*.attention.self.value.bias, encoder.layer.*.attention.self.key.bias, encoder.layer.*.attention.output.LayerNorm.bias, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.output.dense.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.self.query.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.intermediate.dense.weight, embeddings.word_embeddings.weight, encoder.layer.*.output.dense.weight, encoder.layer.*.attention.self.value.weight, encoder.layer.*.attention.output.dense.bias, embeddings.position_embeddings.weight, embeddings.LayerNorm.weight, encoder.layer.*.output.LayerNorm.bias, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, pooler.dense.bias, encoder.layer.*.attention.self.query.bias\n",
      "The following layers were not sharded: embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.LayerNorm.weight, pooler.dense.weight, encoder.layer.*.attention.self.value.bias, encoder.layer.*.attention.self.key.bias, encoder.layer.*.attention.output.LayerNorm.bias, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.output.dense.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.self.query.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.intermediate.dense.weight, embeddings.word_embeddings.weight, encoder.layer.*.output.dense.weight, encoder.layer.*.attention.self.value.weight, encoder.layer.*.attention.output.dense.bias, embeddings.position_embeddings.weight, embeddings.LayerNorm.weight, encoder.layer.*.output.LayerNorm.bias, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, pooler.dense.bias, encoder.layer.*.attention.self.query.bias\n",
      "The following layers were not sharded: embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.LayerNorm.weight, pooler.dense.weight, encoder.layer.*.attention.self.value.bias, encoder.layer.*.attention.self.key.bias, encoder.layer.*.attention.output.LayerNorm.bias, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.output.dense.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.self.query.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.intermediate.dense.weight, embeddings.word_embeddings.weight, encoder.layer.*.output.dense.weight, encoder.layer.*.attention.self.value.weight, encoder.layer.*.attention.output.dense.bias, embeddings.position_embeddings.weight, embeddings.LayerNorm.weight, encoder.layer.*.output.LayerNorm.bias, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, pooler.dense.bias, encoder.layer.*.attention.self.query.bias\n",
      "The following layers were not sharded: embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.LayerNorm.weight, pooler.dense.weight, encoder.layer.*.attention.self.value.bias, encoder.layer.*.attention.self.key.bias, encoder.layer.*.attention.output.LayerNorm.bias, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.output.dense.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.self.query.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.intermediate.dense.weight, embeddings.word_embeddings.weight, encoder.layer.*.output.dense.weight, encoder.layer.*.attention.self.value.weight, encoder.layer.*.attention.output.dense.bias, embeddings.position_embeddings.weight, embeddings.LayerNorm.weight, encoder.layer.*.output.LayerNorm.bias, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, pooler.dense.bias, encoder.layer.*.attention.self.query.bias\n",
      "The following layers were not sharded: embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.LayerNorm.weight, pooler.dense.weight, encoder.layer.*.attention.self.value.bias, encoder.layer.*.attention.self.key.bias, encoder.layer.*.attention.output.LayerNorm.bias, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.output.dense.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.self.query.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.intermediate.dense.weight, embeddings.word_embeddings.weight, encoder.layer.*.output.dense.weight, encoder.layer.*.attention.self.value.weight, encoder.layer.*.attention.output.dense.bias, embeddings.position_embeddings.weight, embeddings.LayerNorm.weight, encoder.layer.*.output.LayerNorm.bias, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, pooler.dense.bias, encoder.layer.*.attention.self.query.bias\n",
      "The following layers were not sharded: embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.LayerNorm.weight, pooler.dense.weight, encoder.layer.*.attention.self.value.bias, encoder.layer.*.attention.self.key.bias, encoder.layer.*.attention.output.LayerNorm.bias, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.output.dense.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.self.query.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.intermediate.dense.weight, embeddings.word_embeddings.weight, encoder.layer.*.output.dense.weight, encoder.layer.*.attention.self.value.weight, encoder.layer.*.attention.output.dense.bias, embeddings.position_embeddings.weight, embeddings.LayerNorm.weight, encoder.layer.*.output.LayerNorm.bias, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, pooler.dense.bias, encoder.layer.*.attention.self.query.bias\n",
      "The following layers were not sharded: embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.LayerNorm.weight, pooler.dense.weight, encoder.layer.*.attention.self.value.bias, encoder.layer.*.attention.self.key.bias, encoder.layer.*.attention.output.LayerNorm.bias, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.output.dense.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.self.query.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.intermediate.dense.weight, embeddings.word_embeddings.weight, encoder.layer.*.output.dense.weight, encoder.layer.*.attention.self.value.weight, encoder.layer.*.attention.output.dense.bias, embeddings.position_embeddings.weight, embeddings.LayerNorm.weight, encoder.layer.*.output.LayerNorm.bias, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, pooler.dense.bias, encoder.layer.*.attention.self.query.bias\n",
      "The following layers were not sharded: embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.LayerNorm.weight, pooler.dense.weight, encoder.layer.*.attention.self.value.bias, encoder.layer.*.attention.self.key.bias, encoder.layer.*.attention.output.LayerNorm.bias, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.output.dense.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.self.query.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.intermediate.dense.weight, embeddings.word_embeddings.weight, encoder.layer.*.output.dense.weight, encoder.layer.*.attention.self.value.weight, encoder.layer.*.attention.output.dense.bias, embeddings.position_embeddings.weight, embeddings.LayerNorm.weight, encoder.layer.*.output.LayerNorm.bias, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, pooler.dense.bias, encoder.layer.*.attention.self.query.bias\n",
      "The following layers were not sharded: embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.LayerNorm.weight, pooler.dense.weight, encoder.layer.*.attention.self.value.bias, encoder.layer.*.attention.self.key.bias, encoder.layer.*.attention.output.LayerNorm.bias, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.output.dense.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.self.query.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.intermediate.dense.weight, embeddings.word_embeddings.weight, encoder.layer.*.output.dense.weight, encoder.layer.*.attention.self.value.weight, encoder.layer.*.attention.output.dense.bias, embeddings.position_embeddings.weight, embeddings.LayerNorm.weight, encoder.layer.*.output.LayerNorm.bias, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, pooler.dense.bias, encoder.layer.*.attention.self.query.bias\n",
      "The following layers were not sharded: embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.LayerNorm.weight, pooler.dense.weight, encoder.layer.*.attention.self.value.bias, encoder.layer.*.attention.self.key.bias, encoder.layer.*.attention.output.LayerNorm.bias, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.output.dense.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.self.query.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.intermediate.dense.weight, embeddings.word_embeddings.weight, encoder.layer.*.output.dense.weight, encoder.layer.*.attention.self.value.weight, encoder.layer.*.attention.output.dense.bias, embeddings.position_embeddings.weight, embeddings.LayerNorm.weight, encoder.layer.*.output.LayerNorm.bias, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, pooler.dense.bias, encoder.layer.*.attention.self.query.bias\n",
      "The following layers were not sharded: embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.LayerNorm.weight, pooler.dense.weight, encoder.layer.*.attention.self.value.bias, encoder.layer.*.attention.self.key.bias, encoder.layer.*.attention.output.LayerNorm.bias, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.output.dense.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.self.query.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.intermediate.dense.weight, embeddings.word_embeddings.weight, encoder.layer.*.output.dense.weight, encoder.layer.*.attention.self.value.weight, encoder.layer.*.attention.output.dense.bias, embeddings.position_embeddings.weight, embeddings.LayerNorm.weight, encoder.layer.*.output.LayerNorm.bias, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, pooler.dense.bias, encoder.layer.*.attention.self.query.bias\n",
      "The following layers were not sharded: embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.LayerNorm.weight, pooler.dense.weight, encoder.layer.*.attention.self.value.bias, encoder.layer.*.attention.self.key.bias, encoder.layer.*.attention.output.LayerNorm.bias, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.output.dense.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.self.query.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.intermediate.dense.weight, embeddings.word_embeddings.weight, encoder.layer.*.output.dense.weight, encoder.layer.*.attention.self.value.weight, encoder.layer.*.attention.output.dense.bias, embeddings.position_embeddings.weight, embeddings.LayerNorm.weight, encoder.layer.*.output.LayerNorm.bias, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, pooler.dense.bias, encoder.layer.*.attention.self.query.bias\n",
      "The following layers were not sharded: embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.LayerNorm.weight, pooler.dense.weight, encoder.layer.*.attention.self.value.bias, encoder.layer.*.attention.self.key.bias, encoder.layer.*.attention.output.LayerNorm.bias, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.output.dense.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.self.query.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.intermediate.dense.weight, embeddings.word_embeddings.weight, encoder.layer.*.output.dense.weight, encoder.layer.*.attention.self.value.weight, encoder.layer.*.attention.output.dense.bias, embeddings.position_embeddings.weight, embeddings.LayerNorm.weight, encoder.layer.*.output.LayerNorm.bias, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, pooler.dense.bias, encoder.layer.*.attention.self.query.bias\n",
      "The following layers were not sharded: embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.LayerNorm.weight, pooler.dense.weight, encoder.layer.*.attention.self.value.bias, encoder.layer.*.attention.self.key.bias, encoder.layer.*.attention.output.LayerNorm.bias, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.output.dense.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.self.query.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.intermediate.dense.weight, embeddings.word_embeddings.weight, encoder.layer.*.output.dense.weight, encoder.layer.*.attention.self.value.weight, encoder.layer.*.attention.output.dense.bias, embeddings.position_embeddings.weight, embeddings.LayerNorm.weight, encoder.layer.*.output.LayerNorm.bias, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, pooler.dense.bias, encoder.layer.*.attention.self.query.bias\n",
      "The following layers were not sharded: embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.LayerNorm.weight, pooler.dense.weight, encoder.layer.*.attention.self.value.bias, encoder.layer.*.attention.self.key.bias, encoder.layer.*.attention.output.LayerNorm.bias, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.output.dense.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.self.query.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.intermediate.dense.weight, embeddings.word_embeddings.weight, encoder.layer.*.output.dense.weight, encoder.layer.*.attention.self.value.weight, encoder.layer.*.attention.output.dense.bias, embeddings.position_embeddings.weight, embeddings.LayerNorm.weight, encoder.layer.*.output.LayerNorm.bias, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, pooler.dense.bias, encoder.layer.*.attention.self.query.bias\n",
      "The following layers were not sharded: embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.LayerNorm.weight, pooler.dense.weight, encoder.layer.*.attention.self.value.bias, encoder.layer.*.attention.self.key.bias, encoder.layer.*.attention.output.LayerNorm.bias, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.output.dense.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.self.query.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.intermediate.dense.weight, embeddings.word_embeddings.weight, encoder.layer.*.output.dense.weight, encoder.layer.*.attention.self.value.weight, encoder.layer.*.attention.output.dense.bias, embeddings.position_embeddings.weight, embeddings.LayerNorm.weight, encoder.layer.*.output.LayerNorm.bias, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, pooler.dense.bias, encoder.layer.*.attention.self.query.bias\n",
      "The following layers were not sharded: embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.LayerNorm.weight, pooler.dense.weight, encoder.layer.*.attention.self.value.bias, encoder.layer.*.attention.self.key.bias, encoder.layer.*.attention.output.LayerNorm.bias, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.output.dense.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.self.query.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.intermediate.dense.weight, embeddings.word_embeddings.weight, encoder.layer.*.output.dense.weight, encoder.layer.*.attention.self.value.weight, encoder.layer.*.attention.output.dense.bias, embeddings.position_embeddings.weight, embeddings.LayerNorm.weight, encoder.layer.*.output.LayerNorm.bias, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, pooler.dense.bias, encoder.layer.*.attention.self.query.bias\n",
      "The following layers were not sharded: embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.LayerNorm.weight, pooler.dense.weight, encoder.layer.*.attention.self.value.bias, encoder.layer.*.attention.self.key.bias, encoder.layer.*.attention.output.LayerNorm.bias, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.output.dense.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.self.query.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.intermediate.dense.weight, embeddings.word_embeddings.weight, encoder.layer.*.output.dense.weight, encoder.layer.*.attention.self.value.weight, encoder.layer.*.attention.output.dense.bias, embeddings.position_embeddings.weight, embeddings.LayerNorm.weight, encoder.layer.*.output.LayerNorm.bias, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, pooler.dense.bias, encoder.layer.*.attention.self.query.bias\n",
      "The following layers were not sharded: embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.LayerNorm.weight, pooler.dense.weight, encoder.layer.*.attention.self.value.bias, encoder.layer.*.attention.self.key.bias, encoder.layer.*.attention.output.LayerNorm.bias, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.output.dense.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.self.query.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.intermediate.dense.weight, embeddings.word_embeddings.weight, encoder.layer.*.output.dense.weight, encoder.layer.*.attention.self.value.weight, encoder.layer.*.attention.output.dense.bias, embeddings.position_embeddings.weight, embeddings.LayerNorm.weight, encoder.layer.*.output.LayerNorm.bias, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, pooler.dense.bias, encoder.layer.*.attention.self.query.bias\n",
      "The following layers were not sharded: embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.LayerNorm.weight, pooler.dense.weight, encoder.layer.*.attention.self.value.bias, encoder.layer.*.attention.self.key.bias, encoder.layer.*.attention.output.LayerNorm.bias, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.output.dense.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.self.query.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.intermediate.dense.weight, embeddings.word_embeddings.weight, encoder.layer.*.output.dense.weight, encoder.layer.*.attention.self.value.weight, encoder.layer.*.attention.output.dense.bias, embeddings.position_embeddings.weight, embeddings.LayerNorm.weight, encoder.layer.*.output.LayerNorm.bias, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, pooler.dense.bias, encoder.layer.*.attention.self.query.bias\n",
      "The following layers were not sharded: embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.LayerNorm.weight, pooler.dense.weight, encoder.layer.*.attention.self.value.bias, encoder.layer.*.attention.self.key.bias, encoder.layer.*.attention.output.LayerNorm.bias, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.output.dense.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.self.query.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.intermediate.dense.weight, embeddings.word_embeddings.weight, encoder.layer.*.output.dense.weight, encoder.layer.*.attention.self.value.weight, encoder.layer.*.attention.output.dense.bias, embeddings.position_embeddings.weight, embeddings.LayerNorm.weight, encoder.layer.*.output.LayerNorm.bias, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, pooler.dense.bias, encoder.layer.*.attention.self.query.bias\n",
      "The following layers were not sharded: embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.LayerNorm.weight, pooler.dense.weight, encoder.layer.*.attention.self.value.bias, encoder.layer.*.attention.self.key.bias, encoder.layer.*.attention.output.LayerNorm.bias, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.output.dense.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.self.query.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.intermediate.dense.weight, embeddings.word_embeddings.weight, encoder.layer.*.output.dense.weight, encoder.layer.*.attention.self.value.weight, encoder.layer.*.attention.output.dense.bias, embeddings.position_embeddings.weight, embeddings.LayerNorm.weight, encoder.layer.*.output.LayerNorm.bias, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, pooler.dense.bias, encoder.layer.*.attention.self.query.bias\n",
      "The following layers were not sharded: embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.LayerNorm.weight, pooler.dense.weight, encoder.layer.*.attention.self.value.bias, encoder.layer.*.attention.self.key.bias, encoder.layer.*.attention.output.LayerNorm.bias, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.output.dense.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.self.query.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.intermediate.dense.weight, embeddings.word_embeddings.weight, encoder.layer.*.output.dense.weight, encoder.layer.*.attention.self.value.weight, encoder.layer.*.attention.output.dense.bias, embeddings.position_embeddings.weight, embeddings.LayerNorm.weight, encoder.layer.*.output.LayerNorm.bias, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, pooler.dense.bias, encoder.layer.*.attention.self.query.bias\n",
      "The following layers were not sharded: embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.LayerNorm.weight, pooler.dense.weight, encoder.layer.*.attention.self.value.bias, encoder.layer.*.attention.self.key.bias, encoder.layer.*.attention.output.LayerNorm.bias, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.output.dense.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.self.query.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.intermediate.dense.weight, embeddings.word_embeddings.weight, encoder.layer.*.output.dense.weight, encoder.layer.*.attention.self.value.weight, encoder.layer.*.attention.output.dense.bias, embeddings.position_embeddings.weight, embeddings.LayerNorm.weight, encoder.layer.*.output.LayerNorm.bias, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, pooler.dense.bias, encoder.layer.*.attention.self.query.bias\n",
      "The following layers were not sharded: embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.LayerNorm.weight, pooler.dense.weight, encoder.layer.*.attention.self.value.bias, encoder.layer.*.attention.self.key.bias, encoder.layer.*.attention.output.LayerNorm.bias, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.output.dense.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.self.query.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.intermediate.dense.weight, embeddings.word_embeddings.weight, encoder.layer.*.output.dense.weight, encoder.layer.*.attention.self.value.weight, encoder.layer.*.attention.output.dense.bias, embeddings.position_embeddings.weight, embeddings.LayerNorm.weight, encoder.layer.*.output.LayerNorm.bias, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, pooler.dense.bias, encoder.layer.*.attention.self.query.bias\n",
      "The following layers were not sharded: embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.LayerNorm.weight, pooler.dense.weight, encoder.layer.*.attention.self.value.bias, encoder.layer.*.attention.self.key.bias, encoder.layer.*.attention.output.LayerNorm.bias, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.output.dense.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.self.query.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.intermediate.dense.weight, embeddings.word_embeddings.weight, encoder.layer.*.output.dense.weight, encoder.layer.*.attention.self.value.weight, encoder.layer.*.attention.output.dense.bias, embeddings.position_embeddings.weight, embeddings.LayerNorm.weight, encoder.layer.*.output.LayerNorm.bias, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, pooler.dense.bias, encoder.layer.*.attention.self.query.bias\n",
      "The following layers were not sharded: embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.LayerNorm.weight, pooler.dense.weight, encoder.layer.*.attention.self.value.bias, encoder.layer.*.attention.self.key.bias, encoder.layer.*.attention.output.LayerNorm.bias, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.output.dense.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.self.query.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.intermediate.dense.weight, embeddings.word_embeddings.weight, encoder.layer.*.output.dense.weight, encoder.layer.*.attention.self.value.weight, encoder.layer.*.attention.output.dense.bias, embeddings.position_embeddings.weight, embeddings.LayerNorm.weight, encoder.layer.*.output.LayerNorm.bias, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, pooler.dense.bias, encoder.layer.*.attention.self.query.bias\n",
      "The following layers were not sharded: embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.LayerNorm.weight, pooler.dense.weight, encoder.layer.*.attention.self.value.bias, encoder.layer.*.attention.self.key.bias, encoder.layer.*.attention.output.LayerNorm.bias, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.output.dense.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.self.query.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.intermediate.dense.weight, embeddings.word_embeddings.weight, encoder.layer.*.output.dense.weight, encoder.layer.*.attention.self.value.weight, encoder.layer.*.attention.output.dense.bias, embeddings.position_embeddings.weight, embeddings.LayerNorm.weight, encoder.layer.*.output.LayerNorm.bias, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, pooler.dense.bias, encoder.layer.*.attention.self.query.bias\n",
      "The following layers were not sharded: embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.LayerNorm.weight, pooler.dense.weight, encoder.layer.*.attention.self.value.bias, encoder.layer.*.attention.self.key.bias, encoder.layer.*.attention.output.LayerNorm.bias, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.output.dense.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.self.query.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.intermediate.dense.weight, embeddings.word_embeddings.weight, encoder.layer.*.output.dense.weight, encoder.layer.*.attention.self.value.weight, encoder.layer.*.attention.output.dense.bias, embeddings.position_embeddings.weight, embeddings.LayerNorm.weight, encoder.layer.*.output.LayerNorm.bias, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, pooler.dense.bias, encoder.layer.*.attention.self.query.bias\n",
      "The following layers were not sharded: embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.LayerNorm.weight, pooler.dense.weight, encoder.layer.*.attention.self.value.bias, encoder.layer.*.attention.self.key.bias, encoder.layer.*.attention.output.LayerNorm.bias, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.output.dense.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.self.query.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.intermediate.dense.weight, embeddings.word_embeddings.weight, encoder.layer.*.output.dense.weight, encoder.layer.*.attention.self.value.weight, encoder.layer.*.attention.output.dense.bias, embeddings.position_embeddings.weight, embeddings.LayerNorm.weight, encoder.layer.*.output.LayerNorm.bias, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, pooler.dense.bias, encoder.layer.*.attention.self.query.bias\n",
      "The following layers were not sharded: embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.LayerNorm.weight, pooler.dense.weight, encoder.layer.*.attention.self.value.bias, encoder.layer.*.attention.self.key.bias, encoder.layer.*.attention.output.LayerNorm.bias, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.output.dense.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.self.query.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.intermediate.dense.weight, embeddings.word_embeddings.weight, encoder.layer.*.output.dense.weight, encoder.layer.*.attention.self.value.weight, encoder.layer.*.attention.output.dense.bias, embeddings.position_embeddings.weight, embeddings.LayerNorm.weight, encoder.layer.*.output.LayerNorm.bias, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, pooler.dense.bias, encoder.layer.*.attention.self.query.bias\n",
      "The following layers were not sharded: embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.LayerNorm.weight, pooler.dense.weight, encoder.layer.*.attention.self.value.bias, encoder.layer.*.attention.self.key.bias, encoder.layer.*.attention.output.LayerNorm.bias, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.output.dense.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.self.query.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.intermediate.dense.weight, embeddings.word_embeddings.weight, encoder.layer.*.output.dense.weight, encoder.layer.*.attention.self.value.weight, encoder.layer.*.attention.output.dense.bias, embeddings.position_embeddings.weight, embeddings.LayerNorm.weight, encoder.layer.*.output.LayerNorm.bias, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, pooler.dense.bias, encoder.layer.*.attention.self.query.bias\n",
      "The following layers were not sharded: embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.LayerNorm.weight, pooler.dense.weight, encoder.layer.*.attention.self.value.bias, encoder.layer.*.attention.self.key.bias, encoder.layer.*.attention.output.LayerNorm.bias, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.output.dense.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.self.query.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.intermediate.dense.weight, embeddings.word_embeddings.weight, encoder.layer.*.output.dense.weight, encoder.layer.*.attention.self.value.weight, encoder.layer.*.attention.output.dense.bias, embeddings.position_embeddings.weight, embeddings.LayerNorm.weight, encoder.layer.*.output.LayerNorm.bias, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, pooler.dense.bias, encoder.layer.*.attention.self.query.bias\n",
      "The following layers were not sharded: embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.LayerNorm.weight, pooler.dense.weight, encoder.layer.*.attention.self.value.bias, encoder.layer.*.attention.self.key.bias, encoder.layer.*.attention.output.LayerNorm.bias, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.output.dense.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.self.query.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.intermediate.dense.weight, embeddings.word_embeddings.weight, encoder.layer.*.output.dense.weight, encoder.layer.*.attention.self.value.weight, encoder.layer.*.attention.output.dense.bias, embeddings.position_embeddings.weight, embeddings.LayerNorm.weight, encoder.layer.*.output.LayerNorm.bias, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, pooler.dense.bias, encoder.layer.*.attention.self.query.bias\n",
      "The following layers were not sharded: embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.LayerNorm.weight, pooler.dense.weight, encoder.layer.*.attention.self.value.bias, encoder.layer.*.attention.self.key.bias, encoder.layer.*.attention.output.LayerNorm.bias, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.output.dense.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.self.query.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.intermediate.dense.weight, embeddings.word_embeddings.weight, encoder.layer.*.output.dense.weight, encoder.layer.*.attention.self.value.weight, encoder.layer.*.attention.output.dense.bias, embeddings.position_embeddings.weight, embeddings.LayerNorm.weight, encoder.layer.*.output.LayerNorm.bias, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, pooler.dense.bias, encoder.layer.*.attention.self.query.bias\n",
      "The following layers were not sharded: embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.LayerNorm.weight, pooler.dense.weight, encoder.layer.*.attention.self.value.bias, encoder.layer.*.attention.self.key.bias, encoder.layer.*.attention.output.LayerNorm.bias, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.output.dense.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.self.query.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.intermediate.dense.weight, embeddings.word_embeddings.weight, encoder.layer.*.output.dense.weight, encoder.layer.*.attention.self.value.weight, encoder.layer.*.attention.output.dense.bias, embeddings.position_embeddings.weight, embeddings.LayerNorm.weight, encoder.layer.*.output.LayerNorm.bias, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, pooler.dense.bias, encoder.layer.*.attention.self.query.bias\n",
      "The following layers were not sharded: embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.LayerNorm.weight, pooler.dense.weight, encoder.layer.*.attention.self.value.bias, encoder.layer.*.attention.self.key.bias, encoder.layer.*.attention.output.LayerNorm.bias, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.output.dense.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.self.query.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.intermediate.dense.weight, embeddings.word_embeddings.weight, encoder.layer.*.output.dense.weight, encoder.layer.*.attention.self.value.weight, encoder.layer.*.attention.output.dense.bias, embeddings.position_embeddings.weight, embeddings.LayerNorm.weight, encoder.layer.*.output.LayerNorm.bias, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, pooler.dense.bias, encoder.layer.*.attention.self.query.bias\n",
      "The following layers were not sharded: embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.LayerNorm.weight, pooler.dense.weight, encoder.layer.*.attention.self.value.bias, encoder.layer.*.attention.self.key.bias, encoder.layer.*.attention.output.LayerNorm.bias, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.output.dense.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.self.query.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.intermediate.dense.weight, embeddings.word_embeddings.weight, encoder.layer.*.output.dense.weight, encoder.layer.*.attention.self.value.weight, encoder.layer.*.attention.output.dense.bias, embeddings.position_embeddings.weight, embeddings.LayerNorm.weight, encoder.layer.*.output.LayerNorm.bias, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, pooler.dense.bias, encoder.layer.*.attention.self.query.bias\n",
      "The following layers were not sharded: embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.LayerNorm.weight, pooler.dense.weight, encoder.layer.*.attention.self.value.bias, encoder.layer.*.attention.self.key.bias, encoder.layer.*.attention.output.LayerNorm.bias, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.output.dense.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.self.query.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.intermediate.dense.weight, embeddings.word_embeddings.weight, encoder.layer.*.output.dense.weight, encoder.layer.*.attention.self.value.weight, encoder.layer.*.attention.output.dense.bias, embeddings.position_embeddings.weight, embeddings.LayerNorm.weight, encoder.layer.*.output.LayerNorm.bias, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, pooler.dense.bias, encoder.layer.*.attention.self.query.bias\n",
      "The following layers were not sharded: embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.LayerNorm.weight, pooler.dense.weight, encoder.layer.*.attention.self.value.bias, encoder.layer.*.attention.self.key.bias, encoder.layer.*.attention.output.LayerNorm.bias, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.output.dense.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.self.query.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.intermediate.dense.weight, embeddings.word_embeddings.weight, encoder.layer.*.output.dense.weight, encoder.layer.*.attention.self.value.weight, encoder.layer.*.attention.output.dense.bias, embeddings.position_embeddings.weight, embeddings.LayerNorm.weight, encoder.layer.*.output.LayerNorm.bias, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, pooler.dense.bias, encoder.layer.*.attention.self.query.bias\n",
      "The following layers were not sharded: embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.LayerNorm.weight, pooler.dense.weight, encoder.layer.*.attention.self.value.bias, encoder.layer.*.attention.self.key.bias, encoder.layer.*.attention.output.LayerNorm.bias, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.output.dense.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.self.query.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.intermediate.dense.weight, embeddings.word_embeddings.weight, encoder.layer.*.output.dense.weight, encoder.layer.*.attention.self.value.weight, encoder.layer.*.attention.output.dense.bias, embeddings.position_embeddings.weight, embeddings.LayerNorm.weight, encoder.layer.*.output.LayerNorm.bias, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, pooler.dense.bias, encoder.layer.*.attention.self.query.bias\n",
      "The following layers were not sharded: embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.LayerNorm.weight, pooler.dense.weight, encoder.layer.*.attention.self.value.bias, encoder.layer.*.attention.self.key.bias, encoder.layer.*.attention.output.LayerNorm.bias, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.output.dense.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.self.query.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.intermediate.dense.weight, embeddings.word_embeddings.weight, encoder.layer.*.output.dense.weight, encoder.layer.*.attention.self.value.weight, encoder.layer.*.attention.output.dense.bias, embeddings.position_embeddings.weight, embeddings.LayerNorm.weight, encoder.layer.*.output.LayerNorm.bias, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, pooler.dense.bias, encoder.layer.*.attention.self.query.bias\n",
      "The following layers were not sharded: embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.LayerNorm.weight, pooler.dense.weight, encoder.layer.*.attention.self.value.bias, encoder.layer.*.attention.self.key.bias, encoder.layer.*.attention.output.LayerNorm.bias, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.output.dense.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.self.query.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.intermediate.dense.weight, embeddings.word_embeddings.weight, encoder.layer.*.output.dense.weight, encoder.layer.*.attention.self.value.weight, encoder.layer.*.attention.output.dense.bias, embeddings.position_embeddings.weight, embeddings.LayerNorm.weight, encoder.layer.*.output.LayerNorm.bias, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, pooler.dense.bias, encoder.layer.*.attention.self.query.bias\n",
      "The following layers were not sharded: embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.LayerNorm.weight, pooler.dense.weight, encoder.layer.*.attention.self.value.bias, encoder.layer.*.attention.self.key.bias, encoder.layer.*.attention.output.LayerNorm.bias, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.output.dense.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.self.query.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.intermediate.dense.weight, embeddings.word_embeddings.weight, encoder.layer.*.output.dense.weight, encoder.layer.*.attention.self.value.weight, encoder.layer.*.attention.output.dense.bias, embeddings.position_embeddings.weight, embeddings.LayerNorm.weight, encoder.layer.*.output.LayerNorm.bias, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, pooler.dense.bias, encoder.layer.*.attention.self.query.bias\n",
      "The following layers were not sharded: embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.LayerNorm.weight, pooler.dense.weight, encoder.layer.*.attention.self.value.bias, encoder.layer.*.attention.self.key.bias, encoder.layer.*.attention.output.LayerNorm.bias, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.output.dense.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.self.query.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.intermediate.dense.weight, embeddings.word_embeddings.weight, encoder.layer.*.output.dense.weight, encoder.layer.*.attention.self.value.weight, encoder.layer.*.attention.output.dense.bias, embeddings.position_embeddings.weight, embeddings.LayerNorm.weight, encoder.layer.*.output.LayerNorm.bias, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, pooler.dense.bias, encoder.layer.*.attention.self.query.bias\n",
      "The following layers were not sharded: embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.LayerNorm.weight, pooler.dense.weight, encoder.layer.*.attention.self.value.bias, encoder.layer.*.attention.self.key.bias, encoder.layer.*.attention.output.LayerNorm.bias, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.output.dense.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.self.query.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.intermediate.dense.weight, embeddings.word_embeddings.weight, encoder.layer.*.output.dense.weight, encoder.layer.*.attention.self.value.weight, encoder.layer.*.attention.output.dense.bias, embeddings.position_embeddings.weight, embeddings.LayerNorm.weight, encoder.layer.*.output.LayerNorm.bias, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, pooler.dense.bias, encoder.layer.*.attention.self.query.bias\n",
      "The following layers were not sharded: embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.LayerNorm.weight, pooler.dense.weight, encoder.layer.*.attention.self.value.bias, encoder.layer.*.attention.self.key.bias, encoder.layer.*.attention.output.LayerNorm.bias, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.output.dense.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.self.query.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.intermediate.dense.weight, embeddings.word_embeddings.weight, encoder.layer.*.output.dense.weight, encoder.layer.*.attention.self.value.weight, encoder.layer.*.attention.output.dense.bias, embeddings.position_embeddings.weight, embeddings.LayerNorm.weight, encoder.layer.*.output.LayerNorm.bias, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, pooler.dense.bias, encoder.layer.*.attention.self.query.bias\n",
      "The following layers were not sharded: embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.LayerNorm.weight, pooler.dense.weight, encoder.layer.*.attention.self.value.bias, encoder.layer.*.attention.self.key.bias, encoder.layer.*.attention.output.LayerNorm.bias, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.output.dense.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.self.query.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.intermediate.dense.weight, embeddings.word_embeddings.weight, encoder.layer.*.output.dense.weight, encoder.layer.*.attention.self.value.weight, encoder.layer.*.attention.output.dense.bias, embeddings.position_embeddings.weight, embeddings.LayerNorm.weight, encoder.layer.*.output.LayerNorm.bias, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, pooler.dense.bias, encoder.layer.*.attention.self.query.bias\n",
      "The following layers were not sharded: embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.LayerNorm.weight, pooler.dense.weight, encoder.layer.*.attention.self.value.bias, encoder.layer.*.attention.self.key.bias, encoder.layer.*.attention.output.LayerNorm.bias, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.output.dense.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.self.query.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.intermediate.dense.weight, embeddings.word_embeddings.weight, encoder.layer.*.output.dense.weight, encoder.layer.*.attention.self.value.weight, encoder.layer.*.attention.output.dense.bias, embeddings.position_embeddings.weight, embeddings.LayerNorm.weight, encoder.layer.*.output.LayerNorm.bias, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, pooler.dense.bias, encoder.layer.*.attention.self.query.bias\n",
      "The following layers were not sharded: embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.LayerNorm.weight, pooler.dense.weight, encoder.layer.*.attention.self.value.bias, encoder.layer.*.attention.self.key.bias, encoder.layer.*.attention.output.LayerNorm.bias, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.output.dense.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.self.query.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.intermediate.dense.weight, embeddings.word_embeddings.weight, encoder.layer.*.output.dense.weight, encoder.layer.*.attention.self.value.weight, encoder.layer.*.attention.output.dense.bias, embeddings.position_embeddings.weight, embeddings.LayerNorm.weight, encoder.layer.*.output.LayerNorm.bias, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, pooler.dense.bias, encoder.layer.*.attention.self.query.bias\n",
      "The following layers were not sharded: embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.LayerNorm.weight, pooler.dense.weight, encoder.layer.*.attention.self.value.bias, encoder.layer.*.attention.self.key.bias, encoder.layer.*.attention.output.LayerNorm.bias, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.output.dense.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.self.query.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.intermediate.dense.weight, embeddings.word_embeddings.weight, encoder.layer.*.output.dense.weight, encoder.layer.*.attention.self.value.weight, encoder.layer.*.attention.output.dense.bias, embeddings.position_embeddings.weight, embeddings.LayerNorm.weight, encoder.layer.*.output.LayerNorm.bias, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, pooler.dense.bias, encoder.layer.*.attention.self.query.bias\n",
      "The following layers were not sharded: embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.LayerNorm.weight, pooler.dense.weight, encoder.layer.*.attention.self.value.bias, encoder.layer.*.attention.self.key.bias, encoder.layer.*.attention.output.LayerNorm.bias, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.output.dense.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.self.query.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.intermediate.dense.weight, embeddings.word_embeddings.weight, encoder.layer.*.output.dense.weight, encoder.layer.*.attention.self.value.weight, encoder.layer.*.attention.output.dense.bias, embeddings.position_embeddings.weight, embeddings.LayerNorm.weight, encoder.layer.*.output.LayerNorm.bias, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, pooler.dense.bias, encoder.layer.*.attention.self.query.bias\n",
      "The following layers were not sharded: embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.LayerNorm.weight, pooler.dense.weight, encoder.layer.*.attention.self.value.bias, encoder.layer.*.attention.self.key.bias, encoder.layer.*.attention.output.LayerNorm.bias, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.output.dense.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.self.query.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.intermediate.dense.weight, embeddings.word_embeddings.weight, encoder.layer.*.output.dense.weight, encoder.layer.*.attention.self.value.weight, encoder.layer.*.attention.output.dense.bias, embeddings.position_embeddings.weight, embeddings.LayerNorm.weight, encoder.layer.*.output.LayerNorm.bias, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, pooler.dense.bias, encoder.layer.*.attention.self.query.bias\n",
      "The following layers were not sharded: embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.LayerNorm.weight, pooler.dense.weight, encoder.layer.*.attention.self.value.bias, encoder.layer.*.attention.self.key.bias, encoder.layer.*.attention.output.LayerNorm.bias, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.output.dense.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.self.query.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.intermediate.dense.weight, embeddings.word_embeddings.weight, encoder.layer.*.output.dense.weight, encoder.layer.*.attention.self.value.weight, encoder.layer.*.attention.output.dense.bias, embeddings.position_embeddings.weight, embeddings.LayerNorm.weight, encoder.layer.*.output.LayerNorm.bias, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, pooler.dense.bias, encoder.layer.*.attention.self.query.bias\n",
      "The following layers were not sharded: embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.LayerNorm.weight, pooler.dense.weight, encoder.layer.*.attention.self.value.bias, encoder.layer.*.attention.self.key.bias, encoder.layer.*.attention.output.LayerNorm.bias, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.output.dense.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.self.query.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.intermediate.dense.weight, embeddings.word_embeddings.weight, encoder.layer.*.output.dense.weight, encoder.layer.*.attention.self.value.weight, encoder.layer.*.attention.output.dense.bias, embeddings.position_embeddings.weight, embeddings.LayerNorm.weight, encoder.layer.*.output.LayerNorm.bias, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, pooler.dense.bias, encoder.layer.*.attention.self.query.bias\n",
      "The following layers were not sharded: embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.LayerNorm.weight, pooler.dense.weight, encoder.layer.*.attention.self.value.bias, encoder.layer.*.attention.self.key.bias, encoder.layer.*.attention.output.LayerNorm.bias, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.output.dense.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.self.query.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.intermediate.dense.weight, embeddings.word_embeddings.weight, encoder.layer.*.output.dense.weight, encoder.layer.*.attention.self.value.weight, encoder.layer.*.attention.output.dense.bias, embeddings.position_embeddings.weight, embeddings.LayerNorm.weight, encoder.layer.*.output.LayerNorm.bias, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, pooler.dense.bias, encoder.layer.*.attention.self.query.bias\n",
      "The following layers were not sharded: embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.LayerNorm.weight, pooler.dense.weight, encoder.layer.*.attention.self.value.bias, encoder.layer.*.attention.self.key.bias, encoder.layer.*.attention.output.LayerNorm.bias, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.output.dense.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.self.query.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.intermediate.dense.weight, embeddings.word_embeddings.weight, encoder.layer.*.output.dense.weight, encoder.layer.*.attention.self.value.weight, encoder.layer.*.attention.output.dense.bias, embeddings.position_embeddings.weight, embeddings.LayerNorm.weight, encoder.layer.*.output.LayerNorm.bias, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, pooler.dense.bias, encoder.layer.*.attention.self.query.bias\n",
      "The following layers were not sharded: embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.LayerNorm.weight, pooler.dense.weight, encoder.layer.*.attention.self.value.bias, encoder.layer.*.attention.self.key.bias, encoder.layer.*.attention.output.LayerNorm.bias, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.output.dense.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.self.query.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.intermediate.dense.weight, embeddings.word_embeddings.weight, encoder.layer.*.output.dense.weight, encoder.layer.*.attention.self.value.weight, encoder.layer.*.attention.output.dense.bias, embeddings.position_embeddings.weight, embeddings.LayerNorm.weight, encoder.layer.*.output.LayerNorm.bias, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, pooler.dense.bias, encoder.layer.*.attention.self.query.bias\n",
      "The following layers were not sharded: embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.LayerNorm.weight, pooler.dense.weight, encoder.layer.*.attention.self.value.bias, encoder.layer.*.attention.self.key.bias, encoder.layer.*.attention.output.LayerNorm.bias, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.output.dense.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.self.query.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.intermediate.dense.weight, embeddings.word_embeddings.weight, encoder.layer.*.output.dense.weight, encoder.layer.*.attention.self.value.weight, encoder.layer.*.attention.output.dense.bias, embeddings.position_embeddings.weight, embeddings.LayerNorm.weight, encoder.layer.*.output.LayerNorm.bias, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, pooler.dense.bias, encoder.layer.*.attention.self.query.bias\n",
      "The following layers were not sharded: embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.LayerNorm.weight, pooler.dense.weight, encoder.layer.*.attention.self.value.bias, encoder.layer.*.attention.self.key.bias, encoder.layer.*.attention.output.LayerNorm.bias, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.output.dense.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.self.query.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.intermediate.dense.weight, embeddings.word_embeddings.weight, encoder.layer.*.output.dense.weight, encoder.layer.*.attention.self.value.weight, encoder.layer.*.attention.output.dense.bias, embeddings.position_embeddings.weight, embeddings.LayerNorm.weight, encoder.layer.*.output.LayerNorm.bias, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, pooler.dense.bias, encoder.layer.*.attention.self.query.bias\n",
      "The following layers were not sharded: embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.LayerNorm.weight, pooler.dense.weight, encoder.layer.*.attention.self.value.bias, encoder.layer.*.attention.self.key.bias, encoder.layer.*.attention.output.LayerNorm.bias, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.output.dense.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.self.query.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.intermediate.dense.weight, embeddings.word_embeddings.weight, encoder.layer.*.output.dense.weight, encoder.layer.*.attention.self.value.weight, encoder.layer.*.attention.output.dense.bias, embeddings.position_embeddings.weight, embeddings.LayerNorm.weight, encoder.layer.*.output.LayerNorm.bias, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, pooler.dense.bias, encoder.layer.*.attention.self.query.bias\n",
      "The following layers were not sharded: embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.LayerNorm.weight, pooler.dense.weight, encoder.layer.*.attention.self.value.bias, encoder.layer.*.attention.self.key.bias, encoder.layer.*.attention.output.LayerNorm.bias, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.output.dense.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.self.query.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.intermediate.dense.weight, embeddings.word_embeddings.weight, encoder.layer.*.output.dense.weight, encoder.layer.*.attention.self.value.weight, encoder.layer.*.attention.output.dense.bias, embeddings.position_embeddings.weight, embeddings.LayerNorm.weight, encoder.layer.*.output.LayerNorm.bias, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, pooler.dense.bias, encoder.layer.*.attention.self.query.bias\n",
      "The following layers were not sharded: embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.LayerNorm.weight, pooler.dense.weight, encoder.layer.*.attention.self.value.bias, encoder.layer.*.attention.self.key.bias, encoder.layer.*.attention.output.LayerNorm.bias, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.output.dense.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.self.query.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.intermediate.dense.weight, embeddings.word_embeddings.weight, encoder.layer.*.output.dense.weight, encoder.layer.*.attention.self.value.weight, encoder.layer.*.attention.output.dense.bias, embeddings.position_embeddings.weight, embeddings.LayerNorm.weight, encoder.layer.*.output.LayerNorm.bias, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, pooler.dense.bias, encoder.layer.*.attention.self.query.bias\n",
      "The following layers were not sharded: embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.LayerNorm.weight, pooler.dense.weight, encoder.layer.*.attention.self.value.bias, encoder.layer.*.attention.self.key.bias, encoder.layer.*.attention.output.LayerNorm.bias, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.output.dense.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.self.query.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.intermediate.dense.weight, embeddings.word_embeddings.weight, encoder.layer.*.output.dense.weight, encoder.layer.*.attention.self.value.weight, encoder.layer.*.attention.output.dense.bias, embeddings.position_embeddings.weight, embeddings.LayerNorm.weight, encoder.layer.*.output.LayerNorm.bias, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, pooler.dense.bias, encoder.layer.*.attention.self.query.bias\n",
      "The following layers were not sharded: embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.LayerNorm.weight, pooler.dense.weight, encoder.layer.*.attention.self.value.bias, encoder.layer.*.attention.self.key.bias, encoder.layer.*.attention.output.LayerNorm.bias, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.output.dense.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.self.query.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.intermediate.dense.weight, embeddings.word_embeddings.weight, encoder.layer.*.output.dense.weight, encoder.layer.*.attention.self.value.weight, encoder.layer.*.attention.output.dense.bias, embeddings.position_embeddings.weight, embeddings.LayerNorm.weight, encoder.layer.*.output.LayerNorm.bias, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, pooler.dense.bias, encoder.layer.*.attention.self.query.bias\n",
      "The following layers were not sharded: embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.LayerNorm.weight, pooler.dense.weight, encoder.layer.*.attention.self.value.bias, encoder.layer.*.attention.self.key.bias, encoder.layer.*.attention.output.LayerNorm.bias, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.output.dense.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.self.query.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.intermediate.dense.weight, embeddings.word_embeddings.weight, encoder.layer.*.output.dense.weight, encoder.layer.*.attention.self.value.weight, encoder.layer.*.attention.output.dense.bias, embeddings.position_embeddings.weight, embeddings.LayerNorm.weight, encoder.layer.*.output.LayerNorm.bias, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, pooler.dense.bias, encoder.layer.*.attention.self.query.bias\n",
      "The following layers were not sharded: embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.LayerNorm.weight, pooler.dense.weight, encoder.layer.*.attention.self.value.bias, encoder.layer.*.attention.self.key.bias, encoder.layer.*.attention.output.LayerNorm.bias, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.output.dense.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.self.query.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.intermediate.dense.weight, embeddings.word_embeddings.weight, encoder.layer.*.output.dense.weight, encoder.layer.*.attention.self.value.weight, encoder.layer.*.attention.output.dense.bias, embeddings.position_embeddings.weight, embeddings.LayerNorm.weight, encoder.layer.*.output.LayerNorm.bias, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, pooler.dense.bias, encoder.layer.*.attention.self.query.bias\n",
      "The following layers were not sharded: embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.LayerNorm.weight, pooler.dense.weight, encoder.layer.*.attention.self.value.bias, encoder.layer.*.attention.self.key.bias, encoder.layer.*.attention.output.LayerNorm.bias, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.output.dense.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.self.query.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.intermediate.dense.weight, embeddings.word_embeddings.weight, encoder.layer.*.output.dense.weight, encoder.layer.*.attention.self.value.weight, encoder.layer.*.attention.output.dense.bias, embeddings.position_embeddings.weight, embeddings.LayerNorm.weight, encoder.layer.*.output.LayerNorm.bias, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, pooler.dense.bias, encoder.layer.*.attention.self.query.bias\n",
      "The following layers were not sharded: embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.LayerNorm.weight, pooler.dense.weight, encoder.layer.*.attention.self.value.bias, encoder.layer.*.attention.self.key.bias, encoder.layer.*.attention.output.LayerNorm.bias, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.output.dense.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.self.query.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.intermediate.dense.weight, embeddings.word_embeddings.weight, encoder.layer.*.output.dense.weight, encoder.layer.*.attention.self.value.weight, encoder.layer.*.attention.output.dense.bias, embeddings.position_embeddings.weight, embeddings.LayerNorm.weight, encoder.layer.*.output.LayerNorm.bias, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, pooler.dense.bias, encoder.layer.*.attention.self.query.bias\n",
      "The following layers were not sharded: embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.LayerNorm.weight, pooler.dense.weight, encoder.layer.*.attention.self.value.bias, encoder.layer.*.attention.self.key.bias, encoder.layer.*.attention.output.LayerNorm.bias, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.output.dense.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.self.query.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.intermediate.dense.weight, embeddings.word_embeddings.weight, encoder.layer.*.output.dense.weight, encoder.layer.*.attention.self.value.weight, encoder.layer.*.attention.output.dense.bias, embeddings.position_embeddings.weight, embeddings.LayerNorm.weight, encoder.layer.*.output.LayerNorm.bias, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, pooler.dense.bias, encoder.layer.*.attention.self.query.bias\n",
      "The following layers were not sharded: embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.LayerNorm.weight, pooler.dense.weight, encoder.layer.*.attention.self.value.bias, encoder.layer.*.attention.self.key.bias, encoder.layer.*.attention.output.LayerNorm.bias, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.output.dense.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.self.query.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.intermediate.dense.weight, embeddings.word_embeddings.weight, encoder.layer.*.output.dense.weight, encoder.layer.*.attention.self.value.weight, encoder.layer.*.attention.output.dense.bias, embeddings.position_embeddings.weight, embeddings.LayerNorm.weight, encoder.layer.*.output.LayerNorm.bias, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, pooler.dense.bias, encoder.layer.*.attention.self.query.bias\n",
      "The following layers were not sharded: embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.LayerNorm.weight, pooler.dense.weight, encoder.layer.*.attention.self.value.bias, encoder.layer.*.attention.self.key.bias, encoder.layer.*.attention.output.LayerNorm.bias, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.output.dense.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.self.query.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.intermediate.dense.weight, embeddings.word_embeddings.weight, encoder.layer.*.output.dense.weight, encoder.layer.*.attention.self.value.weight, encoder.layer.*.attention.output.dense.bias, embeddings.position_embeddings.weight, embeddings.LayerNorm.weight, encoder.layer.*.output.LayerNorm.bias, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, pooler.dense.bias, encoder.layer.*.attention.self.query.bias\n",
      "The following layers were not sharded: embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.LayerNorm.weight, pooler.dense.weight, encoder.layer.*.attention.self.value.bias, encoder.layer.*.attention.self.key.bias, encoder.layer.*.attention.output.LayerNorm.bias, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.output.dense.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.self.query.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.intermediate.dense.weight, embeddings.word_embeddings.weight, encoder.layer.*.output.dense.weight, encoder.layer.*.attention.self.value.weight, encoder.layer.*.attention.output.dense.bias, embeddings.position_embeddings.weight, embeddings.LayerNorm.weight, encoder.layer.*.output.LayerNorm.bias, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, pooler.dense.bias, encoder.layer.*.attention.self.query.bias\n",
      "The following layers were not sharded: embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.LayerNorm.weight, pooler.dense.weight, encoder.layer.*.attention.self.value.bias, encoder.layer.*.attention.self.key.bias, encoder.layer.*.attention.output.LayerNorm.bias, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.output.dense.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.self.query.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.intermediate.dense.weight, embeddings.word_embeddings.weight, encoder.layer.*.output.dense.weight, encoder.layer.*.attention.self.value.weight, encoder.layer.*.attention.output.dense.bias, embeddings.position_embeddings.weight, embeddings.LayerNorm.weight, encoder.layer.*.output.LayerNorm.bias, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, pooler.dense.bias, encoder.layer.*.attention.self.query.bias\n",
      "The following layers were not sharded: embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.LayerNorm.weight, pooler.dense.weight, encoder.layer.*.attention.self.value.bias, encoder.layer.*.attention.self.key.bias, encoder.layer.*.attention.output.LayerNorm.bias, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.output.dense.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.self.query.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.intermediate.dense.weight, embeddings.word_embeddings.weight, encoder.layer.*.output.dense.weight, encoder.layer.*.attention.self.value.weight, encoder.layer.*.attention.output.dense.bias, embeddings.position_embeddings.weight, embeddings.LayerNorm.weight, encoder.layer.*.output.LayerNorm.bias, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, pooler.dense.bias, encoder.layer.*.attention.self.query.bias\n",
      "The following layers were not sharded: embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.LayerNorm.weight, pooler.dense.weight, encoder.layer.*.attention.self.value.bias, encoder.layer.*.attention.self.key.bias, encoder.layer.*.attention.output.LayerNorm.bias, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.output.dense.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.self.query.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.intermediate.dense.weight, embeddings.word_embeddings.weight, encoder.layer.*.output.dense.weight, encoder.layer.*.attention.self.value.weight, encoder.layer.*.attention.output.dense.bias, embeddings.position_embeddings.weight, embeddings.LayerNorm.weight, encoder.layer.*.output.LayerNorm.bias, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, pooler.dense.bias, encoder.layer.*.attention.self.query.bias\n",
      "The following layers were not sharded: embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.LayerNorm.weight, pooler.dense.weight, encoder.layer.*.attention.self.value.bias, encoder.layer.*.attention.self.key.bias, encoder.layer.*.attention.output.LayerNorm.bias, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.output.dense.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.self.query.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.intermediate.dense.weight, embeddings.word_embeddings.weight, encoder.layer.*.output.dense.weight, encoder.layer.*.attention.self.value.weight, encoder.layer.*.attention.output.dense.bias, embeddings.position_embeddings.weight, embeddings.LayerNorm.weight, encoder.layer.*.output.LayerNorm.bias, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, pooler.dense.bias, encoder.layer.*.attention.self.query.bias\n",
      "The following layers were not sharded: embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.LayerNorm.weight, pooler.dense.weight, encoder.layer.*.attention.self.value.bias, encoder.layer.*.attention.self.key.bias, encoder.layer.*.attention.output.LayerNorm.bias, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.output.dense.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.self.query.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.intermediate.dense.weight, embeddings.word_embeddings.weight, encoder.layer.*.output.dense.weight, encoder.layer.*.attention.self.value.weight, encoder.layer.*.attention.output.dense.bias, embeddings.position_embeddings.weight, embeddings.LayerNorm.weight, encoder.layer.*.output.LayerNorm.bias, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, pooler.dense.bias, encoder.layer.*.attention.self.query.bias\n",
      "The following layers were not sharded: embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.LayerNorm.weight, pooler.dense.weight, encoder.layer.*.attention.self.value.bias, encoder.layer.*.attention.self.key.bias, encoder.layer.*.attention.output.LayerNorm.bias, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.output.dense.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.self.query.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.intermediate.dense.weight, embeddings.word_embeddings.weight, encoder.layer.*.output.dense.weight, encoder.layer.*.attention.self.value.weight, encoder.layer.*.attention.output.dense.bias, embeddings.position_embeddings.weight, embeddings.LayerNorm.weight, encoder.layer.*.output.LayerNorm.bias, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, pooler.dense.bias, encoder.layer.*.attention.self.query.bias\n",
      "The following layers were not sharded: embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.LayerNorm.weight, pooler.dense.weight, encoder.layer.*.attention.self.value.bias, encoder.layer.*.attention.self.key.bias, encoder.layer.*.attention.output.LayerNorm.bias, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.output.dense.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.self.query.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.intermediate.dense.weight, embeddings.word_embeddings.weight, encoder.layer.*.output.dense.weight, encoder.layer.*.attention.self.value.weight, encoder.layer.*.attention.output.dense.bias, embeddings.position_embeddings.weight, embeddings.LayerNorm.weight, encoder.layer.*.output.LayerNorm.bias, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, pooler.dense.bias, encoder.layer.*.attention.self.query.bias\n",
      "The following layers were not sharded: embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.LayerNorm.weight, pooler.dense.weight, encoder.layer.*.attention.self.value.bias, encoder.layer.*.attention.self.key.bias, encoder.layer.*.attention.output.LayerNorm.bias, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.output.dense.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.self.query.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.intermediate.dense.weight, embeddings.word_embeddings.weight, encoder.layer.*.output.dense.weight, encoder.layer.*.attention.self.value.weight, encoder.layer.*.attention.output.dense.bias, embeddings.position_embeddings.weight, embeddings.LayerNorm.weight, encoder.layer.*.output.LayerNorm.bias, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, pooler.dense.bias, encoder.layer.*.attention.self.query.bias\n",
      "The following layers were not sharded: embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.LayerNorm.weight, pooler.dense.weight, encoder.layer.*.attention.self.value.bias, encoder.layer.*.attention.self.key.bias, encoder.layer.*.attention.output.LayerNorm.bias, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.output.dense.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.self.query.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.intermediate.dense.weight, embeddings.word_embeddings.weight, encoder.layer.*.output.dense.weight, encoder.layer.*.attention.self.value.weight, encoder.layer.*.attention.output.dense.bias, embeddings.position_embeddings.weight, embeddings.LayerNorm.weight, encoder.layer.*.output.LayerNorm.bias, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, pooler.dense.bias, encoder.layer.*.attention.self.query.bias\n",
      "The following layers were not sharded: embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.LayerNorm.weight, pooler.dense.weight, encoder.layer.*.attention.self.value.bias, encoder.layer.*.attention.self.key.bias, encoder.layer.*.attention.output.LayerNorm.bias, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.output.dense.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.self.query.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.intermediate.dense.weight, embeddings.word_embeddings.weight, encoder.layer.*.output.dense.weight, encoder.layer.*.attention.self.value.weight, encoder.layer.*.attention.output.dense.bias, embeddings.position_embeddings.weight, embeddings.LayerNorm.weight, encoder.layer.*.output.LayerNorm.bias, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, pooler.dense.bias, encoder.layer.*.attention.self.query.bias\n",
      "The following layers were not sharded: embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.LayerNorm.weight, pooler.dense.weight, encoder.layer.*.attention.self.value.bias, encoder.layer.*.attention.self.key.bias, encoder.layer.*.attention.output.LayerNorm.bias, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.output.dense.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.self.query.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.intermediate.dense.weight, embeddings.word_embeddings.weight, encoder.layer.*.output.dense.weight, encoder.layer.*.attention.self.value.weight, encoder.layer.*.attention.output.dense.bias, embeddings.position_embeddings.weight, embeddings.LayerNorm.weight, encoder.layer.*.output.LayerNorm.bias, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, pooler.dense.bias, encoder.layer.*.attention.self.query.bias\n",
      "The following layers were not sharded: embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.LayerNorm.weight, pooler.dense.weight, encoder.layer.*.attention.self.value.bias, encoder.layer.*.attention.self.key.bias, encoder.layer.*.attention.output.LayerNorm.bias, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.output.dense.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.self.query.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.intermediate.dense.weight, embeddings.word_embeddings.weight, encoder.layer.*.output.dense.weight, encoder.layer.*.attention.self.value.weight, encoder.layer.*.attention.output.dense.bias, embeddings.position_embeddings.weight, embeddings.LayerNorm.weight, encoder.layer.*.output.LayerNorm.bias, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, pooler.dense.bias, encoder.layer.*.attention.self.query.bias\n",
      "The following layers were not sharded: embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.LayerNorm.weight, pooler.dense.weight, encoder.layer.*.attention.self.value.bias, encoder.layer.*.attention.self.key.bias, encoder.layer.*.attention.output.LayerNorm.bias, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.output.dense.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.self.query.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.intermediate.dense.weight, embeddings.word_embeddings.weight, encoder.layer.*.output.dense.weight, encoder.layer.*.attention.self.value.weight, encoder.layer.*.attention.output.dense.bias, embeddings.position_embeddings.weight, embeddings.LayerNorm.weight, encoder.layer.*.output.LayerNorm.bias, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, pooler.dense.bias, encoder.layer.*.attention.self.query.bias\n",
      "The following layers were not sharded: embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.LayerNorm.weight, pooler.dense.weight, encoder.layer.*.attention.self.value.bias, encoder.layer.*.attention.self.key.bias, encoder.layer.*.attention.output.LayerNorm.bias, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.output.dense.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.self.query.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.intermediate.dense.weight, embeddings.word_embeddings.weight, encoder.layer.*.output.dense.weight, encoder.layer.*.attention.self.value.weight, encoder.layer.*.attention.output.dense.bias, embeddings.position_embeddings.weight, embeddings.LayerNorm.weight, encoder.layer.*.output.LayerNorm.bias, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, pooler.dense.bias, encoder.layer.*.attention.self.query.bias\n",
      "The following layers were not sharded: embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.LayerNorm.weight, pooler.dense.weight, encoder.layer.*.attention.self.value.bias, encoder.layer.*.attention.self.key.bias, encoder.layer.*.attention.output.LayerNorm.bias, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.output.dense.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.self.query.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.intermediate.dense.weight, embeddings.word_embeddings.weight, encoder.layer.*.output.dense.weight, encoder.layer.*.attention.self.value.weight, encoder.layer.*.attention.output.dense.bias, embeddings.position_embeddings.weight, embeddings.LayerNorm.weight, encoder.layer.*.output.LayerNorm.bias, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, pooler.dense.bias, encoder.layer.*.attention.self.query.bias\n",
      "The following layers were not sharded: embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.LayerNorm.weight, pooler.dense.weight, encoder.layer.*.attention.self.value.bias, encoder.layer.*.attention.self.key.bias, encoder.layer.*.attention.output.LayerNorm.bias, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.output.dense.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.self.query.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.intermediate.dense.weight, embeddings.word_embeddings.weight, encoder.layer.*.output.dense.weight, encoder.layer.*.attention.self.value.weight, encoder.layer.*.attention.output.dense.bias, embeddings.position_embeddings.weight, embeddings.LayerNorm.weight, encoder.layer.*.output.LayerNorm.bias, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, pooler.dense.bias, encoder.layer.*.attention.self.query.bias\n",
      "The following layers were not sharded: embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.LayerNorm.weight, pooler.dense.weight, encoder.layer.*.attention.self.value.bias, encoder.layer.*.attention.self.key.bias, encoder.layer.*.attention.output.LayerNorm.bias, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.output.dense.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.self.query.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.intermediate.dense.weight, embeddings.word_embeddings.weight, encoder.layer.*.output.dense.weight, encoder.layer.*.attention.self.value.weight, encoder.layer.*.attention.output.dense.bias, embeddings.position_embeddings.weight, embeddings.LayerNorm.weight, encoder.layer.*.output.LayerNorm.bias, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, pooler.dense.bias, encoder.layer.*.attention.self.query.bias\n",
      "The following layers were not sharded: embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.LayerNorm.weight, pooler.dense.weight, encoder.layer.*.attention.self.value.bias, encoder.layer.*.attention.self.key.bias, encoder.layer.*.attention.output.LayerNorm.bias, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.output.dense.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.self.query.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.intermediate.dense.weight, embeddings.word_embeddings.weight, encoder.layer.*.output.dense.weight, encoder.layer.*.attention.self.value.weight, encoder.layer.*.attention.output.dense.bias, embeddings.position_embeddings.weight, embeddings.LayerNorm.weight, encoder.layer.*.output.LayerNorm.bias, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, pooler.dense.bias, encoder.layer.*.attention.self.query.bias\n",
      "The following layers were not sharded: embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.LayerNorm.weight, pooler.dense.weight, encoder.layer.*.attention.self.value.bias, encoder.layer.*.attention.self.key.bias, encoder.layer.*.attention.output.LayerNorm.bias, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.output.dense.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.self.query.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.intermediate.dense.weight, embeddings.word_embeddings.weight, encoder.layer.*.output.dense.weight, encoder.layer.*.attention.self.value.weight, encoder.layer.*.attention.output.dense.bias, embeddings.position_embeddings.weight, embeddings.LayerNorm.weight, encoder.layer.*.output.LayerNorm.bias, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, pooler.dense.bias, encoder.layer.*.attention.self.query.bias\n",
      "The following layers were not sharded: embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.LayerNorm.weight, pooler.dense.weight, encoder.layer.*.attention.self.value.bias, encoder.layer.*.attention.self.key.bias, encoder.layer.*.attention.output.LayerNorm.bias, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.output.dense.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.self.query.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.intermediate.dense.weight, embeddings.word_embeddings.weight, encoder.layer.*.output.dense.weight, encoder.layer.*.attention.self.value.weight, encoder.layer.*.attention.output.dense.bias, embeddings.position_embeddings.weight, embeddings.LayerNorm.weight, encoder.layer.*.output.LayerNorm.bias, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, pooler.dense.bias, encoder.layer.*.attention.self.query.bias\n",
      "The following layers were not sharded: embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.LayerNorm.weight, pooler.dense.weight, encoder.layer.*.attention.self.value.bias, encoder.layer.*.attention.self.key.bias, encoder.layer.*.attention.output.LayerNorm.bias, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.output.dense.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.self.query.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.intermediate.dense.weight, embeddings.word_embeddings.weight, encoder.layer.*.output.dense.weight, encoder.layer.*.attention.self.value.weight, encoder.layer.*.attention.output.dense.bias, embeddings.position_embeddings.weight, embeddings.LayerNorm.weight, encoder.layer.*.output.LayerNorm.bias, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, pooler.dense.bias, encoder.layer.*.attention.self.query.bias\n",
      "The following layers were not sharded: embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.LayerNorm.weight, pooler.dense.weight, encoder.layer.*.attention.self.value.bias, encoder.layer.*.attention.self.key.bias, encoder.layer.*.attention.output.LayerNorm.bias, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.output.dense.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.self.query.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.intermediate.dense.weight, embeddings.word_embeddings.weight, encoder.layer.*.output.dense.weight, encoder.layer.*.attention.self.value.weight, encoder.layer.*.attention.output.dense.bias, embeddings.position_embeddings.weight, embeddings.LayerNorm.weight, encoder.layer.*.output.LayerNorm.bias, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, pooler.dense.bias, encoder.layer.*.attention.self.query.bias\n",
      "The following layers were not sharded: embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.LayerNorm.weight, pooler.dense.weight, encoder.layer.*.attention.self.value.bias, encoder.layer.*.attention.self.key.bias, encoder.layer.*.attention.output.LayerNorm.bias, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.output.dense.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.self.query.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.intermediate.dense.weight, embeddings.word_embeddings.weight, encoder.layer.*.output.dense.weight, encoder.layer.*.attention.self.value.weight, encoder.layer.*.attention.output.dense.bias, embeddings.position_embeddings.weight, embeddings.LayerNorm.weight, encoder.layer.*.output.LayerNorm.bias, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, pooler.dense.bias, encoder.layer.*.attention.self.query.bias\n",
      "The following layers were not sharded: embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.LayerNorm.weight, pooler.dense.weight, encoder.layer.*.attention.self.value.bias, encoder.layer.*.attention.self.key.bias, encoder.layer.*.attention.output.LayerNorm.bias, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.output.dense.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.self.query.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.intermediate.dense.weight, embeddings.word_embeddings.weight, encoder.layer.*.output.dense.weight, encoder.layer.*.attention.self.value.weight, encoder.layer.*.attention.output.dense.bias, embeddings.position_embeddings.weight, embeddings.LayerNorm.weight, encoder.layer.*.output.LayerNorm.bias, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, pooler.dense.bias, encoder.layer.*.attention.self.query.bias\n",
      "The following layers were not sharded: embeddings.token_type_embeddings.weight, encoder.layer.*.attention.output.LayerNorm.weight, pooler.dense.weight, encoder.layer.*.attention.self.value.bias, encoder.layer.*.attention.self.key.bias, encoder.layer.*.attention.output.LayerNorm.bias, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.output.dense.weight, embeddings.LayerNorm.bias, encoder.layer.*.attention.self.query.weight, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.intermediate.dense.weight, embeddings.word_embeddings.weight, encoder.layer.*.output.dense.weight, encoder.layer.*.attention.self.value.weight, encoder.layer.*.attention.output.dense.bias, embeddings.position_embeddings.weight, embeddings.LayerNorm.weight, encoder.layer.*.output.LayerNorm.bias, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.key.weight, pooler.dense.bias, encoder.layer.*.attention.self.query.bias\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "         --- Detalles de Respuestas Generadas (DataFrame Completo) ---          \n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import warnings\n",
    "import logging\n",
    "import torch\n",
    "import textstat\n",
    "import nltk\n",
    "import time\n",
    "import re\n",
    "import yaml # Importamos PyYAML\n",
    "\n",
    "# Importaciones específicas de Code 1 para LLMs y Azure\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from openai import AzureOpenAI\n",
    "import google.generativeai as genai\n",
    "from bert_score import score as bert_score\n",
    "from google.generativeai.types import GenerationConfig\n",
    "\n",
    "# --- Cargar configuración desde YAML ---\n",
    "try:\n",
    "    with open('config.yaml', 'r') as file:\n",
    "        config = yaml.safe_load(file)\n",
    "    print(\"Configuración cargada exitosamente desde config.yaml\")\n",
    "except FileNotFoundError:\n",
    "    print(\"Error: 'config.yaml' no encontrado. Asegúrate de que el archivo exista en el mismo directorio.\")\n",
    "    exit()\n",
    "except yaml.YAMLError as e:\n",
    "    print(f\"Error al parsear 'config.yaml': {e}\")\n",
    "    exit()\n",
    "\n",
    "# Temperatura del asistente de creación de preguntas y respuestas\n",
    "MD_GEN = \"GEMINI\"\n",
    "modelo_banco_generacion = \"models/gemini-1.5-pro-latest\"\n",
    "temperature_gen_assitant = 0.7\n",
    "\n",
    "# Asignar prompts y parámetros desde el archivo de configuración\n",
    "prompt_systemE1 = config['prompts']['system_prompt_E1']\n",
    "prompt_systemE2 = config['prompts']['system_prompt_E2']\n",
    "prompt_generacion_qa = config['prompts']['qa_generation_prompt']\n",
    "\n",
    "# Parámetros desde el YAML\n",
    "num_pares_a_generar = config['parameters']['num_qa_to_generate']\n",
    "chunk_text = config['parameters']['chunk_size']\n",
    "chunk_over = config['parameters']['chunk_overlap']\n",
    "top_join = config['parameters']['top_join_chunks'] # Este es el 'top_join' para la búsqueda semántica\n",
    "max_attempts = config['parameters']['max_api_attempts']\n",
    "wait_time_on_error_s = config['parameters']['api_wait_time_on_error_s']\n",
    "temperature_ = config['parameters']['gpt_temperature']\n",
    "temperature_2 = config['parameters']['gemma_temperature']\n",
    "perfil_usuario = config['parameters']['user_profile']\n",
    "contexto_generico = config['parameters']['generic_context']\n",
    "\n",
    "# Umbrales desde el YAML\n",
    "umbrales = [\n",
    "    config['thresholds']['similitud_min'],\n",
    "    config['thresholds']['coherencia_min'],\n",
    "    config['thresholds']['personalizacion_min'],\n",
    "    config['thresholds']['fluidez_min'],\n",
    "    config['thresholds']['alucinacion_min']\n",
    "]\n",
    "\n",
    "#################################################\n",
    "############## Modelos a Comparar ###############\n",
    "#################################################\n",
    "\n",
    "#################### modelo 1 ###################\n",
    "model_embedding = \"text-embedding-ada-002\"\n",
    "#modelo_banco = \"gpt-35-turbo-16k-PQR\"\n",
    "modelo_banco = \"gpt-4o-mini_clasificacion-PQRS\"\n",
    "\n",
    "################### modelo 2 ####################\n",
    "model_embedding2 = \"models/text-embedding-004\"\n",
    "#modelo_banco2 = \"models/gemma-3-27b-it\"\n",
    "modelo_banco2 = \"models/gemini-2.5-flash\"\n",
    "\n",
    "# --- Configuración inicial ---\n",
    "nltk.download('punkt', quiet=True)\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "logging.getLogger(\"transformers.modeling_utils\").setLevel(logging.ERROR)\n",
    "logging.getLogger(\"transformers.configuration_utils\").setLevel(logging.ERROR)\n",
    "logging.getLogger(\"transformers.tokenization_utils_base\").setLevel(logging.ERROR)\n",
    "\n",
    "def medir_coherencia(pregunta, respuesta):\n",
    "    \"\"\"Mide la coherencia entre una pregunta y su respuesta utilizando BERTScore.\"\"\"\n",
    "    if not respuesta.strip() or not pregunta.strip():\n",
    "        return 0.0\n",
    "    # Utiliza 'en' como idioma para BERTScore con 'roberta-base'\n",
    "    P, R, F1 = bert_score([respuesta], [pregunta], lang=\"en\", model_type=\"roberta-base\", rescale_with_baseline=False)\n",
    "    return float(F1[0])\n",
    "\n",
    "def medir_personalizacion(perfil_usuario, respuesta):\n",
    "    \"\"\"Mide qué tan personalizada es una respuesta en relación con un perfil de usuario.\"\"\"\n",
    "    if not respuesta.strip() or not perfil_usuario.strip():\n",
    "        return 0.0\n",
    "    P, R, F1 = bert_score([respuesta], [perfil_usuario], lang=\"en\", model_type=\"roberta-base\", rescale_with_baseline=False)\n",
    "    return float(F1[0])\n",
    "\n",
    "def medir_fluidez(texto):\n",
    "    \"\"\"Mide la fluidez de un texto utilizando el índice de facilidad de lectura de Flesch.\"\"\"\n",
    "    if not texto.strip():\n",
    "        return 0.0\n",
    "    return textstat.flesch_reading_ease(texto)\n",
    "\n",
    "def medir_alucinacion(respuesta, contexto_fuente):\n",
    "    \"\"\"Mide la alucinación comparando la respuesta con un contexto fuente (usando BERTScore).\"\"\"\n",
    "    if not respuesta.strip() or not contexto_fuente.strip():\n",
    "        return 0.0\n",
    "    P, R, F1 = bert_score([respuesta], [contexto_fuente], lang=\"en\", model_type=\"roberta-base\", rescale_with_baseline=False)\n",
    "    return float(F1[0])\n",
    "\n",
    "############## Modelo 1 (Azure OpenAI) #################\n",
    "# 1. Cargar credenciales desde JSON\n",
    "def cargar_credenciales(ruta_credenciales):\n",
    "    with open(ruta_credenciales, \"r\") as file:\n",
    "        return json.load(file)\n",
    "\n",
    "# 2. Crear cliente AzureOpenAI\n",
    "def crear_cliente_azure(creds):\n",
    "    client = AzureOpenAI(\n",
    "        api_key=creds[\"AZURE_API_KEY\"],\n",
    "        api_version=creds[\"AZURE_API_VERSION\"],\n",
    "        azure_endpoint=creds[\"AZURE_ENDPOINT\"]\n",
    "    )\n",
    "    return client\n",
    "\n",
    "# 3. Generar embeddings usando cliente (AzureOpenAI)\n",
    "def text_embedding(text):\n",
    "    input_text = [text] if isinstance(text, str) else text\n",
    "    try:\n",
    "        embeddings = client.embeddings.create(model=model_embedding,\n",
    "                                              input=input_text,\n",
    "                                              encoding_format=\"float\")\n",
    "        return embeddings.data[0].embedding\n",
    "    except Exception as e:\n",
    "        print(f\"Error al generar embedding con Azure: {e}\")\n",
    "        return np.zeros(1536) # Devuelve un array de ceros en caso de error\n",
    "\n",
    "# 4. Inicialización Global del Cliente Azure y Credenciales\n",
    "try:\n",
    "    ruta_credenciales = \"credentials.json\"\n",
    "    creds = cargar_credenciales(ruta_credenciales)\n",
    "    client = crear_cliente_azure(creds)\n",
    "    print(\"Cliente Azure OpenAI inicializado.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error al inicializar cliente Azure OpenAI: {e}\")\n",
    "    client = None # Asegurarse de que el cliente es None si falla la inicialización\n",
    "\n",
    "############## Modelo 2 (Gemini/Gemma) ################\n",
    "def cargar_credenciales2(ruta_credenciales):\n",
    "    with open(ruta_credenciales, \"r\") as file:\n",
    "        return json.load(file)\n",
    "\n",
    "try:\n",
    "    ruta_credenciales2 = \"credentials2.json\"\n",
    "    creds2 = cargar_credenciales2(ruta_credenciales2)\n",
    "    genai.configure(api_key=creds2[\"GEMINI_API_KEY\"])\n",
    "    print(\"Cliente Gemini/Gemma configurado.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error al configurar cliente Gemini/Gemma: {e}\")\n",
    "    genai = None # Asegurarse de que genai es None si falla la configuración\n",
    "\n",
    "def text_embedding_gemma(text):\n",
    "    if genai is None:\n",
    "        print(\"Error: El cliente Gemini/Gemma no está configurado para embeddings.\")\n",
    "        return np.zeros(768)\n",
    "    try:\n",
    "        result = genai.embed_content(model=model_embedding2,\n",
    "                                     content=text,\n",
    "                                     task_type=\"RETRIEVAL_QUERY\") # O RETRIEVAL_DOCUMENT\n",
    "        return result['embedding']\n",
    "    except Exception as e:\n",
    "        print(f\"Error al generar embedding con Gemma: {e}\")\n",
    "        return np.zeros(768) # Devuelve un array de ceros en caso de error\n",
    "\n",
    "# --- Carga y chunking del PDF ---\n",
    "print(\"Cargando y procesando PDF...\")\n",
    "try:\n",
    "    loader = PyPDFLoader(\"llm_doc.pdf\")\n",
    "    documents = loader.load()\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_text, chunk_overlap=chunk_over)\n",
    "    doc_splits = text_splitter.split_documents(documents)\n",
    "\n",
    "    data = [{'Chunks': doc.page_content, 'Metadata': doc.metadata} for doc in doc_splits]\n",
    "    df_vector_store = pd.DataFrame(data)\n",
    "\n",
    "    df_vector_store_gpt = df_vector_store.copy()\n",
    "    df_vector_store_gem = df_vector_store.copy()\n",
    "\n",
    "    # Generar embeddings para GPT\n",
    "    if client:\n",
    "        print(\"Generando embeddings para GPT (Azure)...\")\n",
    "        df_vector_store_gpt[\"Embedding\"] = df_vector_store_gpt[\"Chunks\"].apply(lambda x: text_embedding(x))\n",
    "        df_vector_store_gpt[\"Embedding\"] = df_vector_store_gpt[\"Embedding\"].apply(np.array)\n",
    "    else:\n",
    "        print(\"Saltando embeddings para GPT, cliente Azure no inicializado.\")\n",
    "        df_vector_store_gpt[\"Embedding\"] = [np.zeros(1536)] * len(df_vector_store_gpt)\n",
    "\n",
    "    # Generar embeddings para Gemma\n",
    "    if genai:\n",
    "        print(\"Generando embeddings para Gemma...\")\n",
    "        df_vector_store_gem[\"Embedding\"] = df_vector_store_gem[\"Chunks\"].apply(text_embedding_gemma)\n",
    "        df_vector_store_gem[\"Embedding\"] = df_vector_store_gem[\"Embedding\"].apply(np.array)\n",
    "    else:\n",
    "        print(\"Saltando embeddings para Gemma, cliente Gemini/Gemma no configurado.\")\n",
    "        df_vector_store_gem[\"Embedding\"] = [np.zeros(768)] * len(df_vector_store_gem)\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error durante la carga y procesamiento del PDF: {e}\")\n",
    "    exit()\n",
    "\n",
    "# --- Cargar preguntas_BOT.xlsx para evitar duplicados (si existe) ---\n",
    "try:\n",
    "    df_preguntas_respuestas_existentes = pd.read_excel('preguntas_BOT.xlsx')\n",
    "except FileNotFoundError:\n",
    "    print(\"Advertencia: 'preguntas_BOT.xlsx' no encontrado. Se asumirá que no hay preguntas existentes.\")\n",
    "    df_preguntas_respuestas_existentes = pd.DataFrame(columns=[\"Pregunta\", \"Respuesta\"])\n",
    "\n",
    "# --- FUNCIÓN PARA GENERAR UN CONTEXTO A PARTIR DE CHUNKS SELECCIONADOS ---\n",
    "def obtener_contexto_para_generacion(df_vector_store, num_chunks=5):\n",
    "    if len(df_vector_store) < num_chunks:\n",
    "        # Si no hay suficientes chunks, usa todos los disponibles\n",
    "        return \"\\n\".join(df_vector_store['Chunks'].tolist())\n",
    "    else:\n",
    "        # Distribuye los chunks equitativamente\n",
    "        indices = np.linspace(0, len(df_vector_store) - 1, num_chunks, dtype=int)\n",
    "        selected_chunks = df_vector_store.loc[indices, 'Chunks'].tolist()\n",
    "        return \"\\n\".join(selected_chunks)\n",
    "\n",
    "# --- GENERACIÓN DE NUEVAS PREGUNTAS Y RESPUESTAS ---\n",
    "nuevos_pares_qa = []\n",
    "\n",
    "# Corregido: Usar 'top_join' para la generación de QA, que ahora proviene del YAML\n",
    "contexto_para_generacion = obtener_contexto_para_generacion(df_vector_store_gem, \n",
    "                                                            num_chunks=top_join)\n",
    "\n",
    "# Formatear el prompt de generación con el número deseado (ahora `num_pares_a_generar` del YAML)\n",
    "final_prompt_qa = prompt_generacion_qa.format(\n",
    "    source_text=contexto_para_generacion,\n",
    "    num_to_generate=num_pares_a_generar\n",
    ")\n",
    "\n",
    "print(f\"Enviando solicitud al LLM para generar {num_pares_a_generar} pares QA...\")\n",
    "try:\n",
    "    if client: # Solo intentar si el cliente Azure está inicializado\n",
    "        if MD_GEN == \"GPT\":\n",
    "            completion_qa = client.chat.completions.create(\n",
    "            model=modelo_banco_generacion,\n",
    "            temperature=temperature_gen_assitant, # Usa la temperatura de GPT del YAML\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": final_prompt_qa}\n",
    "            ]\n",
    "            )\n",
    "            generated_text = completion_qa.choices[0].message.content\n",
    "        else:\n",
    "            model_2 = genai.GenerativeModel(model_name=modelo_banco_generacion)\n",
    "            full_query_for_gemma = final_prompt_qa\n",
    "            completion2 = model_2.generate_content(\n",
    "            contents=full_query_for_gemma,\n",
    "            generation_config=GenerationConfig(temperature=temperature_gen_assitant)\n",
    "            )\n",
    "            generated_text = completion2.text\n",
    "\n",
    "\n",
    "        print(\"Generación del LLM completada. Procesando pares QA...\")\n",
    "\n",
    "        # --- PARSEO DE LAS RESPUESTAS GENERADAS CON REGEX (PARA MÚLTIPLES PARES) ---\n",
    "        pattern = re.compile(r\"Pregunta:\\s*(.*?)\\s*Respuesta:\\s*(.*?)(?=(?:Pregunta:|$))\", re.DOTALL | re.IGNORECASE)\n",
    "        matches = pattern.findall(generated_text)\n",
    "\n",
    "        for match in matches:\n",
    "            question = match[0].strip()\n",
    "            answer = match[1].strip()\n",
    "\n",
    "            if question and answer: # Asegurarse de que no estén vacíos\n",
    "                # Validar si ya existe para evitar duplicados\n",
    "                if not df_preguntas_respuestas_existentes['Pregunta'].astype(str).str.contains(question, case=False, regex=False).any():\n",
    "                    nuevos_pares_qa.append({\"Pregunta\": question, \"Respuesta\": answer})\n",
    "                else:\n",
    "                    print(f\"  [QA Gen] Par '{question[:30]}...' ya existe en 'preguntas_BOT.xlsx', omitiendo.\")\n",
    "\n",
    "                if len(nuevos_pares_qa) >= num_pares_a_generar:\n",
    "                    break # Detener si ya generamos la cantidad deseada\n",
    "\n",
    "    else:\n",
    "        print(\"Error: El cliente Azure OpenAI no está inicializado. No se pudo generar QA.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error durante la generación o procesamiento de QA: {e}\")\n",
    "\n",
    "# Asegurarse de tener a lo sumo num_pares_a_generar\n",
    "if len(nuevos_pares_qa) > num_pares_a_generar:\n",
    "    nuevos_pares_qa = nuevos_pares_qa[:num_pares_a_generar]\n",
    "elif len(nuevos_pares_qa) < num_pares_a_generar:\n",
    "    print(f\"Advertencia: Solo se pudieron generar {len(nuevos_pares_qa)} pares QA válidos de {num_pares_a_generar} solicitados.\")\n",
    "\n",
    "df_nuevos_qa = pd.DataFrame(nuevos_pares_qa)\n",
    "\n",
    "if df_nuevos_qa.empty:\n",
    "    print(\"No se pudieron generar pares de preguntas y respuestas válidos. Finalizando.\")\n",
    "    exit()\n",
    "\n",
    "def cosine_similarity_for_chunks(row_embedding_chunk, query_vector):\n",
    "    denominator1 = np.linalg.norm(row_embedding_chunk)\n",
    "    denominator2 = np.linalg.norm(query_vector)\n",
    "    dot_prod = np.dot(row_embedding_chunk, query_vector)\n",
    "    if denominator1 == 0 or denominator2 == 0:\n",
    "        return 0.0\n",
    "    return dot_prod / (denominator1 * denominator2)\n",
    "\n",
    "def cosine_final(v1, v2):\n",
    "    norm_v1 = np.linalg.norm(v1)\n",
    "    norm_v2 = np.linalg.norm(v2)\n",
    "    if norm_v1 == 0 or norm_v2 == 0:\n",
    "        return 0.0\n",
    "    return np.dot(v1, v2) / (norm_v1 * norm_v2)\n",
    "\n",
    "def get_context_from_query(query, vector_store, n_chunks, embedding_func_for_query):\n",
    "    # Usar la función de embedding pasada para generar el embedding de la consulta\n",
    "    query_embedding_val = embedding_func_for_query(query)\n",
    "    query_vector = np.array(query_embedding_val)\n",
    "\n",
    "    # Calcular la similitud de coseno\n",
    "    # La función lambda debe capturar query_vector correctamente\n",
    "    top_matched_indices = (\n",
    "        vector_store[\"Embedding\"]\n",
    "        .apply(lambda x: cosine_similarity_for_chunks(x, query_vector))\n",
    "        .sort_values(ascending=False)[:n_chunks]\n",
    "        .index\n",
    "    )\n",
    "    top_matched_df = vector_store[vector_store.index.isin(top_matched_indices)][[\"Chunks\"]]\n",
    "    return list(top_matched_df['Chunks'])\n",
    "\n",
    "resultados, resultados2 = [], []\n",
    "df_respuestas_detalladas = [] # Lista para almacenar los detalles de todas las respuestas\n",
    "\n",
    "# Los prompts ahora se cargan desde el YAML, pero se usan en un bucle para la evaluación\n",
    "prompts_evaluacion = [prompt_systemE1, prompt_systemE2]\n",
    "# Nombres de los prompts para identificarlos en el DataFrame de resultados\n",
    "prompt_names = [\"prompt_systemE1\", \"prompt_systemE2\"]\n",
    "\n",
    "for idx, prompt_system in enumerate(prompts_evaluacion):\n",
    "    current_prompt_name = prompt_names[idx] # Obtener el nombre del prompt actual\n",
    "\n",
    "    for i, row in df_nuevos_qa.iterrows():\n",
    "        pregunta = str(row[\"Pregunta\"])\n",
    "        respuesta_humana = str(row[\"Respuesta\"])\n",
    "\n",
    "        # --- Evaluación para GPT (Modelo 1) ---\n",
    "        if client: # Solo ejecutar si el cliente Azure está inicializado\n",
    "            respuesta_modelo_gpt = \"No se pudo generar respuesta.\"\n",
    "            input_tokens_gpt = 0\n",
    "            output_tokens_gpt = 0\n",
    "            api_latency_gpt = 0\n",
    "            total_exec_time_gpt = 0\n",
    "            successful_gpt_response = False\n",
    "            contexto_gpt_usado = \"\"\n",
    "\n",
    "            for attempt in range(max_attempts):\n",
    "                try:\n",
    "                    start_time_total = time.time()\n",
    "                    context_chunks_gpt = get_context_from_query(pregunta, df_vector_store_gpt, top_join, text_embedding)\n",
    "                    contexto_gpt_usado = \" \".join(context_chunks_gpt) # Para alucinación\n",
    "                    custom_prompt_gpt = prompt_system.format(source=str(context_chunks_gpt))\n",
    "\n",
    "                    llm_start_time = time.time()\n",
    "                    completion = client.chat.completions.create(\n",
    "                        model=modelo_banco,\n",
    "                        temperature=temperature_,\n",
    "                        messages=[\n",
    "                            {\"role\": \"system\", \"content\": custom_prompt_gpt},\n",
    "                            {\"role\": \"user\", \"content\": pregunta}\n",
    "                        ]\n",
    "                    )\n",
    "                    respuesta_modelo_gpt = completion.choices[0].message.content\n",
    "                    llm_end_time = time.time()\n",
    "\n",
    "                    input_tokens_gpt = completion.usage.prompt_tokens\n",
    "                    output_tokens_gpt = completion.usage.completion_tokens\n",
    "                    api_latency_gpt = llm_end_time - llm_start_time\n",
    "                    total_exec_time_gpt = time.time() - start_time_total\n",
    "                    successful_gpt_response = True\n",
    "                    break # Salir del bucle de reintentos si fue exitoso\n",
    "                except Exception as e:\n",
    "                    print(f\"    [GPT] Error en intento {attempt + 1}/{max_attempts} para '{pregunta[:50]}...': {e}\")\n",
    "                    if \"quota\" in str(e).lower() or \"rate limit\" in str(e).lower():\n",
    "                        print(f\"    [GPT] Error de cuota. Esperando {wait_time_on_error_s}s...\")\n",
    "                        time.sleep(wait_time_on_error_s)\n",
    "                    else:\n",
    "                        break # Otros errores no reintentables\n",
    "\n",
    "            # Calcular métricas para GPT\n",
    "            similitud_gpt = cosine_final(np.array(text_embedding(respuesta_modelo_gpt)), np.array(text_embedding(respuesta_humana))) if successful_gpt_response else 0.0\n",
    "            coherencia_gpt = medir_coherencia(pregunta, respuesta_modelo_gpt) if successful_gpt_response else 0.0\n",
    "            personalizacion_gpt = medir_personalizacion(perfil_usuario, respuesta_modelo_gpt) if successful_gpt_response else 0.0\n",
    "            fluidez_gpt = medir_fluidez(respuesta_modelo_gpt) if successful_gpt_response else 0.0\n",
    "            alucinacion_gpt = medir_alucinacion(respuesta_modelo_gpt, contexto_gpt_usado) if successful_gpt_response else 0.0\n",
    "\n",
    "            df_respuestas_detalladas.append({\n",
    "                \"Modelo\": modelo_banco,\n",
    "                \"Prompt_Tipo\": current_prompt_name, # Usamos el nombre del prompt\n",
    "                \"Pregunta\": pregunta,\n",
    "                \"Respuesta_Humana\": respuesta_humana,\n",
    "                \"Respuesta_Modelo\": respuesta_modelo_gpt,\n",
    "                \"Similitud_Coseno\": similitud_gpt,\n",
    "                \"Coherencia\": coherencia_gpt,\n",
    "                \"Personalizacion\": personalizacion_gpt,\n",
    "                \"Fluidez\": fluidez_gpt,\n",
    "                \"Alucinacion\": alucinacion_gpt,\n",
    "                \"Input_Tokens\": input_tokens_gpt,\n",
    "                \"Output_Tokens\": output_tokens_gpt,\n",
    "                \"API_Latency_s\": api_latency_gpt,\n",
    "                \"Total_Exec_Time_s\": total_exec_time_gpt,\n",
    "                \"Successful_Response\": successful_gpt_response\n",
    "            })\n",
    "        else:\n",
    "            print(f\"Saltando evaluación de GPT para pregunta '{pregunta[:50]}...', cliente no inicializado.\")\n",
    "\n",
    "\n",
    "        # --- Evaluación para GEMINI (Modelo 2) ---\n",
    "        if genai: # Solo ejecutar si el cliente Gemini/Gemma está configurado\n",
    "            respuesta_modelo_gemma = \"No se pudo generar respuesta.\"\n",
    "            input_tokens_gemma = 0\n",
    "            output_tokens_gemma = 0\n",
    "            api_latency_gemma = 0\n",
    "            total_exec_time_gemma = 0\n",
    "            successful_gemma_response = False\n",
    "            contexto_gemma_usado = \"\"\n",
    "\n",
    "            for attempt in range(max_attempts):\n",
    "                try:\n",
    "                    start_time_total = time.time()\n",
    "                    context_chunks_gemma = get_context_from_query(pregunta, df_vector_store_gem, top_join, text_embedding_gemma)\n",
    "                    contexto_gemma_usado = \" \".join(context_chunks_gemma) # Para alucinación\n",
    "                    custom_prompt_gemma = prompt_system.format(source=str(context_chunks_gemma))\n",
    "                    model_2 = genai.GenerativeModel(model_name=modelo_banco2)\n",
    "                    full_query_for_gemma = custom_prompt_gemma + \"\\n\\nPregunta del usuario: \" + pregunta\n",
    "\n",
    "                    llm_start_time = time.time()\n",
    "                    completion2 = model_2.generate_content(\n",
    "                        contents=full_query_for_gemma,\n",
    "                        generation_config=GenerationConfig(temperature=temperature_2)\n",
    "                    )\n",
    "                    respuesta_modelo_gemma = completion2.text\n",
    "                    llm_end_time = time.time()\n",
    "\n",
    "                    # Para Gemini/Gemma, los tokens son una estimación simple si no se exponen directamente en la respuesta\n",
    "                    input_tokens_gemma = len(full_query_for_gemma.split())\n",
    "                    output_tokens_gemma = len(respuesta_modelo_gemma.split())\n",
    "                    api_latency_gemma = llm_end_time - llm_start_time\n",
    "                    total_exec_time_gemma = time.time() - start_time_total\n",
    "                    successful_gemma_response = True\n",
    "                    break # Salir del bucle de reintentos si fue exitoso\n",
    "                except Exception as e:\n",
    "                    print(f\"    [GEMINI] Error en intento {attempt + 1}/{max_attempts} para '{pregunta[:50]}...': {e}\")\n",
    "                    if \"ResourceExhausted\" in str(e) or \"quota\" in str(e).lower() or \"rate limit\" in str(e).lower():\n",
    "                        print(f\"    [GEMINI] Error de cuota. Esperando {wait_time_on_error_s}s...\")\n",
    "                        time.sleep(wait_time_on_error_s)\n",
    "                    else:\n",
    "                        break # Otros errores no reintentables\n",
    "\n",
    "            # Calcular métricas para Gemma\n",
    "            similitud_gemma = cosine_final(np.array(text_embedding_gemma(respuesta_modelo_gemma)), np.array(text_embedding_gemma(respuesta_humana))) if successful_gemma_response else 0.0\n",
    "            coherencia_gemma = medir_coherencia(pregunta, respuesta_modelo_gemma) if successful_gemma_response else 0.0\n",
    "            personalizacion_gemma = medir_personalizacion(perfil_usuario, respuesta_modelo_gemma) if successful_gemma_response else 0.0\n",
    "            fluidez_gemma = medir_fluidez(respuesta_modelo_gemma) if successful_gemma_response else 0.0\n",
    "            alucinacion_gemma = medir_alucinacion(respuesta_modelo_gemma, contexto_gemma_usado) if successful_gemma_response else 0.0\n",
    "\n",
    "            df_respuestas_detalladas.append({\n",
    "                \"Modelo\": modelo_banco2,\n",
    "                \"Prompt_Tipo\": current_prompt_name, # Usamos el nombre del prompt\n",
    "                \"Pregunta\": pregunta,\n",
    "                \"Respuesta_Humana\": respuesta_humana,\n",
    "                \"Respuesta_Modelo\": respuesta_modelo_gemma,\n",
    "                \"Similitud_Coseno\": similitud_gemma,\n",
    "                \"Coherencia\": coherencia_gemma,\n",
    "                \"Personalizacion\": personalizacion_gemma,\n",
    "                \"Fluidez\": fluidez_gemma,\n",
    "                \"Alucinacion\": alucinacion_gemma,\n",
    "                \"Input_Tokens\": input_tokens_gemma,\n",
    "                \"Output_Tokens\": output_tokens_gemma,\n",
    "                \"API_Latency_s\": api_latency_gemma,\n",
    "                \"Total_Exec_Time_s\": total_exec_time_gemma,\n",
    "                \"Successful_Response\": successful_gemma_response\n",
    "            })\n",
    "        else:\n",
    "            print(f\"Saltando evaluación de GEMINI para pregunta '{pregunta[:50]}...', cliente no configurado.\")\n",
    "\n",
    "\n",
    "df_respuestas_detalladas = pd.DataFrame(df_respuestas_detalladas)\n",
    "\n",
    "# --- Cálculo de Métricas y Resumen Final ---\n",
    "# df_res_successful = df_respuestas_detalladas[df_respuestas_detalladas['Successful_Response'] == True].copy()\n",
    "\n",
    "df_respuestas_detalladas = df_respuestas_detalladas[\n",
    "    (df_respuestas_detalladas['Successful_Response'] == True) &\n",
    "    (df_respuestas_detalladas['Similitud_Coseno'] != 0) &\n",
    "    (df_respuestas_detalladas['Coherencia'] != 0) &\n",
    "    (df_respuestas_detalladas['Personalizacion'] != 0) &\n",
    "    (df_respuestas_detalladas['Fluidez'] != 0) &\n",
    "    (df_respuestas_detalladas['Alucinacion'] != 0)\n",
    "].copy()\n",
    "df_respuestas_detalladas.index = range(df_respuestas_detalladas.shape[0])\n",
    "\n",
    "# Calcular las tasas\n",
    "df_respuestas_detalladas[\"Tasa\"] = df_respuestas_detalladas.apply(lambda row: 1 if row[\"Similitud_Coseno\"]>=umbrales[0] else 0, axis = 1)\n",
    "df_respuestas_detalladas[\"Coh\"] = df_respuestas_detalladas.apply(lambda row: 1 if row[\"Coherencia\"]>=umbrales[1] else 0, axis = 1)\n",
    "df_respuestas_detalladas[\"Per\"] = df_respuestas_detalladas.apply(lambda row: 1 if row[\"Personalizacion\"]>=umbrales[2] else 0, axis = 1)\n",
    "df_respuestas_detalladas[\"Flu\"] = df_respuestas_detalladas.apply(lambda row: 1 if row[\"Fluidez\"]>=umbrales[3] else 0, axis = 1)\n",
    "df_respuestas_detalladas[\"Alu\"] = df_respuestas_detalladas.apply(lambda row: 1 if row[\"Alucinacion\"]>=umbrales[4] else 0, axis = 1)\n",
    "df_respuestas_detalladas[\"total\"] = 1 # Para contar el total de filas\n",
    "\n",
    "# Escalamiento de la fluidez\n",
    "if np.mean(df_respuestas_detalladas[\"Fluidez\"]>=umbrales[3])>=0.30:\n",
    "    min_F = df_respuestas_detalladas[\"Fluidez\"].min()\n",
    "    max_F = df_respuestas_detalladas[\"Fluidez\"].max()\n",
    "    df_respuestas_detalladas[\"Fluidez\"]  = (df_respuestas_detalladas[\"Fluidez\"] - min_F) / (max_F - min_F)\n",
    "    df_respuestas_detalladas[\"Flu\"] = df_respuestas_detalladas.apply(lambda row: 1 if row[\"Fluidez\"]>=0.50 else 0, axis = 1)\n",
    "\n",
    "\n",
    "# Agrupar y calcular promedios\n",
    "summary_ = df_respuestas_detalladas.groupby([\"Modelo\", \"Prompt_Tipo\"]).agg(\n",
    "    Tasa=('Tasa', 'mean'),\n",
    "    Coh=('Coh', 'mean'),\n",
    "    Per=('Per', 'mean'),\n",
    "    Flu=('Flu', 'mean'),\n",
    "    Alu=('Alu', 'mean'),\n",
    "    Total_Respuestas=('total', 'sum'),\n",
    "    Respuestas_Exitosas=('Successful_Response', 'sum'), # Contar las exitosas\n",
    "    avg_input_tokens=('Input_Tokens', 'mean'),\n",
    "    avg_output_tokens=('Output_Tokens', 'mean'),\n",
    "    avg_api_latency=('API_Latency_s', 'mean'),\n",
    "    avg_total_execution_time=('Total_Exec_Time_s', 'mean')\n",
    ").reset_index()\n",
    "\n",
    "# Convertir tasas a porcentaje\n",
    "for k in [\"Tasa\", \"Coh\", \"Per\", \"Flu\", \"Alu\"]:\n",
    "    summary_[k] = 100 * summary_[k]\n",
    "\n",
    "# --- Impresión de Resultados ---\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"--- Detalles de Respuestas Generadas (DataFrame Completo) ---\".center(80))\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Modelo</th>\n",
       "      <th>Prompt_Tipo</th>\n",
       "      <th>Pregunta</th>\n",
       "      <th>Respuesta_Humana</th>\n",
       "      <th>Respuesta_Modelo</th>\n",
       "      <th>Similitud_Coseno</th>\n",
       "      <th>Coherencia</th>\n",
       "      <th>Personalizacion</th>\n",
       "      <th>Fluidez</th>\n",
       "      <th>Alucinacion</th>\n",
       "      <th>...</th>\n",
       "      <th>Output_Tokens</th>\n",
       "      <th>API_Latency_s</th>\n",
       "      <th>Total_Exec_Time_s</th>\n",
       "      <th>Successful_Response</th>\n",
       "      <th>Tasa</th>\n",
       "      <th>Coh</th>\n",
       "      <th>Per</th>\n",
       "      <th>Flu</th>\n",
       "      <th>Alu</th>\n",
       "      <th>total</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>gpt-4o-mini_clasificacion-PQRS</td>\n",
       "      <td>prompt_systemE1</td>\n",
       "      <td>¿Qué es LLMOps?</td>\n",
       "      <td>LLMOps (Large Language Model Operations) es un...</td>\n",
       "      <td>LLMOps, que significa \"Large Language Model Op...</td>\n",
       "      <td>0.954564</td>\n",
       "      <td>0.742256</td>\n",
       "      <td>0.790552</td>\n",
       "      <td>0.577402</td>\n",
       "      <td>0.770680</td>\n",
       "      <td>...</td>\n",
       "      <td>368</td>\n",
       "      <td>4.801720</td>\n",
       "      <td>5.840408</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>models/gemini-2.5-flash</td>\n",
       "      <td>prompt_systemE1</td>\n",
       "      <td>¿Qué es LLMOps?</td>\n",
       "      <td>LLMOps (Large Language Model Operations) es un...</td>\n",
       "      <td>¡Hola! Con gusto te explico qué es LLMOps, que...</td>\n",
       "      <td>0.836411</td>\n",
       "      <td>0.768006</td>\n",
       "      <td>0.789986</td>\n",
       "      <td>0.787017</td>\n",
       "      <td>0.766108</td>\n",
       "      <td>...</td>\n",
       "      <td>425</td>\n",
       "      <td>11.972167</td>\n",
       "      <td>12.315998</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>gpt-4o-mini_clasificacion-PQRS</td>\n",
       "      <td>prompt_systemE1</td>\n",
       "      <td>¿Cuáles son los objetivos principales de LLMOps?</td>\n",
       "      <td>LLMOps busca habilitar las capacidades de IA c...</td>\n",
       "      <td>Los objetivos principales de LLMOps, que se re...</td>\n",
       "      <td>0.892747</td>\n",
       "      <td>0.796265</td>\n",
       "      <td>0.792221</td>\n",
       "      <td>0.438995</td>\n",
       "      <td>0.767021</td>\n",
       "      <td>...</td>\n",
       "      <td>395</td>\n",
       "      <td>3.874571</td>\n",
       "      <td>4.946980</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                           Modelo      Prompt_Tipo  \\\n",
       "0  gpt-4o-mini_clasificacion-PQRS  prompt_systemE1   \n",
       "1         models/gemini-2.5-flash  prompt_systemE1   \n",
       "2  gpt-4o-mini_clasificacion-PQRS  prompt_systemE1   \n",
       "\n",
       "                                           Pregunta  \\\n",
       "0                                   ¿Qué es LLMOps?   \n",
       "1                                   ¿Qué es LLMOps?   \n",
       "2  ¿Cuáles son los objetivos principales de LLMOps?   \n",
       "\n",
       "                                    Respuesta_Humana  \\\n",
       "0  LLMOps (Large Language Model Operations) es un...   \n",
       "1  LLMOps (Large Language Model Operations) es un...   \n",
       "2  LLMOps busca habilitar las capacidades de IA c...   \n",
       "\n",
       "                                    Respuesta_Modelo  Similitud_Coseno  \\\n",
       "0  LLMOps, que significa \"Large Language Model Op...          0.954564   \n",
       "1  ¡Hola! Con gusto te explico qué es LLMOps, que...          0.836411   \n",
       "2  Los objetivos principales de LLMOps, que se re...          0.892747   \n",
       "\n",
       "   Coherencia  Personalizacion   Fluidez  Alucinacion  ...  Output_Tokens  \\\n",
       "0    0.742256         0.790552  0.577402     0.770680  ...            368   \n",
       "1    0.768006         0.789986  0.787017     0.766108  ...            425   \n",
       "2    0.796265         0.792221  0.438995     0.767021  ...            395   \n",
       "\n",
       "   API_Latency_s  Total_Exec_Time_s  Successful_Response  Tasa  Coh  Per  Flu  \\\n",
       "0       4.801720           5.840408                 True     1    0    1    1   \n",
       "1      11.972167          12.315998                 True     1    0    1    1   \n",
       "2       3.874571           4.946980                 True     1    0    1    0   \n",
       "\n",
       "   Alu  total  \n",
       "0    1      1  \n",
       "1    1      1  \n",
       "2    1      1  \n",
       "\n",
       "[3 rows x 21 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_respuestas_detalladas.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Modelo</th>\n",
       "      <th>Indice Monitoreo</th>\n",
       "      <th>Prompt_Tipo</th>\n",
       "      <th>Tasa</th>\n",
       "      <th>Coh</th>\n",
       "      <th>Per</th>\n",
       "      <th>Flu</th>\n",
       "      <th>Alu</th>\n",
       "      <th>Total_Respuestas</th>\n",
       "      <th>Respuestas_Exitosas</th>\n",
       "      <th>avg_input_tokens</th>\n",
       "      <th>avg_output_tokens</th>\n",
       "      <th>avg_api_latency</th>\n",
       "      <th>avg_total_execution_time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>gpt-4o-mini_clasificacion-PQRS</td>\n",
       "      <td>86.0</td>\n",
       "      <td>prompt_systemE1</td>\n",
       "      <td>100.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>80.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>2975.7</td>\n",
       "      <td>336.9</td>\n",
       "      <td>3.976038</td>\n",
       "      <td>5.091694</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>gpt-4o-mini_clasificacion-PQRS</td>\n",
       "      <td>84.0</td>\n",
       "      <td>prompt_systemE2</td>\n",
       "      <td>100.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>2728.7</td>\n",
       "      <td>179.3</td>\n",
       "      <td>2.515528</td>\n",
       "      <td>3.593771</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>models/gemini-2.5-flash</td>\n",
       "      <td>78.0</td>\n",
       "      <td>prompt_systemE1</td>\n",
       "      <td>50.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>2108.2</td>\n",
       "      <td>385.9</td>\n",
       "      <td>7.741030</td>\n",
       "      <td>8.026714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>models/gemini-2.5-flash</td>\n",
       "      <td>78.0</td>\n",
       "      <td>prompt_systemE2</td>\n",
       "      <td>70.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>1947.2</td>\n",
       "      <td>143.7</td>\n",
       "      <td>3.470816</td>\n",
       "      <td>3.768012</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           Modelo  Indice Monitoreo      Prompt_Tipo   Tasa  \\\n",
       "0  gpt-4o-mini_clasificacion-PQRS              86.0  prompt_systemE1  100.0   \n",
       "1  gpt-4o-mini_clasificacion-PQRS              84.0  prompt_systemE2  100.0   \n",
       "2         models/gemini-2.5-flash              78.0  prompt_systemE1   50.0   \n",
       "3         models/gemini-2.5-flash              78.0  prompt_systemE2   70.0   \n",
       "\n",
       "    Coh    Per    Flu    Alu  Total_Respuestas  Respuestas_Exitosas  \\\n",
       "0  50.0  100.0   80.0  100.0                10                   10   \n",
       "1  70.0  100.0   50.0  100.0                10                   10   \n",
       "2  40.0  100.0  100.0  100.0                10                   10   \n",
       "3  70.0  100.0   50.0  100.0                10                   10   \n",
       "\n",
       "   avg_input_tokens  avg_output_tokens  avg_api_latency  \\\n",
       "0            2975.7              336.9         3.976038   \n",
       "1            2728.7              179.3         2.515528   \n",
       "2            2108.2              385.9         7.741030   \n",
       "3            1947.2              143.7         3.470816   \n",
       "\n",
       "   avg_total_execution_time  \n",
       "0                  5.091694  \n",
       "1                  3.593771  \n",
       "2                  8.026714  \n",
       "3                  3.768012  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cl = summary_.columns\n",
    "summary_[\"Indice Monitoreo\"] = 0.20 * (summary_[\"Tasa\"] + summary_[\"Coh\"] + summary_[\"Per\"] + summary_[\"Flu\"] + summary_[\"Alu\"])\n",
    "cl2 = [x for x in cl if x not in [\"Modelo\"]]\n",
    "summary_ = summary_.get([\"Modelo\",\"Indice Monitoreo\"] + cl2)\n",
    "summary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "my_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
