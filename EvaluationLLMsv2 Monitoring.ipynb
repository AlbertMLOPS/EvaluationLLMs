{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ST (Pruebas)\n",
    "\n",
    "* Optimización inferencia y tiempos de respuesta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuración cargada exitosamente desde config.yaml\n",
      "Cliente Azure OpenAI inicializado.\n",
      "Cliente Gemini/Gemma configurado.\n",
      "Cargando y procesando PDF...\n",
      "Generando embeddings para GPT (Azure)...\n",
      "Generando embeddings para Gemma...\n",
      "Enviando solicitud al LLM para generar 10 pares QA...\n",
      "Generación del LLM completada. Procesando pares QA...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following layers were not sharded: encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.dense.weight, embeddings.LayerNorm.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.query.weight, encoder.layer.*.attention.self.key.bias, encoder.layer.*.output.LayerNorm.bias, encoder.layer.*.output.dense.weight, encoder.layer.*.attention.self.value.weight, embeddings.word_embeddings.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.weight, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.value.bias, pooler.dense.bias, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.bias, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.output.LayerNorm.bias, encoder.layer.*.intermediate.dense.weight, pooler.dense.weight, embeddings.token_type_embeddings.weight\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.dense.weight, embeddings.LayerNorm.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.query.weight, encoder.layer.*.attention.self.key.bias, encoder.layer.*.output.LayerNorm.bias, encoder.layer.*.output.dense.weight, encoder.layer.*.attention.self.value.weight, embeddings.word_embeddings.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.weight, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.value.bias, pooler.dense.bias, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.bias, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.output.LayerNorm.bias, encoder.layer.*.intermediate.dense.weight, pooler.dense.weight, embeddings.token_type_embeddings.weight\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.dense.weight, embeddings.LayerNorm.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.query.weight, encoder.layer.*.attention.self.key.bias, encoder.layer.*.output.LayerNorm.bias, encoder.layer.*.output.dense.weight, encoder.layer.*.attention.self.value.weight, embeddings.word_embeddings.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.weight, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.value.bias, pooler.dense.bias, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.bias, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.output.LayerNorm.bias, encoder.layer.*.intermediate.dense.weight, pooler.dense.weight, embeddings.token_type_embeddings.weight\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.dense.weight, embeddings.LayerNorm.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.query.weight, encoder.layer.*.attention.self.key.bias, encoder.layer.*.output.LayerNorm.bias, encoder.layer.*.output.dense.weight, encoder.layer.*.attention.self.value.weight, embeddings.word_embeddings.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.weight, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.value.bias, pooler.dense.bias, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.bias, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.output.LayerNorm.bias, encoder.layer.*.intermediate.dense.weight, pooler.dense.weight, embeddings.token_type_embeddings.weight\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.dense.weight, embeddings.LayerNorm.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.query.weight, encoder.layer.*.attention.self.key.bias, encoder.layer.*.output.LayerNorm.bias, encoder.layer.*.output.dense.weight, encoder.layer.*.attention.self.value.weight, embeddings.word_embeddings.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.weight, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.value.bias, pooler.dense.bias, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.bias, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.output.LayerNorm.bias, encoder.layer.*.intermediate.dense.weight, pooler.dense.weight, embeddings.token_type_embeddings.weight\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.dense.weight, embeddings.LayerNorm.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.query.weight, encoder.layer.*.attention.self.key.bias, encoder.layer.*.output.LayerNorm.bias, encoder.layer.*.output.dense.weight, encoder.layer.*.attention.self.value.weight, embeddings.word_embeddings.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.weight, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.value.bias, pooler.dense.bias, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.bias, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.output.LayerNorm.bias, encoder.layer.*.intermediate.dense.weight, pooler.dense.weight, embeddings.token_type_embeddings.weight\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.dense.weight, embeddings.LayerNorm.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.query.weight, encoder.layer.*.attention.self.key.bias, encoder.layer.*.output.LayerNorm.bias, encoder.layer.*.output.dense.weight, encoder.layer.*.attention.self.value.weight, embeddings.word_embeddings.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.weight, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.value.bias, pooler.dense.bias, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.bias, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.output.LayerNorm.bias, encoder.layer.*.intermediate.dense.weight, pooler.dense.weight, embeddings.token_type_embeddings.weight\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.dense.weight, embeddings.LayerNorm.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.query.weight, encoder.layer.*.attention.self.key.bias, encoder.layer.*.output.LayerNorm.bias, encoder.layer.*.output.dense.weight, encoder.layer.*.attention.self.value.weight, embeddings.word_embeddings.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.weight, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.value.bias, pooler.dense.bias, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.bias, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.output.LayerNorm.bias, encoder.layer.*.intermediate.dense.weight, pooler.dense.weight, embeddings.token_type_embeddings.weight\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.dense.weight, embeddings.LayerNorm.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.query.weight, encoder.layer.*.attention.self.key.bias, encoder.layer.*.output.LayerNorm.bias, encoder.layer.*.output.dense.weight, encoder.layer.*.attention.self.value.weight, embeddings.word_embeddings.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.weight, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.value.bias, pooler.dense.bias, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.bias, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.output.LayerNorm.bias, encoder.layer.*.intermediate.dense.weight, pooler.dense.weight, embeddings.token_type_embeddings.weight\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.dense.weight, embeddings.LayerNorm.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.query.weight, encoder.layer.*.attention.self.key.bias, encoder.layer.*.output.LayerNorm.bias, encoder.layer.*.output.dense.weight, encoder.layer.*.attention.self.value.weight, embeddings.word_embeddings.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.weight, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.value.bias, pooler.dense.bias, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.bias, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.output.LayerNorm.bias, encoder.layer.*.intermediate.dense.weight, pooler.dense.weight, embeddings.token_type_embeddings.weight\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.dense.weight, embeddings.LayerNorm.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.query.weight, encoder.layer.*.attention.self.key.bias, encoder.layer.*.output.LayerNorm.bias, encoder.layer.*.output.dense.weight, encoder.layer.*.attention.self.value.weight, embeddings.word_embeddings.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.weight, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.value.bias, pooler.dense.bias, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.bias, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.output.LayerNorm.bias, encoder.layer.*.intermediate.dense.weight, pooler.dense.weight, embeddings.token_type_embeddings.weight\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.dense.weight, embeddings.LayerNorm.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.query.weight, encoder.layer.*.attention.self.key.bias, encoder.layer.*.output.LayerNorm.bias, encoder.layer.*.output.dense.weight, encoder.layer.*.attention.self.value.weight, embeddings.word_embeddings.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.weight, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.value.bias, pooler.dense.bias, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.bias, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.output.LayerNorm.bias, encoder.layer.*.intermediate.dense.weight, pooler.dense.weight, embeddings.token_type_embeddings.weight\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.dense.weight, embeddings.LayerNorm.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.query.weight, encoder.layer.*.attention.self.key.bias, encoder.layer.*.output.LayerNorm.bias, encoder.layer.*.output.dense.weight, encoder.layer.*.attention.self.value.weight, embeddings.word_embeddings.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.weight, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.value.bias, pooler.dense.bias, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.bias, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.output.LayerNorm.bias, encoder.layer.*.intermediate.dense.weight, pooler.dense.weight, embeddings.token_type_embeddings.weight\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.dense.weight, embeddings.LayerNorm.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.query.weight, encoder.layer.*.attention.self.key.bias, encoder.layer.*.output.LayerNorm.bias, encoder.layer.*.output.dense.weight, encoder.layer.*.attention.self.value.weight, embeddings.word_embeddings.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.weight, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.value.bias, pooler.dense.bias, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.bias, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.output.LayerNorm.bias, encoder.layer.*.intermediate.dense.weight, pooler.dense.weight, embeddings.token_type_embeddings.weight\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.dense.weight, embeddings.LayerNorm.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.query.weight, encoder.layer.*.attention.self.key.bias, encoder.layer.*.output.LayerNorm.bias, encoder.layer.*.output.dense.weight, encoder.layer.*.attention.self.value.weight, embeddings.word_embeddings.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.weight, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.value.bias, pooler.dense.bias, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.bias, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.output.LayerNorm.bias, encoder.layer.*.intermediate.dense.weight, pooler.dense.weight, embeddings.token_type_embeddings.weight\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.dense.weight, embeddings.LayerNorm.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.query.weight, encoder.layer.*.attention.self.key.bias, encoder.layer.*.output.LayerNorm.bias, encoder.layer.*.output.dense.weight, encoder.layer.*.attention.self.value.weight, embeddings.word_embeddings.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.weight, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.value.bias, pooler.dense.bias, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.bias, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.output.LayerNorm.bias, encoder.layer.*.intermediate.dense.weight, pooler.dense.weight, embeddings.token_type_embeddings.weight\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.dense.weight, embeddings.LayerNorm.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.query.weight, encoder.layer.*.attention.self.key.bias, encoder.layer.*.output.LayerNorm.bias, encoder.layer.*.output.dense.weight, encoder.layer.*.attention.self.value.weight, embeddings.word_embeddings.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.weight, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.value.bias, pooler.dense.bias, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.bias, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.output.LayerNorm.bias, encoder.layer.*.intermediate.dense.weight, pooler.dense.weight, embeddings.token_type_embeddings.weight\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.dense.weight, embeddings.LayerNorm.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.query.weight, encoder.layer.*.attention.self.key.bias, encoder.layer.*.output.LayerNorm.bias, encoder.layer.*.output.dense.weight, encoder.layer.*.attention.self.value.weight, embeddings.word_embeddings.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.weight, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.value.bias, pooler.dense.bias, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.bias, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.output.LayerNorm.bias, encoder.layer.*.intermediate.dense.weight, pooler.dense.weight, embeddings.token_type_embeddings.weight\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.dense.weight, embeddings.LayerNorm.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.query.weight, encoder.layer.*.attention.self.key.bias, encoder.layer.*.output.LayerNorm.bias, encoder.layer.*.output.dense.weight, encoder.layer.*.attention.self.value.weight, embeddings.word_embeddings.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.weight, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.value.bias, pooler.dense.bias, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.bias, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.output.LayerNorm.bias, encoder.layer.*.intermediate.dense.weight, pooler.dense.weight, embeddings.token_type_embeddings.weight\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.dense.weight, embeddings.LayerNorm.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.query.weight, encoder.layer.*.attention.self.key.bias, encoder.layer.*.output.LayerNorm.bias, encoder.layer.*.output.dense.weight, encoder.layer.*.attention.self.value.weight, embeddings.word_embeddings.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.weight, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.value.bias, pooler.dense.bias, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.bias, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.output.LayerNorm.bias, encoder.layer.*.intermediate.dense.weight, pooler.dense.weight, embeddings.token_type_embeddings.weight\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.dense.weight, embeddings.LayerNorm.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.query.weight, encoder.layer.*.attention.self.key.bias, encoder.layer.*.output.LayerNorm.bias, encoder.layer.*.output.dense.weight, encoder.layer.*.attention.self.value.weight, embeddings.word_embeddings.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.weight, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.value.bias, pooler.dense.bias, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.bias, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.output.LayerNorm.bias, encoder.layer.*.intermediate.dense.weight, pooler.dense.weight, embeddings.token_type_embeddings.weight\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.dense.weight, embeddings.LayerNorm.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.query.weight, encoder.layer.*.attention.self.key.bias, encoder.layer.*.output.LayerNorm.bias, encoder.layer.*.output.dense.weight, encoder.layer.*.attention.self.value.weight, embeddings.word_embeddings.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.weight, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.value.bias, pooler.dense.bias, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.bias, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.output.LayerNorm.bias, encoder.layer.*.intermediate.dense.weight, pooler.dense.weight, embeddings.token_type_embeddings.weight\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.dense.weight, embeddings.LayerNorm.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.query.weight, encoder.layer.*.attention.self.key.bias, encoder.layer.*.output.LayerNorm.bias, encoder.layer.*.output.dense.weight, encoder.layer.*.attention.self.value.weight, embeddings.word_embeddings.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.weight, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.value.bias, pooler.dense.bias, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.bias, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.output.LayerNorm.bias, encoder.layer.*.intermediate.dense.weight, pooler.dense.weight, embeddings.token_type_embeddings.weight\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.dense.weight, embeddings.LayerNorm.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.query.weight, encoder.layer.*.attention.self.key.bias, encoder.layer.*.output.LayerNorm.bias, encoder.layer.*.output.dense.weight, encoder.layer.*.attention.self.value.weight, embeddings.word_embeddings.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.weight, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.value.bias, pooler.dense.bias, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.bias, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.output.LayerNorm.bias, encoder.layer.*.intermediate.dense.weight, pooler.dense.weight, embeddings.token_type_embeddings.weight\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.dense.weight, embeddings.LayerNorm.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.query.weight, encoder.layer.*.attention.self.key.bias, encoder.layer.*.output.LayerNorm.bias, encoder.layer.*.output.dense.weight, encoder.layer.*.attention.self.value.weight, embeddings.word_embeddings.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.weight, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.value.bias, pooler.dense.bias, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.bias, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.output.LayerNorm.bias, encoder.layer.*.intermediate.dense.weight, pooler.dense.weight, embeddings.token_type_embeddings.weight\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.dense.weight, embeddings.LayerNorm.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.query.weight, encoder.layer.*.attention.self.key.bias, encoder.layer.*.output.LayerNorm.bias, encoder.layer.*.output.dense.weight, encoder.layer.*.attention.self.value.weight, embeddings.word_embeddings.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.weight, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.value.bias, pooler.dense.bias, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.bias, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.output.LayerNorm.bias, encoder.layer.*.intermediate.dense.weight, pooler.dense.weight, embeddings.token_type_embeddings.weight\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.dense.weight, embeddings.LayerNorm.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.query.weight, encoder.layer.*.attention.self.key.bias, encoder.layer.*.output.LayerNorm.bias, encoder.layer.*.output.dense.weight, encoder.layer.*.attention.self.value.weight, embeddings.word_embeddings.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.weight, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.value.bias, pooler.dense.bias, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.bias, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.output.LayerNorm.bias, encoder.layer.*.intermediate.dense.weight, pooler.dense.weight, embeddings.token_type_embeddings.weight\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.dense.weight, embeddings.LayerNorm.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.query.weight, encoder.layer.*.attention.self.key.bias, encoder.layer.*.output.LayerNorm.bias, encoder.layer.*.output.dense.weight, encoder.layer.*.attention.self.value.weight, embeddings.word_embeddings.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.weight, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.value.bias, pooler.dense.bias, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.bias, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.output.LayerNorm.bias, encoder.layer.*.intermediate.dense.weight, pooler.dense.weight, embeddings.token_type_embeddings.weight\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.dense.weight, embeddings.LayerNorm.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.query.weight, encoder.layer.*.attention.self.key.bias, encoder.layer.*.output.LayerNorm.bias, encoder.layer.*.output.dense.weight, encoder.layer.*.attention.self.value.weight, embeddings.word_embeddings.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.weight, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.value.bias, pooler.dense.bias, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.bias, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.output.LayerNorm.bias, encoder.layer.*.intermediate.dense.weight, pooler.dense.weight, embeddings.token_type_embeddings.weight\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.dense.weight, embeddings.LayerNorm.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.query.weight, encoder.layer.*.attention.self.key.bias, encoder.layer.*.output.LayerNorm.bias, encoder.layer.*.output.dense.weight, encoder.layer.*.attention.self.value.weight, embeddings.word_embeddings.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.weight, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.value.bias, pooler.dense.bias, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.bias, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.output.LayerNorm.bias, encoder.layer.*.intermediate.dense.weight, pooler.dense.weight, embeddings.token_type_embeddings.weight\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.dense.weight, embeddings.LayerNorm.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.query.weight, encoder.layer.*.attention.self.key.bias, encoder.layer.*.output.LayerNorm.bias, encoder.layer.*.output.dense.weight, encoder.layer.*.attention.self.value.weight, embeddings.word_embeddings.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.weight, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.value.bias, pooler.dense.bias, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.bias, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.output.LayerNorm.bias, encoder.layer.*.intermediate.dense.weight, pooler.dense.weight, embeddings.token_type_embeddings.weight\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.dense.weight, embeddings.LayerNorm.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.query.weight, encoder.layer.*.attention.self.key.bias, encoder.layer.*.output.LayerNorm.bias, encoder.layer.*.output.dense.weight, encoder.layer.*.attention.self.value.weight, embeddings.word_embeddings.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.weight, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.value.bias, pooler.dense.bias, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.bias, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.output.LayerNorm.bias, encoder.layer.*.intermediate.dense.weight, pooler.dense.weight, embeddings.token_type_embeddings.weight\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.dense.weight, embeddings.LayerNorm.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.query.weight, encoder.layer.*.attention.self.key.bias, encoder.layer.*.output.LayerNorm.bias, encoder.layer.*.output.dense.weight, encoder.layer.*.attention.self.value.weight, embeddings.word_embeddings.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.weight, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.value.bias, pooler.dense.bias, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.bias, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.output.LayerNorm.bias, encoder.layer.*.intermediate.dense.weight, pooler.dense.weight, embeddings.token_type_embeddings.weight\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.dense.weight, embeddings.LayerNorm.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.query.weight, encoder.layer.*.attention.self.key.bias, encoder.layer.*.output.LayerNorm.bias, encoder.layer.*.output.dense.weight, encoder.layer.*.attention.self.value.weight, embeddings.word_embeddings.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.weight, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.value.bias, pooler.dense.bias, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.bias, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.output.LayerNorm.bias, encoder.layer.*.intermediate.dense.weight, pooler.dense.weight, embeddings.token_type_embeddings.weight\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.dense.weight, embeddings.LayerNorm.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.query.weight, encoder.layer.*.attention.self.key.bias, encoder.layer.*.output.LayerNorm.bias, encoder.layer.*.output.dense.weight, encoder.layer.*.attention.self.value.weight, embeddings.word_embeddings.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.weight, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.value.bias, pooler.dense.bias, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.bias, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.output.LayerNorm.bias, encoder.layer.*.intermediate.dense.weight, pooler.dense.weight, embeddings.token_type_embeddings.weight\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.dense.weight, embeddings.LayerNorm.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.query.weight, encoder.layer.*.attention.self.key.bias, encoder.layer.*.output.LayerNorm.bias, encoder.layer.*.output.dense.weight, encoder.layer.*.attention.self.value.weight, embeddings.word_embeddings.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.weight, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.value.bias, pooler.dense.bias, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.bias, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.output.LayerNorm.bias, encoder.layer.*.intermediate.dense.weight, pooler.dense.weight, embeddings.token_type_embeddings.weight\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.dense.weight, embeddings.LayerNorm.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.query.weight, encoder.layer.*.attention.self.key.bias, encoder.layer.*.output.LayerNorm.bias, encoder.layer.*.output.dense.weight, encoder.layer.*.attention.self.value.weight, embeddings.word_embeddings.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.weight, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.value.bias, pooler.dense.bias, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.bias, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.output.LayerNorm.bias, encoder.layer.*.intermediate.dense.weight, pooler.dense.weight, embeddings.token_type_embeddings.weight\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.dense.weight, embeddings.LayerNorm.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.query.weight, encoder.layer.*.attention.self.key.bias, encoder.layer.*.output.LayerNorm.bias, encoder.layer.*.output.dense.weight, encoder.layer.*.attention.self.value.weight, embeddings.word_embeddings.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.weight, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.value.bias, pooler.dense.bias, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.bias, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.output.LayerNorm.bias, encoder.layer.*.intermediate.dense.weight, pooler.dense.weight, embeddings.token_type_embeddings.weight\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.dense.weight, embeddings.LayerNorm.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.query.weight, encoder.layer.*.attention.self.key.bias, encoder.layer.*.output.LayerNorm.bias, encoder.layer.*.output.dense.weight, encoder.layer.*.attention.self.value.weight, embeddings.word_embeddings.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.weight, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.value.bias, pooler.dense.bias, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.bias, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.output.LayerNorm.bias, encoder.layer.*.intermediate.dense.weight, pooler.dense.weight, embeddings.token_type_embeddings.weight\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.dense.weight, embeddings.LayerNorm.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.query.weight, encoder.layer.*.attention.self.key.bias, encoder.layer.*.output.LayerNorm.bias, encoder.layer.*.output.dense.weight, encoder.layer.*.attention.self.value.weight, embeddings.word_embeddings.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.weight, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.value.bias, pooler.dense.bias, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.bias, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.output.LayerNorm.bias, encoder.layer.*.intermediate.dense.weight, pooler.dense.weight, embeddings.token_type_embeddings.weight\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.dense.weight, embeddings.LayerNorm.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.query.weight, encoder.layer.*.attention.self.key.bias, encoder.layer.*.output.LayerNorm.bias, encoder.layer.*.output.dense.weight, encoder.layer.*.attention.self.value.weight, embeddings.word_embeddings.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.weight, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.value.bias, pooler.dense.bias, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.bias, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.output.LayerNorm.bias, encoder.layer.*.intermediate.dense.weight, pooler.dense.weight, embeddings.token_type_embeddings.weight\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.dense.weight, embeddings.LayerNorm.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.query.weight, encoder.layer.*.attention.self.key.bias, encoder.layer.*.output.LayerNorm.bias, encoder.layer.*.output.dense.weight, encoder.layer.*.attention.self.value.weight, embeddings.word_embeddings.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.weight, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.value.bias, pooler.dense.bias, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.bias, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.output.LayerNorm.bias, encoder.layer.*.intermediate.dense.weight, pooler.dense.weight, embeddings.token_type_embeddings.weight\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.dense.weight, embeddings.LayerNorm.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.query.weight, encoder.layer.*.attention.self.key.bias, encoder.layer.*.output.LayerNorm.bias, encoder.layer.*.output.dense.weight, encoder.layer.*.attention.self.value.weight, embeddings.word_embeddings.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.weight, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.value.bias, pooler.dense.bias, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.bias, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.output.LayerNorm.bias, encoder.layer.*.intermediate.dense.weight, pooler.dense.weight, embeddings.token_type_embeddings.weight\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.dense.weight, embeddings.LayerNorm.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.query.weight, encoder.layer.*.attention.self.key.bias, encoder.layer.*.output.LayerNorm.bias, encoder.layer.*.output.dense.weight, encoder.layer.*.attention.self.value.weight, embeddings.word_embeddings.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.weight, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.value.bias, pooler.dense.bias, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.bias, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.output.LayerNorm.bias, encoder.layer.*.intermediate.dense.weight, pooler.dense.weight, embeddings.token_type_embeddings.weight\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.dense.weight, embeddings.LayerNorm.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.query.weight, encoder.layer.*.attention.self.key.bias, encoder.layer.*.output.LayerNorm.bias, encoder.layer.*.output.dense.weight, encoder.layer.*.attention.self.value.weight, embeddings.word_embeddings.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.weight, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.value.bias, pooler.dense.bias, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.bias, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.output.LayerNorm.bias, encoder.layer.*.intermediate.dense.weight, pooler.dense.weight, embeddings.token_type_embeddings.weight\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.dense.weight, embeddings.LayerNorm.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.query.weight, encoder.layer.*.attention.self.key.bias, encoder.layer.*.output.LayerNorm.bias, encoder.layer.*.output.dense.weight, encoder.layer.*.attention.self.value.weight, embeddings.word_embeddings.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.weight, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.value.bias, pooler.dense.bias, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.bias, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.output.LayerNorm.bias, encoder.layer.*.intermediate.dense.weight, pooler.dense.weight, embeddings.token_type_embeddings.weight\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.dense.weight, embeddings.LayerNorm.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.query.weight, encoder.layer.*.attention.self.key.bias, encoder.layer.*.output.LayerNorm.bias, encoder.layer.*.output.dense.weight, encoder.layer.*.attention.self.value.weight, embeddings.word_embeddings.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.weight, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.value.bias, pooler.dense.bias, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.bias, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.output.LayerNorm.bias, encoder.layer.*.intermediate.dense.weight, pooler.dense.weight, embeddings.token_type_embeddings.weight\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.dense.weight, embeddings.LayerNorm.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.query.weight, encoder.layer.*.attention.self.key.bias, encoder.layer.*.output.LayerNorm.bias, encoder.layer.*.output.dense.weight, encoder.layer.*.attention.self.value.weight, embeddings.word_embeddings.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.weight, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.value.bias, pooler.dense.bias, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.bias, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.output.LayerNorm.bias, encoder.layer.*.intermediate.dense.weight, pooler.dense.weight, embeddings.token_type_embeddings.weight\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.dense.weight, embeddings.LayerNorm.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.query.weight, encoder.layer.*.attention.self.key.bias, encoder.layer.*.output.LayerNorm.bias, encoder.layer.*.output.dense.weight, encoder.layer.*.attention.self.value.weight, embeddings.word_embeddings.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.weight, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.value.bias, pooler.dense.bias, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.bias, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.output.LayerNorm.bias, encoder.layer.*.intermediate.dense.weight, pooler.dense.weight, embeddings.token_type_embeddings.weight\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.dense.weight, embeddings.LayerNorm.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.query.weight, encoder.layer.*.attention.self.key.bias, encoder.layer.*.output.LayerNorm.bias, encoder.layer.*.output.dense.weight, encoder.layer.*.attention.self.value.weight, embeddings.word_embeddings.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.weight, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.value.bias, pooler.dense.bias, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.bias, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.output.LayerNorm.bias, encoder.layer.*.intermediate.dense.weight, pooler.dense.weight, embeddings.token_type_embeddings.weight\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.dense.weight, embeddings.LayerNorm.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.query.weight, encoder.layer.*.attention.self.key.bias, encoder.layer.*.output.LayerNorm.bias, encoder.layer.*.output.dense.weight, encoder.layer.*.attention.self.value.weight, embeddings.word_embeddings.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.weight, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.value.bias, pooler.dense.bias, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.bias, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.output.LayerNorm.bias, encoder.layer.*.intermediate.dense.weight, pooler.dense.weight, embeddings.token_type_embeddings.weight\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.dense.weight, embeddings.LayerNorm.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.query.weight, encoder.layer.*.attention.self.key.bias, encoder.layer.*.output.LayerNorm.bias, encoder.layer.*.output.dense.weight, encoder.layer.*.attention.self.value.weight, embeddings.word_embeddings.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.weight, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.value.bias, pooler.dense.bias, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.bias, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.output.LayerNorm.bias, encoder.layer.*.intermediate.dense.weight, pooler.dense.weight, embeddings.token_type_embeddings.weight\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.dense.weight, embeddings.LayerNorm.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.query.weight, encoder.layer.*.attention.self.key.bias, encoder.layer.*.output.LayerNorm.bias, encoder.layer.*.output.dense.weight, encoder.layer.*.attention.self.value.weight, embeddings.word_embeddings.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.weight, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.value.bias, pooler.dense.bias, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.bias, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.output.LayerNorm.bias, encoder.layer.*.intermediate.dense.weight, pooler.dense.weight, embeddings.token_type_embeddings.weight\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.dense.weight, embeddings.LayerNorm.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.query.weight, encoder.layer.*.attention.self.key.bias, encoder.layer.*.output.LayerNorm.bias, encoder.layer.*.output.dense.weight, encoder.layer.*.attention.self.value.weight, embeddings.word_embeddings.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.weight, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.value.bias, pooler.dense.bias, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.bias, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.output.LayerNorm.bias, encoder.layer.*.intermediate.dense.weight, pooler.dense.weight, embeddings.token_type_embeddings.weight\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.dense.weight, embeddings.LayerNorm.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.query.weight, encoder.layer.*.attention.self.key.bias, encoder.layer.*.output.LayerNorm.bias, encoder.layer.*.output.dense.weight, encoder.layer.*.attention.self.value.weight, embeddings.word_embeddings.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.weight, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.value.bias, pooler.dense.bias, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.bias, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.output.LayerNorm.bias, encoder.layer.*.intermediate.dense.weight, pooler.dense.weight, embeddings.token_type_embeddings.weight\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.dense.weight, embeddings.LayerNorm.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.query.weight, encoder.layer.*.attention.self.key.bias, encoder.layer.*.output.LayerNorm.bias, encoder.layer.*.output.dense.weight, encoder.layer.*.attention.self.value.weight, embeddings.word_embeddings.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.weight, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.value.bias, pooler.dense.bias, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.bias, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.output.LayerNorm.bias, encoder.layer.*.intermediate.dense.weight, pooler.dense.weight, embeddings.token_type_embeddings.weight\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.dense.weight, embeddings.LayerNorm.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.query.weight, encoder.layer.*.attention.self.key.bias, encoder.layer.*.output.LayerNorm.bias, encoder.layer.*.output.dense.weight, encoder.layer.*.attention.self.value.weight, embeddings.word_embeddings.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.weight, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.value.bias, pooler.dense.bias, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.bias, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.output.LayerNorm.bias, encoder.layer.*.intermediate.dense.weight, pooler.dense.weight, embeddings.token_type_embeddings.weight\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.dense.weight, embeddings.LayerNorm.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.query.weight, encoder.layer.*.attention.self.key.bias, encoder.layer.*.output.LayerNorm.bias, encoder.layer.*.output.dense.weight, encoder.layer.*.attention.self.value.weight, embeddings.word_embeddings.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.weight, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.value.bias, pooler.dense.bias, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.bias, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.output.LayerNorm.bias, encoder.layer.*.intermediate.dense.weight, pooler.dense.weight, embeddings.token_type_embeddings.weight\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.dense.weight, embeddings.LayerNorm.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.query.weight, encoder.layer.*.attention.self.key.bias, encoder.layer.*.output.LayerNorm.bias, encoder.layer.*.output.dense.weight, encoder.layer.*.attention.self.value.weight, embeddings.word_embeddings.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.weight, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.value.bias, pooler.dense.bias, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.bias, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.output.LayerNorm.bias, encoder.layer.*.intermediate.dense.weight, pooler.dense.weight, embeddings.token_type_embeddings.weight\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.dense.weight, embeddings.LayerNorm.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.query.weight, encoder.layer.*.attention.self.key.bias, encoder.layer.*.output.LayerNorm.bias, encoder.layer.*.output.dense.weight, encoder.layer.*.attention.self.value.weight, embeddings.word_embeddings.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.weight, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.value.bias, pooler.dense.bias, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.bias, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.output.LayerNorm.bias, encoder.layer.*.intermediate.dense.weight, pooler.dense.weight, embeddings.token_type_embeddings.weight\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.dense.weight, embeddings.LayerNorm.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.query.weight, encoder.layer.*.attention.self.key.bias, encoder.layer.*.output.LayerNorm.bias, encoder.layer.*.output.dense.weight, encoder.layer.*.attention.self.value.weight, embeddings.word_embeddings.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.weight, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.value.bias, pooler.dense.bias, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.bias, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.output.LayerNorm.bias, encoder.layer.*.intermediate.dense.weight, pooler.dense.weight, embeddings.token_type_embeddings.weight\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.dense.weight, embeddings.LayerNorm.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.query.weight, encoder.layer.*.attention.self.key.bias, encoder.layer.*.output.LayerNorm.bias, encoder.layer.*.output.dense.weight, encoder.layer.*.attention.self.value.weight, embeddings.word_embeddings.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.weight, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.value.bias, pooler.dense.bias, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.bias, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.output.LayerNorm.bias, encoder.layer.*.intermediate.dense.weight, pooler.dense.weight, embeddings.token_type_embeddings.weight\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.dense.weight, embeddings.LayerNorm.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.query.weight, encoder.layer.*.attention.self.key.bias, encoder.layer.*.output.LayerNorm.bias, encoder.layer.*.output.dense.weight, encoder.layer.*.attention.self.value.weight, embeddings.word_embeddings.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.weight, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.value.bias, pooler.dense.bias, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.bias, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.output.LayerNorm.bias, encoder.layer.*.intermediate.dense.weight, pooler.dense.weight, embeddings.token_type_embeddings.weight\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.dense.weight, embeddings.LayerNorm.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.query.weight, encoder.layer.*.attention.self.key.bias, encoder.layer.*.output.LayerNorm.bias, encoder.layer.*.output.dense.weight, encoder.layer.*.attention.self.value.weight, embeddings.word_embeddings.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.weight, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.value.bias, pooler.dense.bias, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.bias, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.output.LayerNorm.bias, encoder.layer.*.intermediate.dense.weight, pooler.dense.weight, embeddings.token_type_embeddings.weight\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.dense.weight, embeddings.LayerNorm.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.query.weight, encoder.layer.*.attention.self.key.bias, encoder.layer.*.output.LayerNorm.bias, encoder.layer.*.output.dense.weight, encoder.layer.*.attention.self.value.weight, embeddings.word_embeddings.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.weight, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.value.bias, pooler.dense.bias, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.bias, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.output.LayerNorm.bias, encoder.layer.*.intermediate.dense.weight, pooler.dense.weight, embeddings.token_type_embeddings.weight\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.dense.weight, embeddings.LayerNorm.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.query.weight, encoder.layer.*.attention.self.key.bias, encoder.layer.*.output.LayerNorm.bias, encoder.layer.*.output.dense.weight, encoder.layer.*.attention.self.value.weight, embeddings.word_embeddings.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.weight, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.value.bias, pooler.dense.bias, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.bias, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.output.LayerNorm.bias, encoder.layer.*.intermediate.dense.weight, pooler.dense.weight, embeddings.token_type_embeddings.weight\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.dense.weight, embeddings.LayerNorm.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.query.weight, encoder.layer.*.attention.self.key.bias, encoder.layer.*.output.LayerNorm.bias, encoder.layer.*.output.dense.weight, encoder.layer.*.attention.self.value.weight, embeddings.word_embeddings.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.weight, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.value.bias, pooler.dense.bias, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.bias, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.output.LayerNorm.bias, encoder.layer.*.intermediate.dense.weight, pooler.dense.weight, embeddings.token_type_embeddings.weight\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.dense.weight, embeddings.LayerNorm.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.query.weight, encoder.layer.*.attention.self.key.bias, encoder.layer.*.output.LayerNorm.bias, encoder.layer.*.output.dense.weight, encoder.layer.*.attention.self.value.weight, embeddings.word_embeddings.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.weight, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.value.bias, pooler.dense.bias, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.bias, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.output.LayerNorm.bias, encoder.layer.*.intermediate.dense.weight, pooler.dense.weight, embeddings.token_type_embeddings.weight\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.dense.weight, embeddings.LayerNorm.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.query.weight, encoder.layer.*.attention.self.key.bias, encoder.layer.*.output.LayerNorm.bias, encoder.layer.*.output.dense.weight, encoder.layer.*.attention.self.value.weight, embeddings.word_embeddings.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.weight, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.value.bias, pooler.dense.bias, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.bias, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.output.LayerNorm.bias, encoder.layer.*.intermediate.dense.weight, pooler.dense.weight, embeddings.token_type_embeddings.weight\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.dense.weight, embeddings.LayerNorm.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.query.weight, encoder.layer.*.attention.self.key.bias, encoder.layer.*.output.LayerNorm.bias, encoder.layer.*.output.dense.weight, encoder.layer.*.attention.self.value.weight, embeddings.word_embeddings.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.weight, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.value.bias, pooler.dense.bias, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.bias, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.output.LayerNorm.bias, encoder.layer.*.intermediate.dense.weight, pooler.dense.weight, embeddings.token_type_embeddings.weight\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.dense.weight, embeddings.LayerNorm.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.query.weight, encoder.layer.*.attention.self.key.bias, encoder.layer.*.output.LayerNorm.bias, encoder.layer.*.output.dense.weight, encoder.layer.*.attention.self.value.weight, embeddings.word_embeddings.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.weight, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.value.bias, pooler.dense.bias, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.bias, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.output.LayerNorm.bias, encoder.layer.*.intermediate.dense.weight, pooler.dense.weight, embeddings.token_type_embeddings.weight\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.dense.weight, embeddings.LayerNorm.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.query.weight, encoder.layer.*.attention.self.key.bias, encoder.layer.*.output.LayerNorm.bias, encoder.layer.*.output.dense.weight, encoder.layer.*.attention.self.value.weight, embeddings.word_embeddings.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.weight, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.value.bias, pooler.dense.bias, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.bias, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.output.LayerNorm.bias, encoder.layer.*.intermediate.dense.weight, pooler.dense.weight, embeddings.token_type_embeddings.weight\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.dense.weight, embeddings.LayerNorm.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.query.weight, encoder.layer.*.attention.self.key.bias, encoder.layer.*.output.LayerNorm.bias, encoder.layer.*.output.dense.weight, encoder.layer.*.attention.self.value.weight, embeddings.word_embeddings.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.weight, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.value.bias, pooler.dense.bias, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.bias, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.output.LayerNorm.bias, encoder.layer.*.intermediate.dense.weight, pooler.dense.weight, embeddings.token_type_embeddings.weight\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.dense.weight, embeddings.LayerNorm.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.query.weight, encoder.layer.*.attention.self.key.bias, encoder.layer.*.output.LayerNorm.bias, encoder.layer.*.output.dense.weight, encoder.layer.*.attention.self.value.weight, embeddings.word_embeddings.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.weight, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.value.bias, pooler.dense.bias, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.bias, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.output.LayerNorm.bias, encoder.layer.*.intermediate.dense.weight, pooler.dense.weight, embeddings.token_type_embeddings.weight\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.dense.weight, embeddings.LayerNorm.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.query.weight, encoder.layer.*.attention.self.key.bias, encoder.layer.*.output.LayerNorm.bias, encoder.layer.*.output.dense.weight, encoder.layer.*.attention.self.value.weight, embeddings.word_embeddings.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.weight, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.value.bias, pooler.dense.bias, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.bias, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.output.LayerNorm.bias, encoder.layer.*.intermediate.dense.weight, pooler.dense.weight, embeddings.token_type_embeddings.weight\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.dense.weight, embeddings.LayerNorm.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.query.weight, encoder.layer.*.attention.self.key.bias, encoder.layer.*.output.LayerNorm.bias, encoder.layer.*.output.dense.weight, encoder.layer.*.attention.self.value.weight, embeddings.word_embeddings.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.weight, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.value.bias, pooler.dense.bias, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.bias, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.output.LayerNorm.bias, encoder.layer.*.intermediate.dense.weight, pooler.dense.weight, embeddings.token_type_embeddings.weight\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.dense.weight, embeddings.LayerNorm.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.query.weight, encoder.layer.*.attention.self.key.bias, encoder.layer.*.output.LayerNorm.bias, encoder.layer.*.output.dense.weight, encoder.layer.*.attention.self.value.weight, embeddings.word_embeddings.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.weight, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.value.bias, pooler.dense.bias, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.bias, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.output.LayerNorm.bias, encoder.layer.*.intermediate.dense.weight, pooler.dense.weight, embeddings.token_type_embeddings.weight\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.dense.weight, embeddings.LayerNorm.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.query.weight, encoder.layer.*.attention.self.key.bias, encoder.layer.*.output.LayerNorm.bias, encoder.layer.*.output.dense.weight, encoder.layer.*.attention.self.value.weight, embeddings.word_embeddings.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.weight, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.value.bias, pooler.dense.bias, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.bias, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.output.LayerNorm.bias, encoder.layer.*.intermediate.dense.weight, pooler.dense.weight, embeddings.token_type_embeddings.weight\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.dense.weight, embeddings.LayerNorm.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.query.weight, encoder.layer.*.attention.self.key.bias, encoder.layer.*.output.LayerNorm.bias, encoder.layer.*.output.dense.weight, encoder.layer.*.attention.self.value.weight, embeddings.word_embeddings.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.weight, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.value.bias, pooler.dense.bias, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.bias, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.output.LayerNorm.bias, encoder.layer.*.intermediate.dense.weight, pooler.dense.weight, embeddings.token_type_embeddings.weight\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.dense.weight, embeddings.LayerNorm.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.query.weight, encoder.layer.*.attention.self.key.bias, encoder.layer.*.output.LayerNorm.bias, encoder.layer.*.output.dense.weight, encoder.layer.*.attention.self.value.weight, embeddings.word_embeddings.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.weight, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.value.bias, pooler.dense.bias, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.bias, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.output.LayerNorm.bias, encoder.layer.*.intermediate.dense.weight, pooler.dense.weight, embeddings.token_type_embeddings.weight\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.dense.weight, embeddings.LayerNorm.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.query.weight, encoder.layer.*.attention.self.key.bias, encoder.layer.*.output.LayerNorm.bias, encoder.layer.*.output.dense.weight, encoder.layer.*.attention.self.value.weight, embeddings.word_embeddings.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.weight, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.value.bias, pooler.dense.bias, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.bias, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.output.LayerNorm.bias, encoder.layer.*.intermediate.dense.weight, pooler.dense.weight, embeddings.token_type_embeddings.weight\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.dense.weight, embeddings.LayerNorm.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.query.weight, encoder.layer.*.attention.self.key.bias, encoder.layer.*.output.LayerNorm.bias, encoder.layer.*.output.dense.weight, encoder.layer.*.attention.self.value.weight, embeddings.word_embeddings.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.weight, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.value.bias, pooler.dense.bias, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.bias, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.output.LayerNorm.bias, encoder.layer.*.intermediate.dense.weight, pooler.dense.weight, embeddings.token_type_embeddings.weight\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.dense.weight, embeddings.LayerNorm.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.query.weight, encoder.layer.*.attention.self.key.bias, encoder.layer.*.output.LayerNorm.bias, encoder.layer.*.output.dense.weight, encoder.layer.*.attention.self.value.weight, embeddings.word_embeddings.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.weight, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.value.bias, pooler.dense.bias, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.bias, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.output.LayerNorm.bias, encoder.layer.*.intermediate.dense.weight, pooler.dense.weight, embeddings.token_type_embeddings.weight\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.dense.weight, embeddings.LayerNorm.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.query.weight, encoder.layer.*.attention.self.key.bias, encoder.layer.*.output.LayerNorm.bias, encoder.layer.*.output.dense.weight, encoder.layer.*.attention.self.value.weight, embeddings.word_embeddings.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.weight, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.value.bias, pooler.dense.bias, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.bias, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.output.LayerNorm.bias, encoder.layer.*.intermediate.dense.weight, pooler.dense.weight, embeddings.token_type_embeddings.weight\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.dense.weight, embeddings.LayerNorm.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.query.weight, encoder.layer.*.attention.self.key.bias, encoder.layer.*.output.LayerNorm.bias, encoder.layer.*.output.dense.weight, encoder.layer.*.attention.self.value.weight, embeddings.word_embeddings.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.weight, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.value.bias, pooler.dense.bias, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.bias, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.output.LayerNorm.bias, encoder.layer.*.intermediate.dense.weight, pooler.dense.weight, embeddings.token_type_embeddings.weight\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.dense.weight, embeddings.LayerNorm.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.query.weight, encoder.layer.*.attention.self.key.bias, encoder.layer.*.output.LayerNorm.bias, encoder.layer.*.output.dense.weight, encoder.layer.*.attention.self.value.weight, embeddings.word_embeddings.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.weight, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.value.bias, pooler.dense.bias, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.bias, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.output.LayerNorm.bias, encoder.layer.*.intermediate.dense.weight, pooler.dense.weight, embeddings.token_type_embeddings.weight\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.dense.weight, embeddings.LayerNorm.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.query.weight, encoder.layer.*.attention.self.key.bias, encoder.layer.*.output.LayerNorm.bias, encoder.layer.*.output.dense.weight, encoder.layer.*.attention.self.value.weight, embeddings.word_embeddings.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.weight, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.value.bias, pooler.dense.bias, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.bias, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.output.LayerNorm.bias, encoder.layer.*.intermediate.dense.weight, pooler.dense.weight, embeddings.token_type_embeddings.weight\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.dense.weight, embeddings.LayerNorm.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.query.weight, encoder.layer.*.attention.self.key.bias, encoder.layer.*.output.LayerNorm.bias, encoder.layer.*.output.dense.weight, encoder.layer.*.attention.self.value.weight, embeddings.word_embeddings.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.weight, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.value.bias, pooler.dense.bias, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.bias, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.output.LayerNorm.bias, encoder.layer.*.intermediate.dense.weight, pooler.dense.weight, embeddings.token_type_embeddings.weight\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.dense.weight, embeddings.LayerNorm.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.query.weight, encoder.layer.*.attention.self.key.bias, encoder.layer.*.output.LayerNorm.bias, encoder.layer.*.output.dense.weight, encoder.layer.*.attention.self.value.weight, embeddings.word_embeddings.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.weight, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.value.bias, pooler.dense.bias, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.bias, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.output.LayerNorm.bias, encoder.layer.*.intermediate.dense.weight, pooler.dense.weight, embeddings.token_type_embeddings.weight\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.dense.weight, embeddings.LayerNorm.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.query.weight, encoder.layer.*.attention.self.key.bias, encoder.layer.*.output.LayerNorm.bias, encoder.layer.*.output.dense.weight, encoder.layer.*.attention.self.value.weight, embeddings.word_embeddings.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.weight, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.value.bias, pooler.dense.bias, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.bias, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.output.LayerNorm.bias, encoder.layer.*.intermediate.dense.weight, pooler.dense.weight, embeddings.token_type_embeddings.weight\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.dense.weight, embeddings.LayerNorm.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.query.weight, encoder.layer.*.attention.self.key.bias, encoder.layer.*.output.LayerNorm.bias, encoder.layer.*.output.dense.weight, encoder.layer.*.attention.self.value.weight, embeddings.word_embeddings.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.weight, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.value.bias, pooler.dense.bias, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.bias, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.output.LayerNorm.bias, encoder.layer.*.intermediate.dense.weight, pooler.dense.weight, embeddings.token_type_embeddings.weight\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.dense.weight, embeddings.LayerNorm.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.query.weight, encoder.layer.*.attention.self.key.bias, encoder.layer.*.output.LayerNorm.bias, encoder.layer.*.output.dense.weight, encoder.layer.*.attention.self.value.weight, embeddings.word_embeddings.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.weight, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.value.bias, pooler.dense.bias, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.bias, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.output.LayerNorm.bias, encoder.layer.*.intermediate.dense.weight, pooler.dense.weight, embeddings.token_type_embeddings.weight\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.dense.weight, embeddings.LayerNorm.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.query.weight, encoder.layer.*.attention.self.key.bias, encoder.layer.*.output.LayerNorm.bias, encoder.layer.*.output.dense.weight, encoder.layer.*.attention.self.value.weight, embeddings.word_embeddings.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.weight, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.value.bias, pooler.dense.bias, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.bias, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.output.LayerNorm.bias, encoder.layer.*.intermediate.dense.weight, pooler.dense.weight, embeddings.token_type_embeddings.weight\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.dense.weight, embeddings.LayerNorm.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.query.weight, encoder.layer.*.attention.self.key.bias, encoder.layer.*.output.LayerNorm.bias, encoder.layer.*.output.dense.weight, encoder.layer.*.attention.self.value.weight, embeddings.word_embeddings.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.weight, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.value.bias, pooler.dense.bias, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.bias, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.output.LayerNorm.bias, encoder.layer.*.intermediate.dense.weight, pooler.dense.weight, embeddings.token_type_embeddings.weight\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.dense.weight, embeddings.LayerNorm.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.query.weight, encoder.layer.*.attention.self.key.bias, encoder.layer.*.output.LayerNorm.bias, encoder.layer.*.output.dense.weight, encoder.layer.*.attention.self.value.weight, embeddings.word_embeddings.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.weight, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.value.bias, pooler.dense.bias, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.bias, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.output.LayerNorm.bias, encoder.layer.*.intermediate.dense.weight, pooler.dense.weight, embeddings.token_type_embeddings.weight\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.dense.weight, embeddings.LayerNorm.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.query.weight, encoder.layer.*.attention.self.key.bias, encoder.layer.*.output.LayerNorm.bias, encoder.layer.*.output.dense.weight, encoder.layer.*.attention.self.value.weight, embeddings.word_embeddings.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.weight, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.value.bias, pooler.dense.bias, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.bias, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.output.LayerNorm.bias, encoder.layer.*.intermediate.dense.weight, pooler.dense.weight, embeddings.token_type_embeddings.weight\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.dense.weight, embeddings.LayerNorm.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.query.weight, encoder.layer.*.attention.self.key.bias, encoder.layer.*.output.LayerNorm.bias, encoder.layer.*.output.dense.weight, encoder.layer.*.attention.self.value.weight, embeddings.word_embeddings.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.weight, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.value.bias, pooler.dense.bias, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.bias, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.output.LayerNorm.bias, encoder.layer.*.intermediate.dense.weight, pooler.dense.weight, embeddings.token_type_embeddings.weight\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.dense.weight, embeddings.LayerNorm.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.query.weight, encoder.layer.*.attention.self.key.bias, encoder.layer.*.output.LayerNorm.bias, encoder.layer.*.output.dense.weight, encoder.layer.*.attention.self.value.weight, embeddings.word_embeddings.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.weight, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.value.bias, pooler.dense.bias, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.bias, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.output.LayerNorm.bias, encoder.layer.*.intermediate.dense.weight, pooler.dense.weight, embeddings.token_type_embeddings.weight\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.dense.weight, embeddings.LayerNorm.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.query.weight, encoder.layer.*.attention.self.key.bias, encoder.layer.*.output.LayerNorm.bias, encoder.layer.*.output.dense.weight, encoder.layer.*.attention.self.value.weight, embeddings.word_embeddings.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.weight, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.value.bias, pooler.dense.bias, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.bias, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.output.LayerNorm.bias, encoder.layer.*.intermediate.dense.weight, pooler.dense.weight, embeddings.token_type_embeddings.weight\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.dense.weight, embeddings.LayerNorm.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.query.weight, encoder.layer.*.attention.self.key.bias, encoder.layer.*.output.LayerNorm.bias, encoder.layer.*.output.dense.weight, encoder.layer.*.attention.self.value.weight, embeddings.word_embeddings.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.weight, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.value.bias, pooler.dense.bias, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.bias, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.output.LayerNorm.bias, encoder.layer.*.intermediate.dense.weight, pooler.dense.weight, embeddings.token_type_embeddings.weight\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.dense.weight, embeddings.LayerNorm.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.query.weight, encoder.layer.*.attention.self.key.bias, encoder.layer.*.output.LayerNorm.bias, encoder.layer.*.output.dense.weight, encoder.layer.*.attention.self.value.weight, embeddings.word_embeddings.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.weight, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.value.bias, pooler.dense.bias, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.bias, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.output.LayerNorm.bias, encoder.layer.*.intermediate.dense.weight, pooler.dense.weight, embeddings.token_type_embeddings.weight\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.dense.weight, embeddings.LayerNorm.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.query.weight, encoder.layer.*.attention.self.key.bias, encoder.layer.*.output.LayerNorm.bias, encoder.layer.*.output.dense.weight, encoder.layer.*.attention.self.value.weight, embeddings.word_embeddings.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.weight, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.value.bias, pooler.dense.bias, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.bias, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.output.LayerNorm.bias, encoder.layer.*.intermediate.dense.weight, pooler.dense.weight, embeddings.token_type_embeddings.weight\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.dense.weight, embeddings.LayerNorm.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.query.weight, encoder.layer.*.attention.self.key.bias, encoder.layer.*.output.LayerNorm.bias, encoder.layer.*.output.dense.weight, encoder.layer.*.attention.self.value.weight, embeddings.word_embeddings.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.weight, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.value.bias, pooler.dense.bias, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.bias, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.output.LayerNorm.bias, encoder.layer.*.intermediate.dense.weight, pooler.dense.weight, embeddings.token_type_embeddings.weight\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.dense.weight, embeddings.LayerNorm.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.query.weight, encoder.layer.*.attention.self.key.bias, encoder.layer.*.output.LayerNorm.bias, encoder.layer.*.output.dense.weight, encoder.layer.*.attention.self.value.weight, embeddings.word_embeddings.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.weight, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.value.bias, pooler.dense.bias, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.bias, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.output.LayerNorm.bias, encoder.layer.*.intermediate.dense.weight, pooler.dense.weight, embeddings.token_type_embeddings.weight\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.dense.weight, embeddings.LayerNorm.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.query.weight, encoder.layer.*.attention.self.key.bias, encoder.layer.*.output.LayerNorm.bias, encoder.layer.*.output.dense.weight, encoder.layer.*.attention.self.value.weight, embeddings.word_embeddings.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.weight, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.value.bias, pooler.dense.bias, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.bias, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.output.LayerNorm.bias, encoder.layer.*.intermediate.dense.weight, pooler.dense.weight, embeddings.token_type_embeddings.weight\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.dense.weight, embeddings.LayerNorm.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.query.weight, encoder.layer.*.attention.self.key.bias, encoder.layer.*.output.LayerNorm.bias, encoder.layer.*.output.dense.weight, encoder.layer.*.attention.self.value.weight, embeddings.word_embeddings.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.weight, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.value.bias, pooler.dense.bias, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.bias, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.output.LayerNorm.bias, encoder.layer.*.intermediate.dense.weight, pooler.dense.weight, embeddings.token_type_embeddings.weight\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.dense.weight, embeddings.LayerNorm.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.query.weight, encoder.layer.*.attention.self.key.bias, encoder.layer.*.output.LayerNorm.bias, encoder.layer.*.output.dense.weight, encoder.layer.*.attention.self.value.weight, embeddings.word_embeddings.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.weight, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.value.bias, pooler.dense.bias, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.bias, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.output.LayerNorm.bias, encoder.layer.*.intermediate.dense.weight, pooler.dense.weight, embeddings.token_type_embeddings.weight\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.dense.weight, embeddings.LayerNorm.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.query.weight, encoder.layer.*.attention.self.key.bias, encoder.layer.*.output.LayerNorm.bias, encoder.layer.*.output.dense.weight, encoder.layer.*.attention.self.value.weight, embeddings.word_embeddings.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.weight, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.value.bias, pooler.dense.bias, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.bias, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.output.LayerNorm.bias, encoder.layer.*.intermediate.dense.weight, pooler.dense.weight, embeddings.token_type_embeddings.weight\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.dense.weight, embeddings.LayerNorm.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.query.weight, encoder.layer.*.attention.self.key.bias, encoder.layer.*.output.LayerNorm.bias, encoder.layer.*.output.dense.weight, encoder.layer.*.attention.self.value.weight, embeddings.word_embeddings.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.weight, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.value.bias, pooler.dense.bias, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.bias, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.output.LayerNorm.bias, encoder.layer.*.intermediate.dense.weight, pooler.dense.weight, embeddings.token_type_embeddings.weight\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.dense.weight, embeddings.LayerNorm.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.query.weight, encoder.layer.*.attention.self.key.bias, encoder.layer.*.output.LayerNorm.bias, encoder.layer.*.output.dense.weight, encoder.layer.*.attention.self.value.weight, embeddings.word_embeddings.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.weight, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.value.bias, pooler.dense.bias, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.bias, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.output.LayerNorm.bias, encoder.layer.*.intermediate.dense.weight, pooler.dense.weight, embeddings.token_type_embeddings.weight\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.dense.weight, embeddings.LayerNorm.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.query.weight, encoder.layer.*.attention.self.key.bias, encoder.layer.*.output.LayerNorm.bias, encoder.layer.*.output.dense.weight, encoder.layer.*.attention.self.value.weight, embeddings.word_embeddings.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.weight, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.value.bias, pooler.dense.bias, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.bias, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.output.LayerNorm.bias, encoder.layer.*.intermediate.dense.weight, pooler.dense.weight, embeddings.token_type_embeddings.weight\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.dense.weight, embeddings.LayerNorm.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.query.weight, encoder.layer.*.attention.self.key.bias, encoder.layer.*.output.LayerNorm.bias, encoder.layer.*.output.dense.weight, encoder.layer.*.attention.self.value.weight, embeddings.word_embeddings.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.weight, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.value.bias, pooler.dense.bias, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.bias, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.output.LayerNorm.bias, encoder.layer.*.intermediate.dense.weight, pooler.dense.weight, embeddings.token_type_embeddings.weight\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.dense.weight, embeddings.LayerNorm.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.query.weight, encoder.layer.*.attention.self.key.bias, encoder.layer.*.output.LayerNorm.bias, encoder.layer.*.output.dense.weight, encoder.layer.*.attention.self.value.weight, embeddings.word_embeddings.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.weight, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.value.bias, pooler.dense.bias, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.bias, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.output.LayerNorm.bias, encoder.layer.*.intermediate.dense.weight, pooler.dense.weight, embeddings.token_type_embeddings.weight\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.dense.weight, embeddings.LayerNorm.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.query.weight, encoder.layer.*.attention.self.key.bias, encoder.layer.*.output.LayerNorm.bias, encoder.layer.*.output.dense.weight, encoder.layer.*.attention.self.value.weight, embeddings.word_embeddings.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.weight, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.value.bias, pooler.dense.bias, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.bias, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.output.LayerNorm.bias, encoder.layer.*.intermediate.dense.weight, pooler.dense.weight, embeddings.token_type_embeddings.weight\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.dense.weight, embeddings.LayerNorm.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.query.weight, encoder.layer.*.attention.self.key.bias, encoder.layer.*.output.LayerNorm.bias, encoder.layer.*.output.dense.weight, encoder.layer.*.attention.self.value.weight, embeddings.word_embeddings.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.weight, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.value.bias, pooler.dense.bias, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.bias, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.output.LayerNorm.bias, encoder.layer.*.intermediate.dense.weight, pooler.dense.weight, embeddings.token_type_embeddings.weight\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.dense.weight, embeddings.LayerNorm.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.query.weight, encoder.layer.*.attention.self.key.bias, encoder.layer.*.output.LayerNorm.bias, encoder.layer.*.output.dense.weight, encoder.layer.*.attention.self.value.weight, embeddings.word_embeddings.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.weight, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.value.bias, pooler.dense.bias, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.bias, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.output.LayerNorm.bias, encoder.layer.*.intermediate.dense.weight, pooler.dense.weight, embeddings.token_type_embeddings.weight\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.dense.weight, embeddings.LayerNorm.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.query.weight, encoder.layer.*.attention.self.key.bias, encoder.layer.*.output.LayerNorm.bias, encoder.layer.*.output.dense.weight, encoder.layer.*.attention.self.value.weight, embeddings.word_embeddings.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.weight, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.value.bias, pooler.dense.bias, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.bias, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.output.LayerNorm.bias, encoder.layer.*.intermediate.dense.weight, pooler.dense.weight, embeddings.token_type_embeddings.weight\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.dense.weight, embeddings.LayerNorm.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.query.weight, encoder.layer.*.attention.self.key.bias, encoder.layer.*.output.LayerNorm.bias, encoder.layer.*.output.dense.weight, encoder.layer.*.attention.self.value.weight, embeddings.word_embeddings.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.weight, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.value.bias, pooler.dense.bias, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.bias, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.output.LayerNorm.bias, encoder.layer.*.intermediate.dense.weight, pooler.dense.weight, embeddings.token_type_embeddings.weight\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.dense.weight, embeddings.LayerNorm.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.query.weight, encoder.layer.*.attention.self.key.bias, encoder.layer.*.output.LayerNorm.bias, encoder.layer.*.output.dense.weight, encoder.layer.*.attention.self.value.weight, embeddings.word_embeddings.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.weight, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.value.bias, pooler.dense.bias, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.bias, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.output.LayerNorm.bias, encoder.layer.*.intermediate.dense.weight, pooler.dense.weight, embeddings.token_type_embeddings.weight\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.dense.weight, embeddings.LayerNorm.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.query.weight, encoder.layer.*.attention.self.key.bias, encoder.layer.*.output.LayerNorm.bias, encoder.layer.*.output.dense.weight, encoder.layer.*.attention.self.value.weight, embeddings.word_embeddings.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.weight, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.value.bias, pooler.dense.bias, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.bias, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.output.LayerNorm.bias, encoder.layer.*.intermediate.dense.weight, pooler.dense.weight, embeddings.token_type_embeddings.weight\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "         --- Detalles de Respuestas Generadas (DataFrame Completo) ---          \n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import warnings\n",
    "import logging\n",
    "import torch\n",
    "import textstat\n",
    "import nltk\n",
    "import time\n",
    "import re\n",
    "import yaml # Importamos PyYAML\n",
    "\n",
    "# Importaciones específicas de Code 1 para LLMs y Azure\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from openai import AzureOpenAI\n",
    "import google.generativeai as genai\n",
    "from bert_score import score as bert_score\n",
    "from google.generativeai.types import GenerationConfig\n",
    "\n",
    "# --- Cargar configuración desde YAML ---\n",
    "try:\n",
    "    with open('config.yaml', 'r') as file:\n",
    "        config = yaml.safe_load(file)\n",
    "    print(\"Configuración cargada exitosamente desde config.yaml\")\n",
    "except FileNotFoundError:\n",
    "    print(\"Error: 'config.yaml' no encontrado. Asegúrate de que el archivo exista en el mismo directorio.\")\n",
    "    exit()\n",
    "except yaml.YAMLError as e:\n",
    "    print(f\"Error al parsear 'config.yaml': {e}\")\n",
    "    exit()\n",
    "\n",
    "# Temperatura del asistente de creación de preguntas y respuestas\n",
    "MD_GEN = \"GEMINI\"\n",
    "modelo_banco_generacion = \"models/gemini-1.5-pro-latest\"\n",
    "temperature_gen_assitant = 0.7\n",
    "\n",
    "# Asignar prompts y parámetros desde el archivo de configuración\n",
    "prompt_systemE1 = config['prompts']['system_prompt_E1']\n",
    "prompt_systemE2 = config['prompts']['system_prompt_E2']\n",
    "prompt_generacion_qa = config['prompts']['qa_generation_prompt']\n",
    "\n",
    "# Parámetros desde el YAML\n",
    "num_pares_a_generar = config['parameters']['num_qa_to_generate']\n",
    "chunk_text = config['parameters']['chunk_size']\n",
    "chunk_over = config['parameters']['chunk_overlap']\n",
    "top_join = config['parameters']['top_join_chunks'] # Este es el 'top_join' para la búsqueda semántica\n",
    "max_attempts = config['parameters']['max_api_attempts']\n",
    "wait_time_on_error_s = config['parameters']['api_wait_time_on_error_s']\n",
    "temperature_ = config['parameters']['gpt_temperature']\n",
    "temperature_2 = config['parameters']['gemma_temperature']\n",
    "perfil_usuario = config['parameters']['user_profile']\n",
    "contexto_generico = config['parameters']['generic_context']\n",
    "\n",
    "# Umbrales desde el YAML\n",
    "umbrales = [\n",
    "    config['thresholds']['similitud_min'],\n",
    "    config['thresholds']['coherencia_min'],\n",
    "    config['thresholds']['personalizacion_min'],\n",
    "    config['thresholds']['fluidez_min'],\n",
    "    config['thresholds']['alucinacion_min']\n",
    "]\n",
    "\n",
    "#################################################\n",
    "############## Modelos a Comparar ###############\n",
    "#################################################\n",
    "\n",
    "#################### modelo 1 ###################\n",
    "model_embedding = \"text-embedding-ada-002\"\n",
    "#modelo_banco = \"gpt-35-turbo-16k-PQR\"\n",
    "modelo_banco = \"gpt-4o-mini_clasificacion-PQRS\"\n",
    "\n",
    "################### modelo 2 ####################\n",
    "model_embedding2 = \"models/text-embedding-004\"\n",
    "#modelo_banco2 = \"models/gemma-3-27b-it\"\n",
    "modelo_banco2 = \"models/gemini-2.5-flash\"\n",
    "\n",
    "# --- Configuración inicial ---\n",
    "nltk.download('punkt', quiet=True)\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "logging.getLogger(\"transformers.modeling_utils\").setLevel(logging.ERROR)\n",
    "logging.getLogger(\"transformers.configuration_utils\").setLevel(logging.ERROR)\n",
    "logging.getLogger(\"transformers.tokenization_utils_base\").setLevel(logging.ERROR)\n",
    "\n",
    "def medir_coherencia(pregunta, respuesta):\n",
    "    \"\"\"Mide la coherencia entre una pregunta y su respuesta utilizando BERTScore.\"\"\"\n",
    "    if not respuesta.strip() or not pregunta.strip():\n",
    "        return 0.0\n",
    "    # Utiliza 'en' como idioma para BERTScore con 'roberta-base'\n",
    "    P, R, F1 = bert_score([respuesta], [pregunta], lang=\"en\", model_type=\"roberta-base\", rescale_with_baseline=False)\n",
    "    return float(F1[0])\n",
    "\n",
    "def medir_personalizacion(perfil_usuario, respuesta):\n",
    "    \"\"\"Mide qué tan personalizada es una respuesta en relación con un perfil de usuario.\"\"\"\n",
    "    if not respuesta.strip() or not perfil_usuario.strip():\n",
    "        return 0.0\n",
    "    P, R, F1 = bert_score([respuesta], [perfil_usuario], lang=\"en\", model_type=\"roberta-base\", rescale_with_baseline=False)\n",
    "    return float(F1[0])\n",
    "\n",
    "def medir_fluidez(texto):\n",
    "    \"\"\"Mide la fluidez de un texto utilizando el índice de facilidad de lectura de Flesch.\"\"\"\n",
    "    if not texto.strip():\n",
    "        return 0.0\n",
    "    return textstat.flesch_reading_ease(texto)\n",
    "\n",
    "def medir_alucinacion(respuesta, contexto_fuente):\n",
    "    \"\"\"Mide la alucinación comparando la respuesta con un contexto fuente (usando BERTScore).\"\"\"\n",
    "    if not respuesta.strip() or not contexto_fuente.strip():\n",
    "        return 0.0\n",
    "    P, R, F1 = bert_score([respuesta], [contexto_fuente], lang=\"en\", model_type=\"roberta-base\", rescale_with_baseline=False)\n",
    "    return float(F1[0])\n",
    "\n",
    "############## Modelo 1 (Azure OpenAI) #################\n",
    "# 1. Cargar credenciales desde JSON\n",
    "def cargar_credenciales(ruta_credenciales):\n",
    "    with open(ruta_credenciales, \"r\") as file:\n",
    "        return json.load(file)\n",
    "\n",
    "# 2. Crear cliente AzureOpenAI\n",
    "def crear_cliente_azure(creds):\n",
    "    client = AzureOpenAI(\n",
    "        api_key=creds[\"AZURE_API_KEY\"],\n",
    "        api_version=creds[\"AZURE_API_VERSION\"],\n",
    "        azure_endpoint=creds[\"AZURE_ENDPOINT\"]\n",
    "    )\n",
    "    return client\n",
    "\n",
    "# 3. Generar embeddings usando cliente (AzureOpenAI)\n",
    "def text_embedding(text):\n",
    "    input_text = [text] if isinstance(text, str) else text\n",
    "    try:\n",
    "        embeddings = client.embeddings.create(model=model_embedding,\n",
    "                                              input=input_text,\n",
    "                                              encoding_format=\"float\")\n",
    "        return embeddings.data[0].embedding\n",
    "    except Exception as e:\n",
    "        print(f\"Error al generar embedding con Azure: {e}\")\n",
    "        return np.zeros(1536) # Devuelve un array de ceros en caso de error\n",
    "\n",
    "# 4. Inicialización Global del Cliente Azure y Credenciales\n",
    "try:\n",
    "    ruta_credenciales = \"credentials.json\"\n",
    "    creds = cargar_credenciales(ruta_credenciales)\n",
    "    client = crear_cliente_azure(creds)\n",
    "    print(\"Cliente Azure OpenAI inicializado.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error al inicializar cliente Azure OpenAI: {e}\")\n",
    "    client = None # Asegurarse de que el cliente es None si falla la inicialización\n",
    "\n",
    "############## Modelo 2 (Gemini/Gemma) ################\n",
    "def cargar_credenciales2(ruta_credenciales):\n",
    "    with open(ruta_credenciales, \"r\") as file:\n",
    "        return json.load(file)\n",
    "\n",
    "try:\n",
    "    ruta_credenciales2 = \"credentials2.json\"\n",
    "    creds2 = cargar_credenciales2(ruta_credenciales2)\n",
    "    genai.configure(api_key=creds2[\"GEMINI_API_KEY\"])\n",
    "    print(\"Cliente Gemini/Gemma configurado.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error al configurar cliente Gemini/Gemma: {e}\")\n",
    "    genai = None # Asegurarse de que genai es None si falla la configuración\n",
    "\n",
    "def text_embedding_gemma(text):\n",
    "    if genai is None:\n",
    "        print(\"Error: El cliente Gemini/Gemma no está configurado para embeddings.\")\n",
    "        return np.zeros(768)\n",
    "    try:\n",
    "        result = genai.embed_content(model=model_embedding2,\n",
    "                                     content=text,\n",
    "                                     task_type=\"RETRIEVAL_QUERY\") # O RETRIEVAL_DOCUMENT\n",
    "        return result['embedding']\n",
    "    except Exception as e:\n",
    "        print(f\"Error al generar embedding con Gemma: {e}\")\n",
    "        return np.zeros(768) # Devuelve un array de ceros en caso de error\n",
    "\n",
    "# --- Carga y chunking del PDF ---\n",
    "print(\"Cargando y procesando PDF...\")\n",
    "try:\n",
    "    loader = PyPDFLoader(\"llm_doc.pdf\")\n",
    "    documents = loader.load()\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_text, chunk_overlap=chunk_over)\n",
    "    doc_splits = text_splitter.split_documents(documents)\n",
    "\n",
    "    data = [{'Chunks': doc.page_content, 'Metadata': doc.metadata} for doc in doc_splits]\n",
    "    df_vector_store = pd.DataFrame(data)\n",
    "\n",
    "    df_vector_store_gpt = df_vector_store.copy()\n",
    "    df_vector_store_gem = df_vector_store.copy()\n",
    "\n",
    "    # Generar embeddings para GPT\n",
    "    if client:\n",
    "        print(\"Generando embeddings para GPT (Azure)...\")\n",
    "        df_vector_store_gpt[\"Embedding\"] = df_vector_store_gpt[\"Chunks\"].apply(lambda x: text_embedding(x))\n",
    "        df_vector_store_gpt[\"Embedding\"] = df_vector_store_gpt[\"Embedding\"].apply(np.array)\n",
    "    else:\n",
    "        print(\"Saltando embeddings para GPT, cliente Azure no inicializado.\")\n",
    "        df_vector_store_gpt[\"Embedding\"] = [np.zeros(1536)] * len(df_vector_store_gpt)\n",
    "\n",
    "    # Generar embeddings para Gemma\n",
    "    if genai:\n",
    "        print(\"Generando embeddings para Gemma...\")\n",
    "        df_vector_store_gem[\"Embedding\"] = df_vector_store_gem[\"Chunks\"].apply(text_embedding_gemma)\n",
    "        df_vector_store_gem[\"Embedding\"] = df_vector_store_gem[\"Embedding\"].apply(np.array)\n",
    "    else:\n",
    "        print(\"Saltando embeddings para Gemma, cliente Gemini/Gemma no configurado.\")\n",
    "        df_vector_store_gem[\"Embedding\"] = [np.zeros(768)] * len(df_vector_store_gem)\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error durante la carga y procesamiento del PDF: {e}\")\n",
    "    exit()\n",
    "\n",
    "# --- Cargar preguntas_BOT.xlsx para evitar duplicados (si existe) ---\n",
    "try:\n",
    "    df_preguntas_respuestas_existentes = pd.read_excel('preguntas_BOT.xlsx')\n",
    "except FileNotFoundError:\n",
    "    print(\"Advertencia: 'preguntas_BOT.xlsx' no encontrado. Se asumirá que no hay preguntas existentes.\")\n",
    "    df_preguntas_respuestas_existentes = pd.DataFrame(columns=[\"Pregunta\", \"Respuesta\"])\n",
    "\n",
    "# --- FUNCIÓN PARA GENERAR UN CONTEXTO A PARTIR DE CHUNKS SELECCIONADOS ---\n",
    "def obtener_contexto_para_generacion(df_vector_store, num_chunks=5):\n",
    "    if len(df_vector_store) < num_chunks:\n",
    "        # Si no hay suficientes chunks, usa todos los disponibles\n",
    "        return \"\\n\".join(df_vector_store['Chunks'].tolist())\n",
    "    else:\n",
    "        # Distribuye los chunks equitativamente\n",
    "        indices = np.linspace(0, len(df_vector_store) - 1, num_chunks, dtype=int)\n",
    "        selected_chunks = df_vector_store.loc[indices, 'Chunks'].tolist()\n",
    "        return \"\\n\".join(selected_chunks)\n",
    "\n",
    "# --- GENERACIÓN DE NUEVAS PREGUNTAS Y RESPUESTAS ---\n",
    "nuevos_pares_qa = []\n",
    "\n",
    "# Corregido: Usar 'top_join' para la generación de QA, que ahora proviene del YAML\n",
    "\n",
    "if MD_GEN == \"GPT\":\n",
    "    contexto_para_generacion = obtener_contexto_para_generacion(df_vector_store_gem, \n",
    "                                                            num_chunks=top_join)\n",
    "else:\n",
    "    contexto_para_generacion = obtener_contexto_para_generacion(df_vector_store_gpt, \n",
    "                                                            num_chunks=top_join)\n",
    "\n",
    "# Formatear el prompt de generación con el número deseado (ahora `num_pares_a_generar` del YAML)\n",
    "final_prompt_qa = prompt_generacion_qa.format(\n",
    "    source_text=contexto_para_generacion,\n",
    "    num_to_generate=num_pares_a_generar\n",
    ")\n",
    "\n",
    "print(f\"Enviando solicitud al LLM para generar {num_pares_a_generar} pares QA...\")\n",
    "try:\n",
    "    if client: # Solo intentar si el cliente Azure está inicializado\n",
    "        if MD_GEN == \"GPT\":\n",
    "            completion_qa = client.chat.completions.create(\n",
    "            model=modelo_banco_generacion,\n",
    "            temperature=temperature_gen_assitant, # Usa la temperatura de GPT del YAML\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": final_prompt_qa}\n",
    "            ]\n",
    "            )\n",
    "            generated_text = completion_qa.choices[0].message.content\n",
    "        else:\n",
    "            model_2 = genai.GenerativeModel(model_name=modelo_banco_generacion)\n",
    "            full_query_for_gemma = final_prompt_qa\n",
    "            completion2 = model_2.generate_content(\n",
    "            contents=full_query_for_gemma,\n",
    "            generation_config=GenerationConfig(temperature=temperature_gen_assitant)\n",
    "            )\n",
    "            generated_text = completion2.text\n",
    "\n",
    "\n",
    "        print(\"Generación del LLM completada. Procesando pares QA...\")\n",
    "\n",
    "        # --- PARSEO DE LAS RESPUESTAS GENERADAS CON REGEX (PARA MÚLTIPLES PARES) ---\n",
    "        pattern = re.compile(r\"Pregunta:\\s*(.*?)\\s*Respuesta:\\s*(.*?)(?=(?:Pregunta:|$))\", re.DOTALL | re.IGNORECASE)\n",
    "        matches = pattern.findall(generated_text)\n",
    "\n",
    "        for match in matches:\n",
    "            question = match[0].strip()\n",
    "            answer = match[1].strip()\n",
    "\n",
    "            if question and answer: # Asegurarse de que no estén vacíos\n",
    "                # Validar si ya existe para evitar duplicados\n",
    "                if not df_preguntas_respuestas_existentes['Pregunta'].astype(str).str.contains(question, case=False, regex=False).any():\n",
    "                    nuevos_pares_qa.append({\"Pregunta\": question, \"Respuesta\": answer})\n",
    "                else:\n",
    "                    print(f\"  [QA Gen] Par '{question[:30]}...' ya existe en 'preguntas_BOT.xlsx', omitiendo.\")\n",
    "\n",
    "                if len(nuevos_pares_qa) >= num_pares_a_generar:\n",
    "                    break # Detener si ya generamos la cantidad deseada\n",
    "\n",
    "    else:\n",
    "        print(\"Error: El cliente Azure OpenAI no está inicializado. No se pudo generar QA.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error durante la generación o procesamiento de QA: {e}\")\n",
    "\n",
    "# Asegurarse de tener a lo sumo num_pares_a_generar\n",
    "if len(nuevos_pares_qa) > num_pares_a_generar:\n",
    "    nuevos_pares_qa = nuevos_pares_qa[:num_pares_a_generar]\n",
    "elif len(nuevos_pares_qa) < num_pares_a_generar:\n",
    "    print(f\"Advertencia: Solo se pudieron generar {len(nuevos_pares_qa)} pares QA válidos de {num_pares_a_generar} solicitados.\")\n",
    "\n",
    "df_nuevos_qa = pd.DataFrame(nuevos_pares_qa)\n",
    "\n",
    "if df_nuevos_qa.empty:\n",
    "    print(\"No se pudieron generar pares de preguntas y respuestas válidos. Finalizando.\")\n",
    "    exit()\n",
    "\n",
    "def cosine_similarity_for_chunks(row_embedding_chunk, query_vector):\n",
    "    denominator1 = np.linalg.norm(row_embedding_chunk)\n",
    "    denominator2 = np.linalg.norm(query_vector)\n",
    "    dot_prod = np.dot(row_embedding_chunk, query_vector)\n",
    "    if denominator1 == 0 or denominator2 == 0:\n",
    "        return 0.0\n",
    "    return dot_prod / (denominator1 * denominator2)\n",
    "\n",
    "def cosine_final(v1, v2):\n",
    "    norm_v1 = np.linalg.norm(v1)\n",
    "    norm_v2 = np.linalg.norm(v2)\n",
    "    if norm_v1 == 0 or norm_v2 == 0:\n",
    "        return 0.0\n",
    "    return np.dot(v1, v2) / (norm_v1 * norm_v2)\n",
    "\n",
    "def get_context_from_query(query, vector_store, n_chunks, embedding_func_for_query):\n",
    "    # Usar la función de embedding pasada para generar el embedding de la consulta\n",
    "    query_embedding_val = embedding_func_for_query(query)\n",
    "    query_vector = np.array(query_embedding_val)\n",
    "\n",
    "    # Calcular la similitud de coseno\n",
    "    # La función lambda debe capturar query_vector correctamente\n",
    "    top_matched_indices = (\n",
    "        vector_store[\"Embedding\"]\n",
    "        .apply(lambda x: cosine_similarity_for_chunks(x, query_vector))\n",
    "        .sort_values(ascending=False)[:n_chunks]\n",
    "        .index\n",
    "    )\n",
    "    top_matched_df = vector_store[vector_store.index.isin(top_matched_indices)][[\"Chunks\"]]\n",
    "    return list(top_matched_df['Chunks'])\n",
    "\n",
    "resultados, resultados2 = [], []\n",
    "df_respuestas_detalladas = [] # Lista para almacenar los detalles de todas las respuestas\n",
    "\n",
    "# Los prompts ahora se cargan desde el YAML, pero se usan en un bucle para la evaluación\n",
    "prompts_evaluacion = [prompt_systemE1, prompt_systemE2]\n",
    "# Nombres de los prompts para identificarlos en el DataFrame de resultados\n",
    "prompt_names = [\"prompt_systemE1\", \"prompt_systemE2\"]\n",
    "\n",
    "for idx, prompt_system in enumerate(prompts_evaluacion):\n",
    "    current_prompt_name = prompt_names[idx] # Obtener el nombre del prompt actual\n",
    "\n",
    "    for i, row in df_nuevos_qa.iterrows():\n",
    "        pregunta = str(row[\"Pregunta\"])\n",
    "        respuesta_humana = str(row[\"Respuesta\"])\n",
    "\n",
    "        # --- Evaluación para GPT (Modelo 1) ---\n",
    "        if client: # Solo ejecutar si el cliente Azure está inicializado\n",
    "            respuesta_modelo_gpt = \"No se pudo generar respuesta.\"\n",
    "            input_tokens_gpt = 0\n",
    "            output_tokens_gpt = 0\n",
    "            api_latency_gpt = 0\n",
    "            total_exec_time_gpt = 0\n",
    "            successful_gpt_response = False\n",
    "            contexto_gpt_usado = \"\"\n",
    "\n",
    "            for attempt in range(max_attempts):\n",
    "                try:\n",
    "                    start_time_total = time.time()\n",
    "                    context_chunks_gpt = get_context_from_query(pregunta, df_vector_store_gpt, top_join, text_embedding)\n",
    "                    contexto_gpt_usado = \" \".join(context_chunks_gpt) # Para alucinación\n",
    "                    custom_prompt_gpt = prompt_system.format(source=str(context_chunks_gpt))\n",
    "\n",
    "                    llm_start_time = time.time()\n",
    "                    completion = client.chat.completions.create(\n",
    "                        model=modelo_banco,\n",
    "                        temperature=temperature_,\n",
    "                        messages=[\n",
    "                            {\"role\": \"system\", \"content\": custom_prompt_gpt},\n",
    "                            {\"role\": \"user\", \"content\": pregunta}\n",
    "                        ]\n",
    "                    )\n",
    "                    respuesta_modelo_gpt = completion.choices[0].message.content\n",
    "                    llm_end_time = time.time()\n",
    "\n",
    "                    input_tokens_gpt = completion.usage.prompt_tokens\n",
    "                    output_tokens_gpt = completion.usage.completion_tokens\n",
    "                    api_latency_gpt = llm_end_time - llm_start_time\n",
    "                    total_exec_time_gpt = time.time() - start_time_total\n",
    "                    successful_gpt_response = True\n",
    "                    break # Salir del bucle de reintentos si fue exitoso\n",
    "                except Exception as e:\n",
    "                    print(f\"    [GPT] Error en intento {attempt + 1}/{max_attempts} para '{pregunta[:50]}...': {e}\")\n",
    "                    if \"quota\" in str(e).lower() or \"rate limit\" in str(e).lower():\n",
    "                        print(f\"    [GPT] Error de cuota. Esperando {wait_time_on_error_s}s...\")\n",
    "                        time.sleep(wait_time_on_error_s)\n",
    "                    else:\n",
    "                        break # Otros errores no reintentables\n",
    "\n",
    "            # Calcular métricas para GPT\n",
    "            similitud_gpt = cosine_final(np.array(text_embedding(respuesta_modelo_gpt)), np.array(text_embedding(respuesta_humana))) if successful_gpt_response else 0.0\n",
    "            coherencia_gpt = medir_coherencia(pregunta, respuesta_modelo_gpt) if successful_gpt_response else 0.0\n",
    "            personalizacion_gpt = medir_personalizacion(perfil_usuario, respuesta_modelo_gpt) if successful_gpt_response else 0.0\n",
    "            fluidez_gpt = medir_fluidez(respuesta_modelo_gpt) if successful_gpt_response else 0.0\n",
    "            alucinacion_gpt = medir_alucinacion(respuesta_modelo_gpt, contexto_gpt_usado) if successful_gpt_response else 0.0\n",
    "\n",
    "            df_respuestas_detalladas.append({\n",
    "                \"Modelo\": modelo_banco,\n",
    "                \"Prompt_Tipo\": prompt_system, # Usamos el nombre del prompt\n",
    "                \"Pregunta\": pregunta,\n",
    "                \"Respuesta_Humana\": respuesta_humana,\n",
    "                \"Respuesta_Modelo\": respuesta_modelo_gpt,\n",
    "                \"Similitud_Coseno\": similitud_gpt,\n",
    "                \"Coherencia\": coherencia_gpt,\n",
    "                \"Personalizacion\": personalizacion_gpt,\n",
    "                \"Fluidez\": fluidez_gpt,\n",
    "                \"Alucinacion\": alucinacion_gpt,\n",
    "                \"Input_Tokens\": input_tokens_gpt,\n",
    "                \"Output_Tokens\": output_tokens_gpt,\n",
    "                \"API_Latency_s\": api_latency_gpt,\n",
    "                \"Total_Exec_Time_s\": total_exec_time_gpt,\n",
    "                \"Successful_Response\": successful_gpt_response\n",
    "            })\n",
    "        else:\n",
    "            print(f\"Saltando evaluación de GPT para pregunta '{pregunta[:50]}...', cliente no inicializado.\")\n",
    "\n",
    "\n",
    "        # --- Evaluación para GEMINI (Modelo 2) ---\n",
    "        if genai: # Solo ejecutar si el cliente Gemini/Gemma está configurado\n",
    "            respuesta_modelo_gemma = \"No se pudo generar respuesta.\"\n",
    "            input_tokens_gemma = 0\n",
    "            output_tokens_gemma = 0\n",
    "            api_latency_gemma = 0\n",
    "            total_exec_time_gemma = 0\n",
    "            successful_gemma_response = False\n",
    "            contexto_gemma_usado = \"\"\n",
    "\n",
    "            for attempt in range(max_attempts):\n",
    "                try:\n",
    "                    start_time_total = time.time()\n",
    "                    context_chunks_gemma = get_context_from_query(pregunta, df_vector_store_gem, top_join, text_embedding_gemma)\n",
    "                    contexto_gemma_usado = \" \".join(context_chunks_gemma) # Para alucinación\n",
    "                    custom_prompt_gemma = prompt_system.format(source=str(context_chunks_gemma))\n",
    "                    model_2 = genai.GenerativeModel(model_name=modelo_banco2)\n",
    "                    full_query_for_gemma = custom_prompt_gemma + \"\\n\\nPregunta del usuario: \" + pregunta\n",
    "\n",
    "                    llm_start_time = time.time()\n",
    "                    completion2 = model_2.generate_content(\n",
    "                        contents=full_query_for_gemma,\n",
    "                        generation_config=GenerationConfig(temperature=temperature_2)\n",
    "                    )\n",
    "                    respuesta_modelo_gemma = completion2.text\n",
    "                    llm_end_time = time.time()\n",
    "\n",
    "                    # Para Gemini/Gemma, los tokens son una estimación simple si no se exponen directamente en la respuesta\n",
    "                    input_tokens_gemma = len(full_query_for_gemma.split())\n",
    "                    output_tokens_gemma = len(respuesta_modelo_gemma.split())\n",
    "                    api_latency_gemma = llm_end_time - llm_start_time\n",
    "                    total_exec_time_gemma = time.time() - start_time_total\n",
    "                    successful_gemma_response = True\n",
    "                    break # Salir del bucle de reintentos si fue exitoso\n",
    "                except Exception as e:\n",
    "                    print(f\"    [GEMINI] Error en intento {attempt + 1}/{max_attempts} para '{pregunta[:50]}...': {e}\")\n",
    "                    if \"ResourceExhausted\" in str(e) or \"quota\" in str(e).lower() or \"rate limit\" in str(e).lower():\n",
    "                        print(f\"    [GEMINI] Error de cuota. Esperando {wait_time_on_error_s}s...\")\n",
    "                        time.sleep(wait_time_on_error_s)\n",
    "                    else:\n",
    "                        break # Otros errores no reintentables\n",
    "\n",
    "            # Calcular métricas para Gemma\n",
    "            similitud_gemma = cosine_final(np.array(text_embedding_gemma(respuesta_modelo_gemma)), np.array(text_embedding_gemma(respuesta_humana))) if successful_gemma_response else 0.0\n",
    "            coherencia_gemma = medir_coherencia(pregunta, respuesta_modelo_gemma) if successful_gemma_response else 0.0\n",
    "            personalizacion_gemma = medir_personalizacion(perfil_usuario, respuesta_modelo_gemma) if successful_gemma_response else 0.0\n",
    "            fluidez_gemma = medir_fluidez(respuesta_modelo_gemma) if successful_gemma_response else 0.0\n",
    "            alucinacion_gemma = medir_alucinacion(respuesta_modelo_gemma, contexto_gemma_usado) if successful_gemma_response else 0.0\n",
    "\n",
    "            df_respuestas_detalladas.append({\n",
    "                \"Modelo\": modelo_banco2,\n",
    "                \"Prompt_Tipo\": prompt_system, # Usamos el nombre del prompt\n",
    "                \"Pregunta\": pregunta,\n",
    "                \"Respuesta_Humana\": respuesta_humana,\n",
    "                \"Respuesta_Modelo\": respuesta_modelo_gemma,\n",
    "                \"Similitud_Coseno\": similitud_gemma,\n",
    "                \"Coherencia\": coherencia_gemma,\n",
    "                \"Personalizacion\": personalizacion_gemma,\n",
    "                \"Fluidez\": fluidez_gemma,\n",
    "                \"Alucinacion\": alucinacion_gemma,\n",
    "                \"Input_Tokens\": input_tokens_gemma,\n",
    "                \"Output_Tokens\": output_tokens_gemma,\n",
    "                \"API_Latency_s\": api_latency_gemma,\n",
    "                \"Total_Exec_Time_s\": total_exec_time_gemma,\n",
    "                \"Successful_Response\": successful_gemma_response\n",
    "            })\n",
    "        else:\n",
    "            print(f\"Saltando evaluación de GEMINI para pregunta '{pregunta[:50]}...', cliente no configurado.\")\n",
    "\n",
    "\n",
    "df_respuestas_detalladas = pd.DataFrame(df_respuestas_detalladas)\n",
    "\n",
    "# --- Cálculo de Métricas y Resumen Final ---\n",
    "# df_res_successful = df_respuestas_detalladas[df_respuestas_detalladas['Successful_Response'] == True].copy()\n",
    "\n",
    "df_respuestas_detalladas = df_respuestas_detalladas[\n",
    "    (df_respuestas_detalladas['Successful_Response'] == True) &\n",
    "    (df_respuestas_detalladas['Similitud_Coseno'] != 0) &\n",
    "    (df_respuestas_detalladas['Coherencia'] != 0) &\n",
    "    (df_respuestas_detalladas['Personalizacion'] != 0) &\n",
    "    (df_respuestas_detalladas['Fluidez'] != 0) &\n",
    "    (df_respuestas_detalladas['Alucinacion'] != 0)\n",
    "].copy()\n",
    "df_respuestas_detalladas.index = range(df_respuestas_detalladas.shape[0])\n",
    "\n",
    "# Calcular las tasas\n",
    "df_respuestas_detalladas[\"Tasa\"] = df_respuestas_detalladas.apply(lambda row: 1 if row[\"Similitud_Coseno\"]>=umbrales[0] else 0, axis = 1)\n",
    "df_respuestas_detalladas[\"Coh\"] = df_respuestas_detalladas.apply(lambda row: 1 if row[\"Coherencia\"]>=umbrales[1] else 0, axis = 1)\n",
    "df_respuestas_detalladas[\"Per\"] = df_respuestas_detalladas.apply(lambda row: 1 if row[\"Personalizacion\"]>=umbrales[2] else 0, axis = 1)\n",
    "df_respuestas_detalladas[\"Flu\"] = df_respuestas_detalladas.apply(lambda row: 1 if row[\"Fluidez\"]>=umbrales[3] else 0, axis = 1)\n",
    "df_respuestas_detalladas[\"Alu\"] = df_respuestas_detalladas.apply(lambda row: 1 if row[\"Alucinacion\"]>=umbrales[4] else 0, axis = 1)\n",
    "df_respuestas_detalladas[\"total\"] = 1 # Para contar el total de filas\n",
    "\n",
    "# Escalamiento de la fluidez\n",
    "if np.mean(df_respuestas_detalladas[\"Fluidez\"]>=umbrales[3])>=0.30:\n",
    "    min_F = df_respuestas_detalladas[\"Fluidez\"].min()\n",
    "    max_F = df_respuestas_detalladas[\"Fluidez\"].max()\n",
    "    df_respuestas_detalladas[\"Fluidez\"]  = (df_respuestas_detalladas[\"Fluidez\"] - min_F) / (max_F - min_F)\n",
    "    df_respuestas_detalladas[\"Flu\"] = df_respuestas_detalladas.apply(lambda row: 1 if row[\"Fluidez\"]>=0.50 else 0, axis = 1)\n",
    "\n",
    "else:\n",
    "    min_F = 0\n",
    "    max_F = 1\n",
    "\n",
    "# Agrupar y calcular promedios\n",
    "summary_ = df_respuestas_detalladas.groupby([\"Modelo\", \"Prompt_Tipo\"]).agg(\n",
    "    Tasa=('Tasa', 'mean'),\n",
    "    Coh=('Coh', 'mean'),\n",
    "    Per=('Per', 'mean'),\n",
    "    Flu=('Flu', 'mean'),\n",
    "    Alu=('Alu', 'mean'),\n",
    "    Total_Respuestas=('total', 'sum'),\n",
    "    Respuestas_Exitosas=('Successful_Response', 'sum'), # Contar las exitosas\n",
    "    avg_input_tokens=('Input_Tokens', 'mean'),\n",
    "    avg_output_tokens=('Output_Tokens', 'mean'),\n",
    "    avg_api_latency=('API_Latency_s', 'mean'),\n",
    "    avg_total_execution_time=('Total_Exec_Time_s', 'mean')\n",
    ").reset_index()\n",
    "\n",
    "# Convertir tasas a porcentaje\n",
    "for k in [\"Tasa\", \"Coh\", \"Per\", \"Flu\", \"Alu\"]:\n",
    "    summary_[k] = 100 * summary_[k]\n",
    "\n",
    "# --- Impresión de Resultados ---\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"--- Detalles de Respuestas Generadas (DataFrame Completo) ---\".center(80))\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Modelo</th>\n",
       "      <th>Prompt_Tipo</th>\n",
       "      <th>Pregunta</th>\n",
       "      <th>Respuesta_Humana</th>\n",
       "      <th>Respuesta_Modelo</th>\n",
       "      <th>Similitud_Coseno</th>\n",
       "      <th>Coherencia</th>\n",
       "      <th>Personalizacion</th>\n",
       "      <th>Fluidez</th>\n",
       "      <th>Alucinacion</th>\n",
       "      <th>...</th>\n",
       "      <th>Output_Tokens</th>\n",
       "      <th>API_Latency_s</th>\n",
       "      <th>Total_Exec_Time_s</th>\n",
       "      <th>Successful_Response</th>\n",
       "      <th>Tasa</th>\n",
       "      <th>Coh</th>\n",
       "      <th>Per</th>\n",
       "      <th>Flu</th>\n",
       "      <th>Alu</th>\n",
       "      <th>total</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>gpt-4o-mini_clasificacion-PQRS</td>\n",
       "      <td>Eres un Asistente Personal de Inteligencia Art...</td>\n",
       "      <td>¿Qué es LLMOps?</td>\n",
       "      <td>LLMOps (Large Language Model Operations) es un...</td>\n",
       "      <td>LLMOps, que significa \"Large Language Model Op...</td>\n",
       "      <td>0.956428</td>\n",
       "      <td>0.748917</td>\n",
       "      <td>0.796899</td>\n",
       "      <td>0.500527</td>\n",
       "      <td>0.771195</td>\n",
       "      <td>...</td>\n",
       "      <td>207</td>\n",
       "      <td>6.591404</td>\n",
       "      <td>7.666058</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>models/gemini-2.5-flash</td>\n",
       "      <td>Eres un Asistente Personal de Inteligencia Art...</td>\n",
       "      <td>¿Qué es LLMOps?</td>\n",
       "      <td>LLMOps (Large Language Model Operations) es un...</td>\n",
       "      <td>¡Hola! Con gusto te explico qué es LLMOps, que...</td>\n",
       "      <td>0.834858</td>\n",
       "      <td>0.768006</td>\n",
       "      <td>0.789986</td>\n",
       "      <td>0.746821</td>\n",
       "      <td>0.766108</td>\n",
       "      <td>...</td>\n",
       "      <td>425</td>\n",
       "      <td>12.073298</td>\n",
       "      <td>12.336439</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>gpt-4o-mini_clasificacion-PQRS</td>\n",
       "      <td>Eres un Asistente Personal de Inteligencia Art...</td>\n",
       "      <td>¿Cuáles son los objetivos principales de LLMOps?</td>\n",
       "      <td>LLMOps busca facilitar las capacidades de IA c...</td>\n",
       "      <td>Los objetivos principales de LLMOps, que se re...</td>\n",
       "      <td>0.892234</td>\n",
       "      <td>0.795700</td>\n",
       "      <td>0.793326</td>\n",
       "      <td>0.351179</td>\n",
       "      <td>0.766315</td>\n",
       "      <td>...</td>\n",
       "      <td>391</td>\n",
       "      <td>3.521293</td>\n",
       "      <td>4.228223</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                           Modelo  \\\n",
       "0  gpt-4o-mini_clasificacion-PQRS   \n",
       "1         models/gemini-2.5-flash   \n",
       "2  gpt-4o-mini_clasificacion-PQRS   \n",
       "\n",
       "                                         Prompt_Tipo  \\\n",
       "0  Eres un Asistente Personal de Inteligencia Art...   \n",
       "1  Eres un Asistente Personal de Inteligencia Art...   \n",
       "2  Eres un Asistente Personal de Inteligencia Art...   \n",
       "\n",
       "                                           Pregunta  \\\n",
       "0                                   ¿Qué es LLMOps?   \n",
       "1                                   ¿Qué es LLMOps?   \n",
       "2  ¿Cuáles son los objetivos principales de LLMOps?   \n",
       "\n",
       "                                    Respuesta_Humana  \\\n",
       "0  LLMOps (Large Language Model Operations) es un...   \n",
       "1  LLMOps (Large Language Model Operations) es un...   \n",
       "2  LLMOps busca facilitar las capacidades de IA c...   \n",
       "\n",
       "                                    Respuesta_Modelo  Similitud_Coseno  \\\n",
       "0  LLMOps, que significa \"Large Language Model Op...          0.956428   \n",
       "1  ¡Hola! Con gusto te explico qué es LLMOps, que...          0.834858   \n",
       "2  Los objetivos principales de LLMOps, que se re...          0.892234   \n",
       "\n",
       "   Coherencia  Personalizacion   Fluidez  Alucinacion  ...  Output_Tokens  \\\n",
       "0    0.748917         0.796899  0.500527     0.771195  ...            207   \n",
       "1    0.768006         0.789986  0.746821     0.766108  ...            425   \n",
       "2    0.795700         0.793326  0.351179     0.766315  ...            391   \n",
       "\n",
       "   API_Latency_s  Total_Exec_Time_s  Successful_Response  Tasa  Coh  Per  Flu  \\\n",
       "0       6.591404           7.666058                 True     1    0    1    1   \n",
       "1      12.073298          12.336439                 True     1    0    1    1   \n",
       "2       3.521293           4.228223                 True     1    0    1    0   \n",
       "\n",
       "   Alu  total  \n",
       "0    1      1  \n",
       "1    1      1  \n",
       "2    1      1  \n",
       "\n",
       "[3 rows x 21 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_respuestas_detalladas.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Modelo</th>\n",
       "      <th>Prompt_Tipo</th>\n",
       "      <th>Indice Monitoreo</th>\n",
       "      <th>Tasa</th>\n",
       "      <th>Coh</th>\n",
       "      <th>Per</th>\n",
       "      <th>Flu</th>\n",
       "      <th>Alu</th>\n",
       "      <th>Total_Respuestas</th>\n",
       "      <th>Respuestas_Exitosas</th>\n",
       "      <th>avg_input_tokens</th>\n",
       "      <th>avg_output_tokens</th>\n",
       "      <th>avg_api_latency</th>\n",
       "      <th>avg_total_execution_time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>gpt-4o-mini_clasificacion-PQRS</td>\n",
       "      <td>Eres un Asistente Personal de Inteligencia Art...</td>\n",
       "      <td>82.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>2904.6</td>\n",
       "      <td>342.1</td>\n",
       "      <td>4.634059</td>\n",
       "      <td>5.511642</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>gpt-4o-mini_clasificacion-PQRS</td>\n",
       "      <td>Eres una Inteligencia Artificial super avanzad...</td>\n",
       "      <td>72.0</td>\n",
       "      <td>90.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>2657.6</td>\n",
       "      <td>215.3</td>\n",
       "      <td>2.447868</td>\n",
       "      <td>3.198969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>models/gemini-2.5-flash</td>\n",
       "      <td>Eres un Asistente Personal de Inteligencia Art...</td>\n",
       "      <td>72.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>90.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>2112.2</td>\n",
       "      <td>370.5</td>\n",
       "      <td>7.410099</td>\n",
       "      <td>7.754081</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>models/gemini-2.5-flash</td>\n",
       "      <td>Eres una Inteligencia Artificial super avanzad...</td>\n",
       "      <td>70.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>1951.2</td>\n",
       "      <td>127.4</td>\n",
       "      <td>3.273753</td>\n",
       "      <td>3.577786</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           Modelo  \\\n",
       "0  gpt-4o-mini_clasificacion-PQRS   \n",
       "1  gpt-4o-mini_clasificacion-PQRS   \n",
       "2         models/gemini-2.5-flash   \n",
       "3         models/gemini-2.5-flash   \n",
       "\n",
       "                                         Prompt_Tipo  Indice Monitoreo   Tasa  \\\n",
       "0  Eres un Asistente Personal de Inteligencia Art...              82.0  100.0   \n",
       "1  Eres una Inteligencia Artificial super avanzad...              72.0   90.0   \n",
       "2  Eres un Asistente Personal de Inteligencia Art...              72.0   40.0   \n",
       "3  Eres una Inteligencia Artificial super avanzad...              70.0   60.0   \n",
       "\n",
       "    Coh    Per   Flu    Alu  Total_Respuestas  Respuestas_Exitosas  \\\n",
       "0  40.0  100.0  70.0  100.0                10                   10   \n",
       "1  60.0  100.0  10.0  100.0                10                   10   \n",
       "2  30.0  100.0  90.0  100.0                10                   10   \n",
       "3  70.0  100.0  20.0  100.0                10                   10   \n",
       "\n",
       "   avg_input_tokens  avg_output_tokens  avg_api_latency  \\\n",
       "0            2904.6              342.1         4.634059   \n",
       "1            2657.6              215.3         2.447868   \n",
       "2            2112.2              370.5         7.410099   \n",
       "3            1951.2              127.4         3.273753   \n",
       "\n",
       "   avg_total_execution_time  \n",
       "0                  5.511642  \n",
       "1                  3.198969  \n",
       "2                  7.754081  \n",
       "3                  3.577786  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cl = summary_.columns\n",
    "summary_[\"Indice Monitoreo\"] = 0.20 * (summary_[\"Tasa\"] + summary_[\"Coh\"] + summary_[\"Per\"] + summary_[\"Flu\"] + summary_[\"Alu\"])\n",
    "cl2 = [x for x in cl if x not in [\"Modelo\",\"Prompt_Tipo\"]]\n",
    "summary_ = summary_.get([\"Modelo\",\"Prompt_Tipo\",\"Indice Monitoreo\"] + cl2)\n",
    "summary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_[\"GPT\"] = summary_['Modelo'].str.contains(r'(?i)\\bgpt\\b', na=False).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = summary_.loc[(summary_[\"Indice Monitoreo\"]==np.max(summary_[\"Indice Monitoreo\"])),]\n",
    "best_model.index = range(best_model.shape[0])\n",
    "\n",
    "best_model = best_model.loc[best_model[\"avg_total_execution_time\"]==np.min(best_model[\"avg_total_execution_time\"]),]\n",
    "best_model.index = range(best_model.shape[0])\n",
    "\n",
    "model_ = best_model[\"Modelo\"][0]\n",
    "pmpt_ = best_model[\"Prompt_Tipo\"][0]\n",
    "ind_flow_gpt = summary_[\"GPT\"][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## retornos\n",
    "\n",
    "* summary_\n",
    "* df_respuestas_detalladas\n",
    "* min_F\n",
    "* max_F\n",
    "* model_\n",
    "* pmpt_\n",
    "* ind_flow_gpt\n",
    "* df_vector_store_gpt\n",
    "* df_vector_store_gem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PR\n",
    "\n",
    "* Creación de preguntas y respuestas\n",
    "\n",
    "* Monitoring"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## INPUTS\n",
    "\n",
    "* contexto_para_generacion,\n",
    "* num_pares_a_generar\n",
    "* temp_\n",
    "* model_\n",
    "* pmpt_\n",
    "* ind_flow_gpt\n",
    "* df_vector_store_gpt\n",
    "* df_vector_store_gem\n",
    "* MD_GEN\n",
    "* modelo_banco_generacion\n",
    "* temperature_gen_assitant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'models/gemini-1.5-pro-latest'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modelo_banco_generacion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temperature_gen_assitant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_pares_a_generar = 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_ = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'gpt-4o-mini_clasificacion-PQRS'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Eres un Asistente Personal de Inteligencia Artificial experto en explicar información técnica de manera clara, concisa y fácil de entender para cualquier persona, incluso para un público no técnico. Tu objetivo es ser extremadamente útil y amigable.\\n\\nPara responder a las preguntas del usuario, **siempre debes basarte en los RESULTADOS DE BÚSQUEDA SEMÁNTICA proporcionados.**\\n\\nInstrucciones CLAVE para generar tu respuesta:\\n1.  **Prioriza la Coherencia:** Asegúrate de que tu respuesta esté DIRECTAMENTE relacionada con la pregunta. No te desvíes del tema.\\n2.  **Responde de manera fluida. Imagina que le estás explicando a alguien que no es experto en el tema. Puedes hacer una mezcla donde prime lo coloquial en el discurso sobre lo técnico.\\n3.  **Mantén la Precisión y Veracidad:** Solo utiliza la información de la BÚSQUEDA SEMÁNTICA si es relevante y tiene relación directa con la pregunta.\\n4.  **No Inventes (Evita Alucinaciones):** Si la respuesta no se encuentra explícitamente en el contexto de la búsqueda semántica, no intentes adivinar ni inventes información. En ese caso, responde amablemente que no tienes información suficiente para responder a esa pregunta específica.\\n5.  **Tono y Personalización:** Mantén un tono amigable y de asistente personal. Si la pregunta lo permite y la información de personalización está implícita o disponible, considera cómo hacer la respuesta más relevante para un \"Usuario colombiano interesado en modelos de lenguaje e IA aplicada\".\\n\\nRESULTADOS DE BÚSQUEDA SEMÁNTICA:\\n{source}\\n\\nAhora, respira profundo, piensa paso a paso y genera la mejor respuesta posible para el usuario, aplicando todas las instrucciones anteriores.\\n'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pmpt_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.int64(1)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ind_flow_gpt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enviando solicitud al LLM para generar 15 pares QA...\n",
      "Generación del LLM completada. Procesando pares QA...\n"
     ]
    }
   ],
   "source": [
    "# --- GENERACIÓN DE NUEVAS PREGUNTAS Y RESPUESTAS ---\n",
    "nuevos_pares_qa = []\n",
    "\n",
    "# Corregido: Usar 'top_join' para la generación de QA, que ahora proviene del YAML\n",
    "\n",
    "if MD_GEN == \"GPT\":\n",
    "    contexto_para_generacion = obtener_contexto_para_generacion(df_vector_store_gem, \n",
    "                                                            num_chunks=top_join)\n",
    "else:\n",
    "    contexto_para_generacion = obtener_contexto_para_generacion(df_vector_store_gpt, \n",
    "                                                            num_chunks=top_join)\n",
    "\n",
    "# Formatear el prompt de generación con el número deseado (ahora `num_pares_a_generar` del YAML)\n",
    "final_prompt_qa = prompt_generacion_qa.format(\n",
    "    source_text=contexto_para_generacion,\n",
    "    num_to_generate=num_pares_a_generar\n",
    ")\n",
    "\n",
    "print(f\"Enviando solicitud al LLM para generar {num_pares_a_generar} pares QA...\")\n",
    "try:\n",
    "    if client: # Solo intentar si el cliente Azure está inicializado\n",
    "        if MD_GEN == \"GPT\":\n",
    "            completion_qa = client.chat.completions.create(\n",
    "            model=modelo_banco_generacion,\n",
    "            temperature=temperature_gen_assitant, # Usa la temperatura de GPT del YAML\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": final_prompt_qa}\n",
    "            ]\n",
    "            )\n",
    "            generated_text = completion_qa.choices[0].message.content\n",
    "        else:\n",
    "            model_2 = genai.GenerativeModel(model_name=modelo_banco_generacion)\n",
    "            full_query_for_gemma = final_prompt_qa\n",
    "            completion2 = model_2.generate_content(\n",
    "            contents=full_query_for_gemma,\n",
    "            generation_config=GenerationConfig(temperature=temperature_gen_assitant)\n",
    "            )\n",
    "            generated_text = completion2.text\n",
    "\n",
    "\n",
    "        print(\"Generación del LLM completada. Procesando pares QA...\")\n",
    "\n",
    "        # --- PARSEO DE LAS RESPUESTAS GENERADAS CON REGEX (PARA MÚLTIPLES PARES) ---\n",
    "        pattern = re.compile(r\"Pregunta:\\s*(.*?)\\s*Respuesta:\\s*(.*?)(?=(?:Pregunta:|$))\", re.DOTALL | re.IGNORECASE)\n",
    "        matches = pattern.findall(generated_text)\n",
    "\n",
    "        for match in matches:\n",
    "            question = match[0].strip()\n",
    "            answer = match[1].strip()\n",
    "\n",
    "            if question and answer: # Asegurarse de que no estén vacíos\n",
    "                # Validar si ya existe para evitar duplicados\n",
    "                if not df_preguntas_respuestas_existentes['Pregunta'].astype(str).str.contains(question, case=False, regex=False).any():\n",
    "                    nuevos_pares_qa.append({\"Pregunta\": question, \"Respuesta\": answer})\n",
    "                else:\n",
    "                    print(f\"  [QA Gen] Par '{question[:30]}...' ya existe en 'preguntas_BOT.xlsx', omitiendo.\")\n",
    "\n",
    "                if len(nuevos_pares_qa) >= num_pares_a_generar:\n",
    "                    break # Detener si ya generamos la cantidad deseada\n",
    "\n",
    "    else:\n",
    "        print(\"Error: El cliente Azure OpenAI no está inicializado. No se pudo generar QA.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error durante la generación o procesamiento de QA: {e}\")\n",
    "\n",
    "# Asegurarse de tener a lo sumo num_pares_a_generar\n",
    "if len(nuevos_pares_qa) > num_pares_a_generar:\n",
    "    nuevos_pares_qa = nuevos_pares_qa[:num_pares_a_generar]\n",
    "elif len(nuevos_pares_qa) < num_pares_a_generar:\n",
    "    print(f\"Advertencia: Solo se pudieron generar {len(nuevos_pares_qa)} pares QA válidos de {num_pares_a_generar} solicitados.\")\n",
    "\n",
    "df_nuevos_qa = pd.DataFrame(nuevos_pares_qa)\n",
    "\n",
    "if df_nuevos_qa.empty:\n",
    "    print(\"No se pudieron generar pares de preguntas y respuestas válidos. Finalizando.\")\n",
    "    exit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pregunta</th>\n",
       "      <th>Respuesta</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>¿Qué es LLMOps?</td>\n",
       "      <td>LLMOps (Large Language Model Operations) es un...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>¿Cuál es el objetivo principal de LLMOps?</td>\n",
       "      <td>El objetivo de LLMOps es habilitar las capacid...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>¿Por qué es importante LLMOps para los modelos...</td>\n",
       "      <td>LLMOps aborda los desafíos específicos de los ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>¿Cuáles son algunos de los desafíos que LLMOps...</td>\n",
       "      <td>Los LLMs requieren importantes recursos comput...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>¿Por qué es preferible usar modelos de base pr...</td>\n",
       "      <td>Entrenar LLMs desde cero es costoso y requiere...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            Pregunta  \\\n",
       "0                                    ¿Qué es LLMOps?   \n",
       "1          ¿Cuál es el objetivo principal de LLMOps?   \n",
       "2  ¿Por qué es importante LLMOps para los modelos...   \n",
       "3  ¿Cuáles son algunos de los desafíos que LLMOps...   \n",
       "4  ¿Por qué es preferible usar modelos de base pr...   \n",
       "\n",
       "                                           Respuesta  \n",
       "0  LLMOps (Large Language Model Operations) es un...  \n",
       "1  El objetivo de LLMOps es habilitar las capacid...  \n",
       "2  LLMOps aborda los desafíos específicos de los ...  \n",
       "3  Los LLMs requieren importantes recursos comput...  \n",
       "4  Entrenar LLMs desde cero es costoso y requiere...  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_nuevos_qa.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following layers were not sharded: encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.dense.weight, embeddings.LayerNorm.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.query.weight, encoder.layer.*.attention.self.key.bias, encoder.layer.*.output.LayerNorm.bias, encoder.layer.*.output.dense.weight, encoder.layer.*.attention.self.value.weight, embeddings.word_embeddings.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.weight, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.value.bias, pooler.dense.bias, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.bias, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.output.LayerNorm.bias, encoder.layer.*.intermediate.dense.weight, pooler.dense.weight, embeddings.token_type_embeddings.weight\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.dense.weight, embeddings.LayerNorm.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.query.weight, encoder.layer.*.attention.self.key.bias, encoder.layer.*.output.LayerNorm.bias, encoder.layer.*.output.dense.weight, encoder.layer.*.attention.self.value.weight, embeddings.word_embeddings.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.weight, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.value.bias, pooler.dense.bias, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.bias, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.output.LayerNorm.bias, encoder.layer.*.intermediate.dense.weight, pooler.dense.weight, embeddings.token_type_embeddings.weight\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.dense.weight, embeddings.LayerNorm.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.query.weight, encoder.layer.*.attention.self.key.bias, encoder.layer.*.output.LayerNorm.bias, encoder.layer.*.output.dense.weight, encoder.layer.*.attention.self.value.weight, embeddings.word_embeddings.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.weight, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.value.bias, pooler.dense.bias, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.bias, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.output.LayerNorm.bias, encoder.layer.*.intermediate.dense.weight, pooler.dense.weight, embeddings.token_type_embeddings.weight\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.dense.weight, embeddings.LayerNorm.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.query.weight, encoder.layer.*.attention.self.key.bias, encoder.layer.*.output.LayerNorm.bias, encoder.layer.*.output.dense.weight, encoder.layer.*.attention.self.value.weight, embeddings.word_embeddings.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.weight, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.value.bias, pooler.dense.bias, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.bias, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.output.LayerNorm.bias, encoder.layer.*.intermediate.dense.weight, pooler.dense.weight, embeddings.token_type_embeddings.weight\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.dense.weight, embeddings.LayerNorm.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.query.weight, encoder.layer.*.attention.self.key.bias, encoder.layer.*.output.LayerNorm.bias, encoder.layer.*.output.dense.weight, encoder.layer.*.attention.self.value.weight, embeddings.word_embeddings.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.weight, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.value.bias, pooler.dense.bias, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.bias, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.output.LayerNorm.bias, encoder.layer.*.intermediate.dense.weight, pooler.dense.weight, embeddings.token_type_embeddings.weight\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.dense.weight, embeddings.LayerNorm.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.query.weight, encoder.layer.*.attention.self.key.bias, encoder.layer.*.output.LayerNorm.bias, encoder.layer.*.output.dense.weight, encoder.layer.*.attention.self.value.weight, embeddings.word_embeddings.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.weight, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.value.bias, pooler.dense.bias, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.bias, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.output.LayerNorm.bias, encoder.layer.*.intermediate.dense.weight, pooler.dense.weight, embeddings.token_type_embeddings.weight\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.dense.weight, embeddings.LayerNorm.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.query.weight, encoder.layer.*.attention.self.key.bias, encoder.layer.*.output.LayerNorm.bias, encoder.layer.*.output.dense.weight, encoder.layer.*.attention.self.value.weight, embeddings.word_embeddings.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.weight, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.value.bias, pooler.dense.bias, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.bias, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.output.LayerNorm.bias, encoder.layer.*.intermediate.dense.weight, pooler.dense.weight, embeddings.token_type_embeddings.weight\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.dense.weight, embeddings.LayerNorm.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.query.weight, encoder.layer.*.attention.self.key.bias, encoder.layer.*.output.LayerNorm.bias, encoder.layer.*.output.dense.weight, encoder.layer.*.attention.self.value.weight, embeddings.word_embeddings.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.weight, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.value.bias, pooler.dense.bias, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.bias, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.output.LayerNorm.bias, encoder.layer.*.intermediate.dense.weight, pooler.dense.weight, embeddings.token_type_embeddings.weight\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.dense.weight, embeddings.LayerNorm.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.query.weight, encoder.layer.*.attention.self.key.bias, encoder.layer.*.output.LayerNorm.bias, encoder.layer.*.output.dense.weight, encoder.layer.*.attention.self.value.weight, embeddings.word_embeddings.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.weight, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.value.bias, pooler.dense.bias, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.bias, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.output.LayerNorm.bias, encoder.layer.*.intermediate.dense.weight, pooler.dense.weight, embeddings.token_type_embeddings.weight\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.dense.weight, embeddings.LayerNorm.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.query.weight, encoder.layer.*.attention.self.key.bias, encoder.layer.*.output.LayerNorm.bias, encoder.layer.*.output.dense.weight, encoder.layer.*.attention.self.value.weight, embeddings.word_embeddings.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.weight, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.value.bias, pooler.dense.bias, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.bias, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.output.LayerNorm.bias, encoder.layer.*.intermediate.dense.weight, pooler.dense.weight, embeddings.token_type_embeddings.weight\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.dense.weight, embeddings.LayerNorm.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.query.weight, encoder.layer.*.attention.self.key.bias, encoder.layer.*.output.LayerNorm.bias, encoder.layer.*.output.dense.weight, encoder.layer.*.attention.self.value.weight, embeddings.word_embeddings.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.weight, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.value.bias, pooler.dense.bias, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.bias, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.output.LayerNorm.bias, encoder.layer.*.intermediate.dense.weight, pooler.dense.weight, embeddings.token_type_embeddings.weight\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.dense.weight, embeddings.LayerNorm.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.query.weight, encoder.layer.*.attention.self.key.bias, encoder.layer.*.output.LayerNorm.bias, encoder.layer.*.output.dense.weight, encoder.layer.*.attention.self.value.weight, embeddings.word_embeddings.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.weight, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.value.bias, pooler.dense.bias, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.bias, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.output.LayerNorm.bias, encoder.layer.*.intermediate.dense.weight, pooler.dense.weight, embeddings.token_type_embeddings.weight\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.dense.weight, embeddings.LayerNorm.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.query.weight, encoder.layer.*.attention.self.key.bias, encoder.layer.*.output.LayerNorm.bias, encoder.layer.*.output.dense.weight, encoder.layer.*.attention.self.value.weight, embeddings.word_embeddings.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.weight, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.value.bias, pooler.dense.bias, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.bias, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.output.LayerNorm.bias, encoder.layer.*.intermediate.dense.weight, pooler.dense.weight, embeddings.token_type_embeddings.weight\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.dense.weight, embeddings.LayerNorm.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.query.weight, encoder.layer.*.attention.self.key.bias, encoder.layer.*.output.LayerNorm.bias, encoder.layer.*.output.dense.weight, encoder.layer.*.attention.self.value.weight, embeddings.word_embeddings.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.weight, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.value.bias, pooler.dense.bias, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.bias, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.output.LayerNorm.bias, encoder.layer.*.intermediate.dense.weight, pooler.dense.weight, embeddings.token_type_embeddings.weight\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.dense.weight, embeddings.LayerNorm.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.query.weight, encoder.layer.*.attention.self.key.bias, encoder.layer.*.output.LayerNorm.bias, encoder.layer.*.output.dense.weight, encoder.layer.*.attention.self.value.weight, embeddings.word_embeddings.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.weight, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.value.bias, pooler.dense.bias, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.bias, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.output.LayerNorm.bias, encoder.layer.*.intermediate.dense.weight, pooler.dense.weight, embeddings.token_type_embeddings.weight\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.dense.weight, embeddings.LayerNorm.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.query.weight, encoder.layer.*.attention.self.key.bias, encoder.layer.*.output.LayerNorm.bias, encoder.layer.*.output.dense.weight, encoder.layer.*.attention.self.value.weight, embeddings.word_embeddings.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.weight, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.value.bias, pooler.dense.bias, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.bias, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.output.LayerNorm.bias, encoder.layer.*.intermediate.dense.weight, pooler.dense.weight, embeddings.token_type_embeddings.weight\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.dense.weight, embeddings.LayerNorm.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.query.weight, encoder.layer.*.attention.self.key.bias, encoder.layer.*.output.LayerNorm.bias, encoder.layer.*.output.dense.weight, encoder.layer.*.attention.self.value.weight, embeddings.word_embeddings.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.weight, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.value.bias, pooler.dense.bias, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.bias, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.output.LayerNorm.bias, encoder.layer.*.intermediate.dense.weight, pooler.dense.weight, embeddings.token_type_embeddings.weight\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.dense.weight, embeddings.LayerNorm.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.query.weight, encoder.layer.*.attention.self.key.bias, encoder.layer.*.output.LayerNorm.bias, encoder.layer.*.output.dense.weight, encoder.layer.*.attention.self.value.weight, embeddings.word_embeddings.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.weight, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.value.bias, pooler.dense.bias, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.bias, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.output.LayerNorm.bias, encoder.layer.*.intermediate.dense.weight, pooler.dense.weight, embeddings.token_type_embeddings.weight\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.dense.weight, embeddings.LayerNorm.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.query.weight, encoder.layer.*.attention.self.key.bias, encoder.layer.*.output.LayerNorm.bias, encoder.layer.*.output.dense.weight, encoder.layer.*.attention.self.value.weight, embeddings.word_embeddings.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.weight, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.value.bias, pooler.dense.bias, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.bias, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.output.LayerNorm.bias, encoder.layer.*.intermediate.dense.weight, pooler.dense.weight, embeddings.token_type_embeddings.weight\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.dense.weight, embeddings.LayerNorm.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.query.weight, encoder.layer.*.attention.self.key.bias, encoder.layer.*.output.LayerNorm.bias, encoder.layer.*.output.dense.weight, encoder.layer.*.attention.self.value.weight, embeddings.word_embeddings.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.weight, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.value.bias, pooler.dense.bias, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.bias, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.output.LayerNorm.bias, encoder.layer.*.intermediate.dense.weight, pooler.dense.weight, embeddings.token_type_embeddings.weight\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.dense.weight, embeddings.LayerNorm.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.query.weight, encoder.layer.*.attention.self.key.bias, encoder.layer.*.output.LayerNorm.bias, encoder.layer.*.output.dense.weight, encoder.layer.*.attention.self.value.weight, embeddings.word_embeddings.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.weight, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.value.bias, pooler.dense.bias, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.bias, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.output.LayerNorm.bias, encoder.layer.*.intermediate.dense.weight, pooler.dense.weight, embeddings.token_type_embeddings.weight\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.dense.weight, embeddings.LayerNorm.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.query.weight, encoder.layer.*.attention.self.key.bias, encoder.layer.*.output.LayerNorm.bias, encoder.layer.*.output.dense.weight, encoder.layer.*.attention.self.value.weight, embeddings.word_embeddings.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.weight, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.value.bias, pooler.dense.bias, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.bias, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.output.LayerNorm.bias, encoder.layer.*.intermediate.dense.weight, pooler.dense.weight, embeddings.token_type_embeddings.weight\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.dense.weight, embeddings.LayerNorm.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.query.weight, encoder.layer.*.attention.self.key.bias, encoder.layer.*.output.LayerNorm.bias, encoder.layer.*.output.dense.weight, encoder.layer.*.attention.self.value.weight, embeddings.word_embeddings.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.weight, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.value.bias, pooler.dense.bias, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.bias, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.output.LayerNorm.bias, encoder.layer.*.intermediate.dense.weight, pooler.dense.weight, embeddings.token_type_embeddings.weight\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.dense.weight, embeddings.LayerNorm.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.query.weight, encoder.layer.*.attention.self.key.bias, encoder.layer.*.output.LayerNorm.bias, encoder.layer.*.output.dense.weight, encoder.layer.*.attention.self.value.weight, embeddings.word_embeddings.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.weight, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.value.bias, pooler.dense.bias, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.bias, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.output.LayerNorm.bias, encoder.layer.*.intermediate.dense.weight, pooler.dense.weight, embeddings.token_type_embeddings.weight\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.dense.weight, embeddings.LayerNorm.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.query.weight, encoder.layer.*.attention.self.key.bias, encoder.layer.*.output.LayerNorm.bias, encoder.layer.*.output.dense.weight, encoder.layer.*.attention.self.value.weight, embeddings.word_embeddings.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.weight, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.value.bias, pooler.dense.bias, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.bias, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.output.LayerNorm.bias, encoder.layer.*.intermediate.dense.weight, pooler.dense.weight, embeddings.token_type_embeddings.weight\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.dense.weight, embeddings.LayerNorm.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.query.weight, encoder.layer.*.attention.self.key.bias, encoder.layer.*.output.LayerNorm.bias, encoder.layer.*.output.dense.weight, encoder.layer.*.attention.self.value.weight, embeddings.word_embeddings.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.weight, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.value.bias, pooler.dense.bias, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.bias, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.output.LayerNorm.bias, encoder.layer.*.intermediate.dense.weight, pooler.dense.weight, embeddings.token_type_embeddings.weight\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.dense.weight, embeddings.LayerNorm.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.query.weight, encoder.layer.*.attention.self.key.bias, encoder.layer.*.output.LayerNorm.bias, encoder.layer.*.output.dense.weight, encoder.layer.*.attention.self.value.weight, embeddings.word_embeddings.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.weight, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.value.bias, pooler.dense.bias, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.bias, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.output.LayerNorm.bias, encoder.layer.*.intermediate.dense.weight, pooler.dense.weight, embeddings.token_type_embeddings.weight\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.dense.weight, embeddings.LayerNorm.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.query.weight, encoder.layer.*.attention.self.key.bias, encoder.layer.*.output.LayerNorm.bias, encoder.layer.*.output.dense.weight, encoder.layer.*.attention.self.value.weight, embeddings.word_embeddings.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.weight, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.value.bias, pooler.dense.bias, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.bias, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.output.LayerNorm.bias, encoder.layer.*.intermediate.dense.weight, pooler.dense.weight, embeddings.token_type_embeddings.weight\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.dense.weight, embeddings.LayerNorm.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.query.weight, encoder.layer.*.attention.self.key.bias, encoder.layer.*.output.LayerNorm.bias, encoder.layer.*.output.dense.weight, encoder.layer.*.attention.self.value.weight, embeddings.word_embeddings.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.weight, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.value.bias, pooler.dense.bias, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.bias, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.output.LayerNorm.bias, encoder.layer.*.intermediate.dense.weight, pooler.dense.weight, embeddings.token_type_embeddings.weight\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.dense.weight, embeddings.LayerNorm.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.query.weight, encoder.layer.*.attention.self.key.bias, encoder.layer.*.output.LayerNorm.bias, encoder.layer.*.output.dense.weight, encoder.layer.*.attention.self.value.weight, embeddings.word_embeddings.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.weight, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.value.bias, pooler.dense.bias, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.bias, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.output.LayerNorm.bias, encoder.layer.*.intermediate.dense.weight, pooler.dense.weight, embeddings.token_type_embeddings.weight\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.dense.weight, embeddings.LayerNorm.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.query.weight, encoder.layer.*.attention.self.key.bias, encoder.layer.*.output.LayerNorm.bias, encoder.layer.*.output.dense.weight, encoder.layer.*.attention.self.value.weight, embeddings.word_embeddings.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.weight, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.value.bias, pooler.dense.bias, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.bias, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.output.LayerNorm.bias, encoder.layer.*.intermediate.dense.weight, pooler.dense.weight, embeddings.token_type_embeddings.weight\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.dense.weight, embeddings.LayerNorm.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.query.weight, encoder.layer.*.attention.self.key.bias, encoder.layer.*.output.LayerNorm.bias, encoder.layer.*.output.dense.weight, encoder.layer.*.attention.self.value.weight, embeddings.word_embeddings.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.weight, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.value.bias, pooler.dense.bias, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.bias, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.output.LayerNorm.bias, encoder.layer.*.intermediate.dense.weight, pooler.dense.weight, embeddings.token_type_embeddings.weight\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.dense.weight, embeddings.LayerNorm.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.query.weight, encoder.layer.*.attention.self.key.bias, encoder.layer.*.output.LayerNorm.bias, encoder.layer.*.output.dense.weight, encoder.layer.*.attention.self.value.weight, embeddings.word_embeddings.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.weight, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.value.bias, pooler.dense.bias, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.bias, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.output.LayerNorm.bias, encoder.layer.*.intermediate.dense.weight, pooler.dense.weight, embeddings.token_type_embeddings.weight\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.dense.weight, embeddings.LayerNorm.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.query.weight, encoder.layer.*.attention.self.key.bias, encoder.layer.*.output.LayerNorm.bias, encoder.layer.*.output.dense.weight, encoder.layer.*.attention.self.value.weight, embeddings.word_embeddings.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.weight, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.value.bias, pooler.dense.bias, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.bias, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.output.LayerNorm.bias, encoder.layer.*.intermediate.dense.weight, pooler.dense.weight, embeddings.token_type_embeddings.weight\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.dense.weight, embeddings.LayerNorm.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.query.weight, encoder.layer.*.attention.self.key.bias, encoder.layer.*.output.LayerNorm.bias, encoder.layer.*.output.dense.weight, encoder.layer.*.attention.self.value.weight, embeddings.word_embeddings.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.weight, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.value.bias, pooler.dense.bias, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.bias, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.output.LayerNorm.bias, encoder.layer.*.intermediate.dense.weight, pooler.dense.weight, embeddings.token_type_embeddings.weight\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.dense.weight, embeddings.LayerNorm.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.query.weight, encoder.layer.*.attention.self.key.bias, encoder.layer.*.output.LayerNorm.bias, encoder.layer.*.output.dense.weight, encoder.layer.*.attention.self.value.weight, embeddings.word_embeddings.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.weight, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.value.bias, pooler.dense.bias, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.bias, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.output.LayerNorm.bias, encoder.layer.*.intermediate.dense.weight, pooler.dense.weight, embeddings.token_type_embeddings.weight\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.dense.weight, embeddings.LayerNorm.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.query.weight, encoder.layer.*.attention.self.key.bias, encoder.layer.*.output.LayerNorm.bias, encoder.layer.*.output.dense.weight, encoder.layer.*.attention.self.value.weight, embeddings.word_embeddings.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.weight, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.value.bias, pooler.dense.bias, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.bias, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.output.LayerNorm.bias, encoder.layer.*.intermediate.dense.weight, pooler.dense.weight, embeddings.token_type_embeddings.weight\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.dense.weight, embeddings.LayerNorm.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.query.weight, encoder.layer.*.attention.self.key.bias, encoder.layer.*.output.LayerNorm.bias, encoder.layer.*.output.dense.weight, encoder.layer.*.attention.self.value.weight, embeddings.word_embeddings.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.weight, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.value.bias, pooler.dense.bias, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.bias, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.output.LayerNorm.bias, encoder.layer.*.intermediate.dense.weight, pooler.dense.weight, embeddings.token_type_embeddings.weight\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.dense.weight, embeddings.LayerNorm.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.query.weight, encoder.layer.*.attention.self.key.bias, encoder.layer.*.output.LayerNorm.bias, encoder.layer.*.output.dense.weight, encoder.layer.*.attention.self.value.weight, embeddings.word_embeddings.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.weight, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.value.bias, pooler.dense.bias, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.bias, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.output.LayerNorm.bias, encoder.layer.*.intermediate.dense.weight, pooler.dense.weight, embeddings.token_type_embeddings.weight\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.dense.weight, embeddings.LayerNorm.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.query.weight, encoder.layer.*.attention.self.key.bias, encoder.layer.*.output.LayerNorm.bias, encoder.layer.*.output.dense.weight, encoder.layer.*.attention.self.value.weight, embeddings.word_embeddings.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.weight, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.value.bias, pooler.dense.bias, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.bias, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.output.LayerNorm.bias, encoder.layer.*.intermediate.dense.weight, pooler.dense.weight, embeddings.token_type_embeddings.weight\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.dense.weight, embeddings.LayerNorm.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.query.weight, encoder.layer.*.attention.self.key.bias, encoder.layer.*.output.LayerNorm.bias, encoder.layer.*.output.dense.weight, encoder.layer.*.attention.self.value.weight, embeddings.word_embeddings.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.weight, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.value.bias, pooler.dense.bias, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.bias, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.output.LayerNorm.bias, encoder.layer.*.intermediate.dense.weight, pooler.dense.weight, embeddings.token_type_embeddings.weight\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.dense.weight, embeddings.LayerNorm.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.query.weight, encoder.layer.*.attention.self.key.bias, encoder.layer.*.output.LayerNorm.bias, encoder.layer.*.output.dense.weight, encoder.layer.*.attention.self.value.weight, embeddings.word_embeddings.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.weight, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.value.bias, pooler.dense.bias, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.bias, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.output.LayerNorm.bias, encoder.layer.*.intermediate.dense.weight, pooler.dense.weight, embeddings.token_type_embeddings.weight\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.dense.weight, embeddings.LayerNorm.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.query.weight, encoder.layer.*.attention.self.key.bias, encoder.layer.*.output.LayerNorm.bias, encoder.layer.*.output.dense.weight, encoder.layer.*.attention.self.value.weight, embeddings.word_embeddings.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.weight, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.value.bias, pooler.dense.bias, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.bias, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.output.LayerNorm.bias, encoder.layer.*.intermediate.dense.weight, pooler.dense.weight, embeddings.token_type_embeddings.weight\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.dense.weight, embeddings.LayerNorm.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.query.weight, encoder.layer.*.attention.self.key.bias, encoder.layer.*.output.LayerNorm.bias, encoder.layer.*.output.dense.weight, encoder.layer.*.attention.self.value.weight, embeddings.word_embeddings.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.weight, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.value.bias, pooler.dense.bias, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.bias, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.output.LayerNorm.bias, encoder.layer.*.intermediate.dense.weight, pooler.dense.weight, embeddings.token_type_embeddings.weight\n",
      "The following layers were not sharded: encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.output.dense.weight, embeddings.LayerNorm.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.attention.self.query.weight, encoder.layer.*.attention.self.key.bias, encoder.layer.*.output.LayerNorm.bias, encoder.layer.*.output.dense.weight, encoder.layer.*.attention.self.value.weight, embeddings.word_embeddings.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.output.LayerNorm.weight, embeddings.position_embeddings.weight, encoder.layer.*.attention.self.value.bias, pooler.dense.bias, embeddings.LayerNorm.bias, encoder.layer.*.attention.output.dense.bias, encoder.layer.*.intermediate.dense.bias, encoder.layer.*.attention.output.LayerNorm.bias, encoder.layer.*.intermediate.dense.weight, pooler.dense.weight, embeddings.token_type_embeddings.weight\n"
     ]
    }
   ],
   "source": [
    "df_respuestas_detalladas = []\n",
    "for i, row in df_nuevos_qa.iterrows():\n",
    "    if ind_flow_gpt==1:\n",
    "        pregunta = str(row[\"Pregunta\"])\n",
    "        respuesta_humana = str(row[\"Respuesta\"])\n",
    "\n",
    "        respuesta_modelo_gpt = \"No se pudo generar respuesta.\"\n",
    "        input_tokens_gpt = 0\n",
    "        output_tokens_gpt = 0\n",
    "        api_latency_gpt = 0\n",
    "        total_exec_time_gpt = 0\n",
    "        successful_gpt_response = False\n",
    "        contexto_gpt_usado = \"\"\n",
    "\n",
    "        for attempt in range(max_attempts):\n",
    "            try:\n",
    "                start_time_total = time.time()\n",
    "                context_chunks_gpt = get_context_from_query(pregunta, df_vector_store_gpt, top_join, text_embedding)\n",
    "                contexto_gpt_usado = \" \".join(context_chunks_gpt) # Para alucinación\n",
    "                custom_prompt_gpt = pmpt_.format(source=str(context_chunks_gpt))\n",
    "\n",
    "                llm_start_time = time.time()\n",
    "                completion = client.chat.completions.create(\n",
    "                        model=model_,\n",
    "                        temperature=temp_,\n",
    "                        messages=[\n",
    "                            {\"role\": \"system\", \"content\": custom_prompt_gpt},\n",
    "                            {\"role\": \"user\", \"content\": pregunta}\n",
    "                        ]\n",
    "                )\n",
    "                respuesta_modelo_gpt = completion.choices[0].message.content\n",
    "                llm_end_time = time.time()\n",
    "\n",
    "                input_tokens_gpt = completion.usage.prompt_tokens\n",
    "                output_tokens_gpt = completion.usage.completion_tokens\n",
    "                api_latency_gpt = llm_end_time - llm_start_time\n",
    "                total_exec_time_gpt = time.time() - start_time_total\n",
    "                successful_gpt_response = True\n",
    "                break # Salir del bucle de reintentos si fue exitoso\n",
    "            except Exception as e:\n",
    "                print(f\"    [GPT] Error en intento {attempt + 1}/{max_attempts} para '{pregunta[:50]}...': {e}\")\n",
    "                if \"quota\" in str(e).lower() or \"rate limit\" in str(e).lower():\n",
    "                    print(f\"    [GPT] Error de cuota. Esperando {wait_time_on_error_s}s...\")\n",
    "                    time.sleep(wait_time_on_error_s)\n",
    "                else:\n",
    "                    break # Otros errores no reintentables\n",
    "\n",
    "        # Calcular métricas para GPT\n",
    "        similitud_gpt = cosine_final(np.array(text_embedding(respuesta_modelo_gpt)), np.array(text_embedding(respuesta_humana))) if successful_gpt_response else 0.0\n",
    "        coherencia_gpt = medir_coherencia(pregunta, respuesta_modelo_gpt) if successful_gpt_response else 0.0\n",
    "        personalizacion_gpt = medir_personalizacion(perfil_usuario, respuesta_modelo_gpt) if successful_gpt_response else 0.0\n",
    "        fluidez_gpt = medir_fluidez(respuesta_modelo_gpt) if successful_gpt_response else 0.0\n",
    "        alucinacion_gpt = medir_alucinacion(respuesta_modelo_gpt, contexto_gpt_usado) if successful_gpt_response else 0.0\n",
    "\n",
    "        df_respuestas_detalladas.append({\n",
    "                \"Pregunta\": pregunta,\n",
    "                \"Respuesta_Humana\": respuesta_humana,\n",
    "                \"Respuesta_Modelo\": respuesta_modelo_gpt,\n",
    "                \"Similitud_Coseno\": similitud_gpt,\n",
    "                \"Coherencia\": coherencia_gpt,\n",
    "                \"Personalizacion\": personalizacion_gpt,\n",
    "                \"Fluidez\": fluidez_gpt,\n",
    "                \"Alucinacion\": alucinacion_gpt,\n",
    "                \"Input_Tokens\": input_tokens_gpt,\n",
    "                \"Output_Tokens\": output_tokens_gpt,\n",
    "                \"API_Latency_s\": api_latency_gpt,\n",
    "                \"Total_Exec_Time_s\": total_exec_time_gpt,\n",
    "                \"Successful_Response\": successful_gpt_response\n",
    "        })\n",
    "\n",
    "\n",
    "        \n",
    "    else:\n",
    "        respuesta_modelo_gemma = \"No se pudo generar respuesta.\"\n",
    "        input_tokens_gemma = 0\n",
    "        output_tokens_gemma = 0\n",
    "        api_latency_gemma = 0\n",
    "        total_exec_time_gemma = 0\n",
    "        successful_gemma_response = False\n",
    "        contexto_gemma_usado = \"\"\n",
    "\n",
    "        for attempt in range(max_attempts):\n",
    "            try:\n",
    "                start_time_total = time.time()\n",
    "                context_chunks_gemma = get_context_from_query(pregunta, df_vector_store_gem, top_join, text_embedding_gemma)\n",
    "                contexto_gemma_usado = \" \".join(context_chunks_gemma) # Para alucinación\n",
    "                custom_prompt_gemma = pmpt_.format(source=str(context_chunks_gemma))\n",
    "                model_2 = genai.GenerativeModel(model_name=model_)\n",
    "                full_query_for_gemma = custom_prompt_gemma + \"\\n\\nPregunta del usuario: \" + pregunta\n",
    "\n",
    "                llm_start_time = time.time()\n",
    "                completion2 = model_2.generate_content(\n",
    "                        contents=full_query_for_gemma,\n",
    "                        generation_config=GenerationConfig(temperature=temp_)\n",
    "                )\n",
    "                respuesta_modelo_gemma = completion2.text\n",
    "                llm_end_time = time.time()\n",
    "\n",
    "                # Para Gemini/Gemma, los tokens son una estimación simple si no se exponen directamente en la respuesta\n",
    "                input_tokens_gemma = len(full_query_for_gemma.split())\n",
    "                output_tokens_gemma = len(respuesta_modelo_gemma.split())\n",
    "                api_latency_gemma = llm_end_time - llm_start_time\n",
    "                total_exec_time_gemma = time.time() - start_time_total\n",
    "                successful_gemma_response = True\n",
    "                break # Salir del bucle de reintentos si fue exitoso\n",
    "            except Exception as e:\n",
    "                print(f\"    [GEMINI] Error en intento {attempt + 1}/{max_attempts} para '{pregunta[:50]}...': {e}\")\n",
    "                if \"ResourceExhausted\" in str(e) or \"quota\" in str(e).lower() or \"rate limit\" in str(e).lower():\n",
    "                    print(f\"    [GEMINI] Error de cuota. Esperando {wait_time_on_error_s}s...\")\n",
    "                    time.sleep(wait_time_on_error_s)\n",
    "                else:\n",
    "                    break # Otros errores no reintentables\n",
    "\n",
    "        # Calcular métricas para Gemma\n",
    "        similitud_gemma = cosine_final(np.array(text_embedding_gemma(respuesta_modelo_gemma)), np.array(text_embedding_gemma(respuesta_humana))) if successful_gemma_response else 0.0\n",
    "        coherencia_gemma = medir_coherencia(pregunta, respuesta_modelo_gemma) if successful_gemma_response else 0.0\n",
    "        personalizacion_gemma = medir_personalizacion(perfil_usuario, respuesta_modelo_gemma) if successful_gemma_response else 0.0\n",
    "        fluidez_gemma = medir_fluidez(respuesta_modelo_gemma) if successful_gemma_response else 0.0\n",
    "        alucinacion_gemma = medir_alucinacion(respuesta_modelo_gemma, contexto_gemma_usado) if successful_gemma_response else 0.0\n",
    "\n",
    "        df_respuestas_detalladas.append({\n",
    "                \"Pregunta\": pregunta,\n",
    "                \"Respuesta_Humana\": respuesta_humana,\n",
    "                \"Respuesta_Modelo\": respuesta_modelo_gemma,\n",
    "                \"Similitud_Coseno\": similitud_gemma,\n",
    "                \"Coherencia\": coherencia_gemma,\n",
    "                \"Personalizacion\": personalizacion_gemma,\n",
    "                \"Fluidez\": fluidez_gemma,\n",
    "                \"Alucinacion\": alucinacion_gemma,\n",
    "                \"Input_Tokens\": input_tokens_gemma,\n",
    "                \"Output_Tokens\": output_tokens_gemma,\n",
    "                \"API_Latency_s\": api_latency_gemma,\n",
    "                \"Total_Exec_Time_s\": total_exec_time_gemma,\n",
    "                \"Successful_Response\": successful_gemma_response\n",
    "        })\n",
    "\n",
    "\n",
    "df_respuestas_detalladas = pd.DataFrame(df_respuestas_detalladas)\n",
    "\n",
    "# --- Cálculo de Métricas y Resumen Final ---\n",
    "# df_res_successful = df_respuestas_detalladas[df_respuestas_detalladas['Successful_Response'] == True].copy()\n",
    "\n",
    "df_respuestas_detalladas = df_respuestas_detalladas[\n",
    "    (df_respuestas_detalladas['Successful_Response'] == True) &\n",
    "    (df_respuestas_detalladas['Similitud_Coseno'] != 0) &\n",
    "    (df_respuestas_detalladas['Coherencia'] != 0) &\n",
    "    (df_respuestas_detalladas['Personalizacion'] != 0) &\n",
    "    (df_respuestas_detalladas['Fluidez'] != 0) &\n",
    "    (df_respuestas_detalladas['Alucinacion'] != 0)\n",
    "].copy()\n",
    "df_respuestas_detalladas.index = range(df_respuestas_detalladas.shape[0])\n",
    "\n",
    "# Calcular las tasas\n",
    "df_respuestas_detalladas[\"Tasa\"] = df_respuestas_detalladas.apply(lambda row: 1 if row[\"Similitud_Coseno\"]>=umbrales[0] else 0, axis = 1)\n",
    "df_respuestas_detalladas[\"Coh\"] = df_respuestas_detalladas.apply(lambda row: 1 if row[\"Coherencia\"]>=umbrales[1] else 0, axis = 1)\n",
    "df_respuestas_detalladas[\"Per\"] = df_respuestas_detalladas.apply(lambda row: 1 if row[\"Personalizacion\"]>=umbrales[2] else 0, axis = 1)\n",
    "df_respuestas_detalladas[\"Flu\"] = df_respuestas_detalladas.apply(lambda row: 1 if row[\"Fluidez\"]>=umbrales[3] else 0, axis = 1)\n",
    "df_respuestas_detalladas[\"Alu\"] = df_respuestas_detalladas.apply(lambda row: 1 if row[\"Alucinacion\"]>=umbrales[4] else 0, axis = 1)\n",
    "df_respuestas_detalladas[\"total\"] = 1 # Para contar el total de filas\n",
    "df_respuestas_detalladas[\"Modelo\"] = model_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (min_F!=0) & (max_F!=1):\n",
    "    df_respuestas_detalladas[\"Fluidez2\"]  = (df_respuestas_detalladas[\"Fluidez\"] - min_F) / (max_F - min_F)\n",
    "    df_respuestas_detalladas[\"Flu2\"] = df_respuestas_detalladas.apply(lambda row: 1 if row[\"Fluidez2\"]>=0.50 else 0, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Modelo</th>\n",
       "      <th>Tasa</th>\n",
       "      <th>Coh</th>\n",
       "      <th>Per</th>\n",
       "      <th>Flu</th>\n",
       "      <th>Flu2</th>\n",
       "      <th>Alu</th>\n",
       "      <th>Total_Respuestas</th>\n",
       "      <th>Respuestas_Exitosas</th>\n",
       "      <th>avg_input_tokens</th>\n",
       "      <th>avg_output_tokens</th>\n",
       "      <th>avg_api_latency</th>\n",
       "      <th>avg_total_execution_time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>gpt-4o-mini_clasificacion-PQRS</td>\n",
       "      <td>100.0</td>\n",
       "      <td>86.666667</td>\n",
       "      <td>100.0</td>\n",
       "      <td>13.333333</td>\n",
       "      <td>20.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>15</td>\n",
       "      <td>15</td>\n",
       "      <td>2736.733333</td>\n",
       "      <td>226.333333</td>\n",
       "      <td>2.778824</td>\n",
       "      <td>3.261617</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           Modelo   Tasa        Coh    Per        Flu  Flu2  \\\n",
       "0  gpt-4o-mini_clasificacion-PQRS  100.0  86.666667  100.0  13.333333  20.0   \n",
       "\n",
       "     Alu  Total_Respuestas  Respuestas_Exitosas  avg_input_tokens  \\\n",
       "0  100.0                15                   15       2736.733333   \n",
       "\n",
       "   avg_output_tokens  avg_api_latency  avg_total_execution_time  \n",
       "0         226.333333         2.778824                  3.261617  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Agrupar y calcular promedios\n",
    "summary_ = df_respuestas_detalladas.groupby([\"Modelo\"]).agg(\n",
    "    Tasa=('Tasa', 'mean'),\n",
    "    Coh=('Coh', 'mean'),\n",
    "    Per=('Per', 'mean'),\n",
    "    Flu=('Flu', 'mean'),\n",
    "    Flu2=('Flu2', 'mean'),\n",
    "    Alu=('Alu', 'mean'),\n",
    "    Total_Respuestas=('total', 'sum'),\n",
    "    Respuestas_Exitosas=('Successful_Response', 'sum'), # Contar las exitosas\n",
    "    avg_input_tokens=('Input_Tokens', 'mean'),\n",
    "    avg_output_tokens=('Output_Tokens', 'mean'),\n",
    "    avg_api_latency=('API_Latency_s', 'mean'),\n",
    "    avg_total_execution_time=('Total_Exec_Time_s', 'mean')\n",
    ").reset_index()\n",
    "\n",
    "# Convertir tasas a porcentaje\n",
    "for k in [\"Tasa\", \"Coh\", \"Per\", \"Flu\", \"Flu2\" ,\"Alu\"]:\n",
    "    summary_[k] = 100 * summary_[k]\n",
    "\n",
    "summary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tasa de respuestas correctas:  100.0 %\n",
      "Tasa de coherencia:  86.667 %\n",
      "Tasa de personalización:  100.0 %\n",
      "Tasa de fluidez:  20.0 %\n",
      "Tasa de no alucinación:  100.0 %\n",
      "Tasa de precisión: 100.0 %\n",
      "Tasa de calidad linguística: 53.3335 %\n",
      "Promedio de tokens en input:  2736.733\n",
      "Promedio de tokens en output:  226.333\n",
      "Promedio de latencia API (seg):  2.779\n",
      "Promedio de respuesta (seg):  3.262\n"
     ]
    }
   ],
   "source": [
    "## Métricas FInales Monitoring:\n",
    "fl = np.max([summary_[\"Flu\"][0].round(3),summary_[\"Flu2\"][0].round(3)])\n",
    "\n",
    "print(\"Tasa de respuestas correctas: \",summary_[\"Tasa\"][0].round(3),\"%\")\n",
    "print(\"Tasa de coherencia: \",summary_[\"Coh\"][0].round(3),\"%\")\n",
    "print(\"Tasa de personalización: \",summary_[\"Per\"][0].round(3),\"%\")\n",
    "print(\"Tasa de fluidez: \",  fl  ,\"%\")\n",
    "print(\"Tasa de no alucinación: \",summary_[\"Alu\"][0].round(3),\"%\")\n",
    "\n",
    "print(\"Tasa de precisión:\" , 1/3 * (summary_[\"Tasa\"][0].round(3) + summary_[\"Per\"][0].round(3) + summary_[\"Alu\"][0].round(3) ) ,\"%\" )\n",
    "print(\"Tasa de calidad linguística:\", 1/2 * (fl +  summary_[\"Coh\"][0].round(3) ),\"%\" )\n",
    "\n",
    "print(\"Promedio de tokens en input: \",summary_[\"avg_input_tokens\"][0].round(3))\n",
    "print(\"Promedio de tokens en output: \",summary_[\"avg_output_tokens\"][0].round(3))\n",
    "print(\"Promedio de latencia API (seg): \",summary_[\"avg_api_latency\"][0].round(3))\n",
    "print(\"Promedio de respuesta (seg): \",summary_[\"avg_total_execution_time\"][0].round(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BATCH CUSTOM PREDICT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pregunta</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>¿Qué son los modelos de lenguaje de dominio es...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>¿Cuál es la importancia de los términos de lic...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>¿Qué es un entorno de sandbox en el contexto d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>¿Cuál es el propósito de la evaluación de recu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>¿Qué es la evaluación de benchmarks en LLMs?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>¿Qué es el modelo Koala y cuáles son sus carac...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>¿Cómo se evalúan los modelos LLM para medir su...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>¿Qué papel juegan las evaluaciones de sesgo en...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>¿Cuál es el objetivo del uso de modelos de IA ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>¿Qué es el modelo BloombergGPT y en qué sector...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>68 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             Pregunta\n",
       "0   ¿Qué son los modelos de lenguaje de dominio es...\n",
       "1   ¿Cuál es la importancia de los términos de lic...\n",
       "2   ¿Qué es un entorno de sandbox en el contexto d...\n",
       "3   ¿Cuál es el propósito de la evaluación de recu...\n",
       "4        ¿Qué es la evaluación de benchmarks en LLMs?\n",
       "..                                                ...\n",
       "63  ¿Qué es el modelo Koala y cuáles son sus carac...\n",
       "64  ¿Cómo se evalúan los modelos LLM para medir su...\n",
       "65  ¿Qué papel juegan las evaluaciones de sesgo en...\n",
       "66  ¿Cuál es el objetivo del uso de modelos de IA ...\n",
       "67  ¿Qué es el modelo BloombergGPT y en qué sector...\n",
       "\n",
       "[68 rows x 1 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch = pd.read_excel(\"CRONBACH.xlsx\")\n",
    "batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "if ind_flow_gpt == 1:\n",
    "    df_vector_ = df_vector_store_gpt.copy()\n",
    "else:\n",
    "    df_vector_ = df_vector_store_gem.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_response(pregunta,ind_flow_gpt,top_join,df_vector_,pmpt_,model_,temp_):\n",
    "    if ind_flow_gpt==1:\n",
    "        context_chunks_gpt = get_context_from_query(pregunta, df_vector_, top_join, text_embedding)\n",
    "        custom_prompt_gpt = pmpt_.format(source=str(context_chunks_gpt))\n",
    "\n",
    "        completion = client.chat.completions.create(\n",
    "                        model=model_,\n",
    "                        temperature=temp_,\n",
    "                        messages=[\n",
    "                            {\"role\": \"system\", \"content\": custom_prompt_gpt},\n",
    "                            {\"role\": \"user\", \"content\": pregunta}\n",
    "                        ]\n",
    "        )\n",
    "        respuesta_modelo = completion.choices[0].message.content\n",
    "    else:\n",
    "        context_chunks_gemma = get_context_from_query(pregunta, df_vector_, top_join, text_embedding_gemma)\n",
    "        custom_prompt_gemma = pmpt_.format(source=str(context_chunks_gemma))\n",
    "        model_2 = genai.GenerativeModel(model_name=model_)\n",
    "        full_query_for_gemma = custom_prompt_gemma + \"\\n\\nPregunta del usuario: \" + pregunta\n",
    "\n",
    "        completion2 = model_2.generate_content(\n",
    "                        contents=full_query_for_gemma,\n",
    "                        generation_config=GenerationConfig(temperature=temp_)\n",
    "        )\n",
    "        respuesta_modelo = completion2.text\n",
    "    return respuesta_modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "respuestas_ = []\n",
    "\n",
    "for i, row in batch.iterrows(): \n",
    "    pregunta = str(row[\"Pregunta\"])\n",
    "    respuesta = model_response(pregunta = pregunta,ind_flow_gpt = ind_flow_gpt,top_join = top_join,df_vector_ = df_vector_,pmpt_ = pmpt_,model_ = model_,temp_ = temp_)\n",
    "    respuestas_.append(respuesta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch[\"Respuesta\"] = respuestas_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pregunta</th>\n",
       "      <th>Respuesta</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>¿Qué son los modelos de lenguaje de dominio es...</td>\n",
       "      <td>Los modelos de lenguaje de dominio específico ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>¿Cuál es la importancia de los términos de lic...</td>\n",
       "      <td>La importancia de los términos de licencia en ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>¿Qué es un entorno de sandbox en el contexto d...</td>\n",
       "      <td>Un entorno de sandbox en el contexto de LLMOps...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>¿Cuál es el propósito de la evaluación de recu...</td>\n",
       "      <td>La evaluación de recuperación en el uso de Mod...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>¿Qué es la evaluación de benchmarks en LLMs?</td>\n",
       "      <td>La evaluación de benchmarks en modelos de leng...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>¿Qué es el modelo Vicuna y cuál es su relación...</td>\n",
       "      <td>Vicuna es un modelo de lenguaje que se inspira...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>¿Cómo se define un entorno de producción en LL...</td>\n",
       "      <td>Un entorno de producción en LLMOps es el espac...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>¿Qué pasos son esenciales antes de iniciar un ...</td>\n",
       "      <td>Antes de iniciar un proyecto de LLM (Modelos d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>¿Qué es CHAIR en la evaluación de modelos de l...</td>\n",
       "      <td>CHAIR, que significa \"Caption Hallucination As...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>¿Por qué es importante la democratización de l...</td>\n",
       "      <td>La democratización de la tecnología de intelig...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            Pregunta  \\\n",
       "0  ¿Qué son los modelos de lenguaje de dominio es...   \n",
       "1  ¿Cuál es la importancia de los términos de lic...   \n",
       "2  ¿Qué es un entorno de sandbox en el contexto d...   \n",
       "3  ¿Cuál es el propósito de la evaluación de recu...   \n",
       "4       ¿Qué es la evaluación de benchmarks en LLMs?   \n",
       "5  ¿Qué es el modelo Vicuna y cuál es su relación...   \n",
       "6  ¿Cómo se define un entorno de producción en LL...   \n",
       "7  ¿Qué pasos son esenciales antes de iniciar un ...   \n",
       "8  ¿Qué es CHAIR en la evaluación de modelos de l...   \n",
       "9  ¿Por qué es importante la democratización de l...   \n",
       "\n",
       "                                           Respuesta  \n",
       "0  Los modelos de lenguaje de dominio específico ...  \n",
       "1  La importancia de los términos de licencia en ...  \n",
       "2  Un entorno de sandbox en el contexto de LLMOps...  \n",
       "3  La evaluación de recuperación en el uso de Mod...  \n",
       "4  La evaluación de benchmarks en modelos de leng...  \n",
       "5  Vicuna es un modelo de lenguaje que se inspira...  \n",
       "6  Un entorno de producción en LLMOps es el espac...  \n",
       "7  Antes de iniciar un proyecto de LLM (Modelos d...  \n",
       "8  CHAIR, que significa \"Caption Hallucination As...  \n",
       "9  La democratización de la tecnología de intelig...  "
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch.to_excel(\"Mail_Notification.xlsx\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "my_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
